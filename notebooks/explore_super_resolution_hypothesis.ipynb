{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0fd7f89-b549-4982-afec-59b319fd3a79",
   "metadata": {},
   "source": [
    "# Imports/Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c687e91b-80c6-4bb9-8770-30e2e311f393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "# Specific to NERSC: Set up kernel using: https://docs.nersc.gov/services/jupyter/how-to-guides/\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from neuralop import H1Loss\n",
    "from neuralop import LpLoss\n",
    "from neuralop.data.datasets import load_darcy_flow_small\n",
    "from neuralop.data.datasets.darcy import DarcyDataset\n",
    "from neuralop.data.transforms.data_processors import IncrementalDataProcessor\n",
    "from neuralop.models import FNO\n",
    "from neuralop.training import AdamW\n",
    "from neuralop.training.incremental import IncrementalFNOTrainer\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.utils import get_project_root\n",
    "\n",
    "root_dir = get_project_root() / 'neuralop/data/datasets/data'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21c98068-8bf8-44c8-9a9c-f749758de8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 16 with 32 samples \n",
      "Loading test db for resolution 32 with 32 samples \n",
      "Loading test db for resolution 64 with 32 samples \n",
      "Loading test db for resolution 128 with 32 samples \n"
     ]
    }
   ],
   "source": [
    "# first download data\n",
    "data = DarcyDataset(\n",
    "    root_dir=root_dir,\n",
    "    n_train=100,\n",
    "    n_tests=[32, 32, 32, 32],\n",
    "    batch_size=16,\n",
    "    test_batch_sizes=[16, 16, 16, 16],\n",
    "    train_resolution=128,  # change resolution to download different data\n",
    "    test_resolutions=[16, 32, 64, 128],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07713ce0-df3f-41f2-813c-3f3bd93ef1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 100 samples \n",
      "Loading test db for resolution 64 with 100 samples \n",
      "Loading test db for resolution 128 with 100 samples \n"
     ]
    }
   ],
   "source": [
    "# load darcy flow dataset\n",
    "\n",
    "train_loader, test_loaders, output_encoder = load_darcy_flow_small(\n",
    "    n_train=1000,\n",
    "    batch_size=16,\n",
    "    test_resolutions=[16, 32, 64, 128],\n",
    "    n_tests=[100, 100, 100, 100],\n",
    "    test_batch_sizes=[32, 32, 32, 32],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7478773a-e5b1-46eb-8332-db7bf84c32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental = True\n",
    "starting_modes = (16, 16)\n",
    "if incremental:\n",
    "    starting_modes = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "688f0d28-435a-4d70-880c-f4e02e82ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model = FNO(\n",
    "    max_n_modes=(16, 16),\n",
    "    n_modes=starting_modes,\n",
    "    hidden_channels=32,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    ")\n",
    "model = model.to(device)\n",
    "n_params = count_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ff56ee2-5476-4168-a8f2-330cf4e7ecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Incre Res: change index to 0\n",
      "Original Incre Res: change sub to 2\n",
      "Original Incre Res: change res to 8\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=8e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "data_transform = IncrementalDataProcessor(\n",
    "    in_normalizer=None,\n",
    "    out_normalizer=None,\n",
    "    device=device,\n",
    "    subsampling_rates=[2, 1],\n",
    "    dataset_resolution=16,\n",
    "    dataset_indices=[2, 3],\n",
    "    epoch_gap=10,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "data_transform = data_transform.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "548e6491-a40b-4736-be10-2b45b472da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### N PARAMS ###\n",
      " 2110305\n",
      "\n",
      "### OPTIMIZER ###\n",
      " AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 0.008\n",
      "    lr: 0.008\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7fc7243bf010>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      "### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fc72419e020>\n",
      "\n",
      " * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fc72419e020>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fc72445f340>}\n"
     ]
    }
   ],
   "source": [
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "train_loss = h1loss\n",
    "eval_losses = {'h1': h1loss, 'l2': l2loss}\n",
    "print('\\n### N PARAMS ###\\n', n_params)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print('\\n### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a609d3ab-24aa-47f4-b903-f9733554e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally pass all of these to the Trainer\n",
    "trainer = IncrementalFNOTrainer(\n",
    "    model=model,\n",
    "    n_epochs=20,\n",
    "    data_processor=data_transform,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    incremental_loss_gap=False,\n",
    "    incremental_grad=True,\n",
    "    incremental_grad_eps=0.9999,\n",
    "    incremental_loss_eps=0.001,\n",
    "    incremental_buffer=5,\n",
    "    incremental_max_iter=1,\n",
    "    incremental_grad_max_iter=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b14e1541-4083-49f8-841f-53c45db9b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1000 samples\n",
      "Testing on [50, 100, 100, 100] samples         on resolutions [16, 32, 64, 128].\n",
      "Raw outputs of shape torch.Size([16, 1, 8, 8])\n",
      "[0] time=1.21, avg_loss=0.6164, train_err=9.7849\n",
      "Eval: 16_h1=0.6505, 16_l2=0.3764, 32_h1=0.8450, 32_l2=0.4031, 64_h1=1.1013, 64_l2=0.4172, 128_h1=1.5083, 128_l2=0.4897\n",
      "[1] time=0.74, avg_loss=0.4188, train_err=6.6470\n",
      "Eval: 16_h1=0.5863, 16_l2=0.3301, 32_h1=0.8127, 32_l2=0.3561, 64_h1=1.0889, 64_l2=0.3740, 128_h1=1.5940, 128_l2=0.4818\n",
      "[2] time=0.75, avg_loss=0.3691, train_err=5.8588\n",
      "Eval: 16_h1=0.5545, 16_l2=0.2863, 32_h1=0.8036, 32_l2=0.3216, 64_h1=1.0984, 64_l2=0.3437, 128_h1=1.6642, 128_l2=0.4832\n",
      "[3] time=0.75, avg_loss=0.3473, train_err=5.5134\n",
      "Eval: 16_h1=0.5317, 16_l2=0.2860, 32_h1=0.7604, 32_l2=0.3209, 64_h1=1.0230, 64_l2=0.3432, 128_h1=1.5462, 128_l2=0.4739\n",
      "[4] time=0.77, avg_loss=0.3267, train_err=5.1863\n",
      "Eval: 16_h1=0.5213, 16_l2=0.2676, 32_h1=0.7558, 32_l2=0.3043, 64_h1=1.0268, 64_l2=0.3290, 128_h1=1.5899, 128_l2=0.4788\n",
      "[5] time=0.75, avg_loss=0.3164, train_err=5.0219\n",
      "Eval: 16_h1=0.5152, 16_l2=0.2638, 32_h1=0.7437, 32_l2=0.3000, 64_h1=1.0153, 64_l2=0.3262, 128_h1=1.5738, 128_l2=0.4782\n",
      "[6] time=0.74, avg_loss=0.3022, train_err=4.7967\n",
      "Eval: 16_h1=0.5148, 16_l2=0.2620, 32_h1=0.7407, 32_l2=0.2986, 64_h1=1.0001, 64_l2=0.3266, 128_h1=1.5339, 128_l2=0.4730\n",
      "[7] time=0.75, avg_loss=0.2872, train_err=4.5585\n",
      "Eval: 16_h1=0.5042, 16_l2=0.2579, 32_h1=0.7312, 32_l2=0.2953, 64_h1=0.9872, 64_l2=0.3240, 128_h1=1.5077, 128_l2=0.4602\n",
      "[8] time=0.76, avg_loss=0.2816, train_err=4.4691\n",
      "Eval: 16_h1=0.5061, 16_l2=0.2553, 32_h1=0.7511, 32_l2=0.2980, 64_h1=1.0277, 64_l2=0.3279, 128_h1=1.5849, 128_l2=0.4604\n",
      "[9] time=0.75, avg_loss=0.2758, train_err=4.3778\n",
      "Eval: 16_h1=0.5045, 16_l2=0.2545, 32_h1=0.7524, 32_l2=0.2949, 64_h1=1.0313, 64_l2=0.3242, 128_h1=1.5699, 128_l2=0.4426\n",
      "Incre Res Update: change index to 1\n",
      "Incre Res Update: change sub to 1\n",
      "Incre Res Update: change res to 16\n",
      "[10] time=0.82, avg_loss=0.2997, train_err=4.7574\n",
      "Eval: 16_h1=0.2707, 16_l2=0.1532, 32_h1=0.4406, 32_l2=0.1873, 64_h1=0.6403, 64_l2=0.2096, 128_h1=1.1049, 128_l2=0.3740\n",
      "[11] time=0.74, avg_loss=0.2622, train_err=4.1624\n",
      "Eval: 16_h1=0.2578, 16_l2=0.1362, 32_h1=0.4326, 32_l2=0.1720, 64_h1=0.6382, 64_l2=0.1958, 128_h1=1.1303, 128_l2=0.3741\n",
      "[12] time=0.70, avg_loss=0.2318, train_err=3.6801\n",
      "Eval: 16_h1=0.2189, 16_l2=0.1104, 32_h1=0.3881, 32_l2=0.1480, 64_h1=0.5873, 64_l2=0.1754, 128_h1=1.0665, 128_l2=0.3728\n",
      "[13] time=0.69, avg_loss=0.2157, train_err=3.4244\n",
      "Eval: 16_h1=0.2151, 16_l2=0.1109, 32_h1=0.3817, 32_l2=0.1489, 64_h1=0.5782, 64_l2=0.1758, 128_h1=1.0454, 128_l2=0.3606\n",
      "[14] time=0.69, avg_loss=0.2075, train_err=3.2932\n",
      "Eval: 16_h1=0.2131, 16_l2=0.1135, 32_h1=0.3753, 32_l2=0.1496, 64_h1=0.5699, 64_l2=0.1758, 128_h1=1.0325, 128_l2=0.3547\n",
      "[15] time=0.69, avg_loss=0.2013, train_err=3.1957\n",
      "Eval: 16_h1=0.2068, 16_l2=0.1095, 32_h1=0.3711, 32_l2=0.1455, 64_h1=0.5679, 64_l2=0.1724, 128_h1=1.0223, 128_l2=0.3497\n",
      "[16] time=0.69, avg_loss=0.1953, train_err=3.0995\n",
      "Eval: 16_h1=0.2107, 16_l2=0.1196, 32_h1=0.3711, 32_l2=0.1514, 64_h1=0.5678, 64_l2=0.1765, 128_h1=1.0023, 128_l2=0.3405\n",
      "[17] time=0.69, avg_loss=0.1924, train_err=3.0542\n",
      "Eval: 16_h1=0.2051, 16_l2=0.1041, 32_h1=0.3746, 32_l2=0.1382, 64_h1=0.5780, 64_l2=0.1663, 128_h1=1.0344, 128_l2=0.3557\n",
      "[18] time=0.69, avg_loss=0.1849, train_err=2.9353\n",
      "Eval: 16_h1=0.1951, 16_l2=0.1022, 32_h1=0.3658, 32_l2=0.1357, 64_h1=0.5667, 64_l2=0.1643, 128_h1=1.0112, 128_l2=0.3545\n",
      "[19] time=0.69, avg_loss=0.1770, train_err=2.8088\n",
      "Eval: 16_h1=0.1961, 16_l2=0.1059, 32_h1=0.3642, 32_l2=0.1398, 64_h1=0.5666, 64_l2=0.1682, 128_h1=1.0160, 128_l2=0.3496\n"
     ]
    }
   ],
   "source": [
    "end_stats = trainer.train(\n",
    "    train_loader,\n",
    "    test_loaders,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    regularizer=False,\n",
    "    training_loss=train_loss,\n",
    "    eval_losses=eval_losses,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc9a92c9-f8cd-444e-96c5-4a768aa357f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_err': 2.808846625070723,\n",
       " 'avg_loss': 0.17695733737945557,\n",
       " 'avg_lasso_loss': None,\n",
       " 'epoch_train_time': 0.6864375360019039,\n",
       " '16_h1': tensor(0.1961, device='cuda:0'),\n",
       " '16_l2': tensor(0.1059, device='cuda:0'),\n",
       " '32_h1': tensor(0.3642, device='cuda:0'),\n",
       " '32_l2': tensor(0.1398, device='cuda:0'),\n",
       " '64_h1': tensor(0.5666, device='cuda:0'),\n",
       " '64_l2': tensor(0.1682, device='cuda:0'),\n",
       " '128_h1': tensor(1.0160, device='cuda:0'),\n",
       " '128_l2': tensor(0.3496, device='cuda:0')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3b8b3af-72cd-416e-9094-0919ba71d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['train_resolution', 'test_resolution', 'l2', 'h1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67136f36-3dca-4ada-98b7-1d9470eae315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "operator_alias",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
