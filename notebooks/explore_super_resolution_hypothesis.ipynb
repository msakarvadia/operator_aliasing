{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0fd7f89-b549-4982-afec-59b319fd3a79",
   "metadata": {},
   "source": [
    "# Imports/Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c687e91b-80c6-4bb9-8770-30e2e311f393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "# Specific to NERSC: Set up kernel using: https://docs.nersc.gov/services/jupyter/how-to-guides/\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from neuralop import H1Loss\n",
    "from neuralop import LpLoss\n",
    "from neuralop.data.datasets.darcy import DarcyDataset\n",
    "from neuralop.data.transforms.data_processors import IncrementalDataProcessor\n",
    "from neuralop.models import FNO\n",
    "from neuralop.training import AdamW\n",
    "from neuralop.training.incremental import IncrementalFNOTrainer\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.utils import get_project_root\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root_dir = get_project_root() / 'neuralop/data/datasets/data'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c98068-8bf8-44c8-9a9c-f749758de8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 16 with 32 samples \n",
      "Loading test db for resolution 32 with 32 samples \n",
      "Loading test db for resolution 64 with 32 samples \n",
      "Loading test db for resolution 128 with 32 samples \n"
     ]
    }
   ],
   "source": [
    "# first download data\n",
    "data = DarcyDataset(\n",
    "    root_dir=root_dir,\n",
    "    n_train=100,\n",
    "    n_tests=[32, 32, 32, 32],\n",
    "    batch_size=16,\n",
    "    test_batch_sizes=[16, 16, 16, 16],\n",
    "    train_resolution=128,  # change resolution to download different data\n",
    "    test_resolutions=[16, 32, 64, 128],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07713ce0-df3f-41f2-813c-3f3bd93ef1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load darcy flow dataset\n",
    "\n",
    "\n",
    "def load_darcy_flow_small(\n",
    "    n_train,\n",
    "    n_tests,\n",
    "    data_root=root_dir,\n",
    "    test_resolutions=(16, 32),\n",
    "    train_resolution=16,\n",
    "):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    batch_size = 16\n",
    "    test_batch_sizes = [batch_size] * len(test_resolutions)\n",
    "\n",
    "    dataset = DarcyDataset(\n",
    "        root_dir=data_root,\n",
    "        n_train=n_train,\n",
    "        n_tests=n_tests,\n",
    "        batch_size=batch_size,\n",
    "        test_batch_sizes=test_batch_sizes,\n",
    "        train_resolution=train_resolution,\n",
    "        test_resolutions=test_resolutions,\n",
    "        encode_input=False,\n",
    "        encode_output=True,\n",
    "        channel_dim=1,\n",
    "        encoding='channel-wise',\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    # return dataloaders for backwards compat\n",
    "    train_loader = DataLoader(\n",
    "        dataset.train_db,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    test_loaders = {}\n",
    "    for res, test_bsize in zip(test_resolutions, test_batch_sizes):\n",
    "        test_loaders[res] = DataLoader(\n",
    "            dataset.test_dbs[res],\n",
    "            batch_size=test_bsize,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "\n",
    "    return train_loader, test_loaders, dataset.data_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd3e95-8881-48ba-8024-7aee6549bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 100 samples \n",
      "Loading test db for resolution 64 with 100 samples \n",
      "Loading test db for resolution 128 with 100 samples \n",
      "Original Incre Res: change index to 0\n",
      "Original Incre Res: change sub to 2\n",
      "Original Incre Res: change res to 8\n",
      "\n",
      "### N PARAMS ###\n",
      " 2110305\n",
      "\n",
      "### OPTIMIZER ###\n",
      " AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 0.008\n",
      "    lr: 0.008\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7fe716b915a0>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      "### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fe8ae73fd60>\n",
      "\n",
      " * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fe8ae73fd60>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fe9346f16f0>}\n",
      "Incre Res Update: change index to 1\n",
      "Incre Res Update: change sub to 1\n",
      "Incre Res Update: change res to 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2287260/3641991373.py:101: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 100 samples \n",
      "Loading test db for resolution 64 with 100 samples \n",
      "Loading test db for resolution 128 with 100 samples \n",
      "Original Incre Res: change index to 0\n",
      "Original Incre Res: change sub to 2\n",
      "Original Incre Res: change res to 8\n",
      "\n",
      "### N PARAMS ###\n",
      " 2110305\n",
      "\n",
      "### OPTIMIZER ###\n",
      " AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 0.008\n",
      "    lr: 0.008\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7fe7141a2bf0>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      "### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fe7141a2b90>\n",
      "\n",
      " * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fe7141a2b90>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fe7141a2c20>}\n",
      "Incre Res Update: change index to 1\n",
      "Incre Res Update: change sub to 1\n",
      "Incre Res Update: change res to 16\n",
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 100 samples \n",
      "Loading test db for resolution 64 with 100 samples \n",
      "Loading test db for resolution 128 with 100 samples \n",
      "Original Incre Res: change index to 0\n",
      "Original Incre Res: change sub to 2\n",
      "Original Incre Res: change res to 8\n",
      "\n",
      "### N PARAMS ###\n",
      " 2110305\n",
      "\n",
      "### OPTIMIZER ###\n",
      " AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 0.008\n",
      "    lr: 0.008\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7fe8ae76ab90>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      "### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fe8ae76ae00>\n",
      "\n",
      " * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fe8ae76ae00>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fe8ae76b1f0>}\n",
      "Incre Res Update: change index to 1\n",
      "Incre Res Update: change sub to 1\n",
      "Incre Res Update: change res to 16\n",
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 100 samples \n",
      "Loading test db for resolution 64 with 100 samples \n",
      "Loading test db for resolution 128 with 100 samples \n",
      "Original Incre Res: change index to 0\n",
      "Original Incre Res: change sub to 2\n",
      "Original Incre Res: change res to 8\n",
      "\n",
      "### N PARAMS ###\n",
      " 2110305\n",
      "\n",
      "### OPTIMIZER ###\n",
      " AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 0.008\n",
      "    lr: 0.008\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7fe716b92b90>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      "### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fe7141a2f80>\n",
      "\n",
      " * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fe7141a2f80>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fe7141a00a0>}\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    columns=['train_resolution', 'test_resolution', 'loss_type', 'loss'],\n",
    ")\n",
    "\n",
    "for train_resolution in [16, 32, 64, 128]:\n",
    "    # get data\n",
    "    train_loader, test_loaders, output_encoder = load_darcy_flow_small(\n",
    "        n_train=1000,\n",
    "        # batch_size=16,\n",
    "        train_resolution=train_resolution,\n",
    "        test_resolutions=[16, 32, 64, 128],\n",
    "        n_tests=[100, 100, 100, 100],\n",
    "        # test_batch_sizes=[32, 32, 32, 32],\n",
    "    )\n",
    "\n",
    "    # incrementally vary modes\n",
    "    incremental = True\n",
    "    starting_modes = (16, 16)\n",
    "    if incremental:\n",
    "        starting_modes = (2, 2)\n",
    "\n",
    "    # Set up model\n",
    "    model = FNO(\n",
    "        max_n_modes=(16, 16),\n",
    "        n_modes=starting_modes,\n",
    "        hidden_channels=32,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    n_params = count_model_params(model)\n",
    "\n",
    "    # optimizer + data\n",
    "    optimizer = AdamW(model.parameters(), lr=8e-3, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "    data_transform = IncrementalDataProcessor(\n",
    "        in_normalizer=None,\n",
    "        out_normalizer=None,\n",
    "        device=device,\n",
    "        subsampling_rates=[2, 1],\n",
    "        dataset_resolution=16,\n",
    "        dataset_indices=[2, 3],\n",
    "        epoch_gap=10,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    data_transform = data_transform.to(device)\n",
    "\n",
    "    l2loss = LpLoss(d=2, p=2)\n",
    "    h1loss = H1Loss(d=2)\n",
    "    train_loss = h1loss\n",
    "    eval_losses = {'h1': h1loss, 'l2': l2loss}\n",
    "    print('\\n### N PARAMS ###\\n', n_params)\n",
    "    print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "    print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "    print('\\n### LOSSES ###')\n",
    "    print('\\n### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###')\n",
    "    print(f'\\n * Train: {train_loss}')\n",
    "    print(f'\\n * Test: {eval_losses}')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Finally pass all of these to the Trainer\n",
    "    trainer = IncrementalFNOTrainer(\n",
    "        model=model,\n",
    "        n_epochs=20,\n",
    "        data_processor=data_transform,\n",
    "        device=device,\n",
    "        verbose=False,\n",
    "        incremental_loss_gap=False,\n",
    "        incremental_grad=True,\n",
    "        incremental_grad_eps=0.9999,\n",
    "        incremental_loss_eps=0.001,\n",
    "        incremental_buffer=5,\n",
    "        incremental_max_iter=1,\n",
    "        incremental_grad_max_iter=2,\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    end_stats = trainer.train(\n",
    "        train_loader,\n",
    "        test_loaders,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        regularizer=False,\n",
    "        training_loss=train_loss,\n",
    "        eval_losses=eval_losses,\n",
    "    )\n",
    "\n",
    "    # save stats\n",
    "    for k, v in end_stats.items():\n",
    "        s = k.split('_')\n",
    "\n",
    "        if 'h1' in s or ('l2' in s):\n",
    "            row = {\n",
    "                'train_resolution': train_resolution,\n",
    "                'test_resolution': s[0],\n",
    "                'loss_type': s[1],\n",
    "                'loss': v.item(),\n",
    "            }\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e995380d-edcf-4c8e-833a-4bf214a3e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='test_resolution',\n",
    "    y='loss',\n",
    "    hue='loss_type',\n",
    "    style='train_resolution',\n",
    ")\n",
    "plt.title('Training resolution affect on Test Performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab93af8-aea9-4aea-bb96-9d5d756ba67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap from green (high engagement) to red (low engagement)\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "    'green_red',\n",
    "    ['green', 'yellow', 'red'],\n",
    "    N=100,\n",
    ")\n",
    "for loss_type in ['l2', 'h1']:\n",
    "    heat = df[df.loss_type == loss_type].pivot(\n",
    "        index='train_resolution',\n",
    "        columns='test_resolution',\n",
    "        values='loss',\n",
    "    )\n",
    "    heat = heat.loc[:, ['16', '32', '64', '128']]\n",
    "    sns.heatmap(heat, cmap=cmap)\n",
    "    plt.title(f'{loss_type=}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf7967-7d1a-46a6-9c5b-4bbdb3f09dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frmse(\n",
    "    pred,\n",
    "    ground_truth,\n",
    "):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    batch_size = 16\n",
    "\n",
    "    return batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606dc271-ce66-41f0-9580-6feacae0fae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "operator_alias",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
