{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b41f01e-9bb8-4ef5-9e8b-35896d86188e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports\n",
    "\n",
    "- https://github.com/camlab-ethz/ConvolutionalNeuralOperator/tree/main/CNO2d_vanilla_torch_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660548ef-5a3d-4b9a-b0f7-3a0310dd9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from neuralop.models import FNO\n",
    "\n",
    "from operator_aliasing.models.cno2d import CNO2d\n",
    "from operator_aliasing.models.FNOModules import FNO2d\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd47e5-c52c-4d58-adcb-af2518b13714",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436c41fe-609c-4e2b-913c-19de9dbec3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/wenhangao21/ICLR25-CROP/blob/main/original_code_and_trained_models/Sec5_1_NS_with_low_Reynolds/CROP/CROP.py\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "#from utilities3 import *\n",
    "from timeit import default_timer\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "\n",
    "\n",
    "class Spectral_weights(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        dtype = torch.cfloat\n",
    "        self.kernel_size_Y = 2*modes1 - 1\n",
    "        self.kernel_size_X = modes2\n",
    "        self.W = nn.ParameterDict({\n",
    "            'y0_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, modes1 - 1, 1, dtype=dtype)),\n",
    "            'yposx_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, self.kernel_size_Y, self.kernel_size_X - 1, dtype=dtype)),\n",
    "            '00_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, 1, 1, dtype=torch.float))\n",
    "        })\n",
    "        self.eval_build = True\n",
    "        self.reset_parameters()\n",
    "        self.get_weight()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for v in self.W.values():\n",
    "            nn.init.kaiming_uniform_(v, a=math.sqrt(5))\n",
    "            \n",
    "    def get_weight(self):\n",
    "        if self.training:\n",
    "            self.eval_build = True\n",
    "        elif self.eval_build:\n",
    "            self.eval_build = False\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        self.weights = torch.cat([self.W[\"y0_modes\"], self.W[\"00_modes\"].cfloat(), self.W[\"y0_modes\"].flip(dims=(-2, )).conj()], dim=-2)\n",
    "        self.weights = torch.cat([self.weights, self.W[\"yposx_modes\"]], dim=-1)\n",
    "        self.weights = self.weights.view(self.in_channels, self.out_channels,\n",
    "                                         self.kernel_size_Y, self.kernel_size_X)\n",
    "        \n",
    "\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "        self.spectral_weight = Spectral_weights(in_channels=in_channels, out_channels=out_channels, modes1=modes1, modes2=modes2)\n",
    "        self.get_weight()\n",
    "\n",
    "    def get_weight(self):\n",
    "        self.spectral_weight.get_weight()\n",
    "        self.weights = self.spectral_weight.weights\n",
    "        \n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # get the index of the zero frequency and construct weight\n",
    "        freq0_y = (torch.fft.fftshift(torch.fft.fftfreq(x.shape[-2])) == 0).nonzero().item()\n",
    "        self.get_weight()\n",
    "        # Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.fftshift(torch.fft.rfft2(x), dim=-2)\n",
    "        x_ft = x_ft[..., (freq0_y - self.modes1 + 1):(freq0_y + self.modes1), :self.modes2]\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-2), x.size(-1) // 2 + 1, dtype=torch.cfloat,\n",
    "                             device=x.device)              \n",
    "        out_ft[..., (freq0_y - self.modes1 + 1):(freq0_y + self.modes1), :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft, self.weights)\n",
    "\n",
    "\n",
    "        # Return to physical space\n",
    "        x = torch.fft.irfft2(torch.fft.ifftshift(out_ft, dim=-2), s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp1 = nn.Conv2d(in_channels, mid_channels, 1)\n",
    "        self.mlp2 = nn.Conv2d(mid_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.mlp2(x)\n",
    "        return x\n",
    "        \n",
    "class Crop_to_latent_size(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(Crop_to_latent_size, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.temp_size = min(in_size, out_size)\n",
    "    def forward(self, u1):\n",
    "        B, C, _, _ = u1.shape\n",
    "        fu1 = torch.fft.rfft2(u1, norm=\"ortho\")\n",
    "        fu1_recover = torch.zeros((B, C, self.out_size, self.out_size // 2 + 1), dtype=torch.complex64, device=u1.device)\n",
    "        fu1_recover[:, :, :self.temp_size // 2, :self.temp_size // 2 + 1] = fu1[:, :, :self.temp_size // 2, :self.temp_size // 2 + 1]\n",
    "        fu1_recover[:, :, -self.temp_size // 2:, :self.temp_size // 2 + 1] = fu1[:, :, -self.temp_size // 2:, :self.temp_size // 2 + 1]\n",
    "        # Inverse FFT and scaling\n",
    "        u1_recover = torch.fft.irfft2(fu1_recover, norm=\"ortho\") * (self.out_size / self.in_size)\n",
    "        return u1_recover\n",
    "\n",
    "class CROP_FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2, width, in_size, latent_size, time_steps):\n",
    "        super(CROP_FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        input shape: (batchsize, x=64, y=64, c=12)\n",
    "        output: the solution of the next timestep\n",
    "        output shape: (batchsize, x=64, y=64, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.in_size = in_size\n",
    "        self.latent_size = latent_size\n",
    "        self.padding = 8 # pad the domain if input is non-periodic\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        self.CROP_to_latent = Crop_to_latent_size(self.in_size, self.latent_size)\n",
    "        self.CROP_back = Crop_to_latent_size(self.latent_size, self.in_size)\n",
    "        self.p = nn.Conv2d(self.time_steps, self.width, 1) # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.mlp0 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp1 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp2 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp3 = MLP(self.width, self.width, self.width)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.norm = nn.InstanceNorm2d(self.width)\n",
    "        self.q = MLP(self.width, 1, self.width * 4) # output channel is 1: u(x, y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.permute(0, 3, 1, 2)\n",
    "        print(\"Pre projection: \", x.shape)\n",
    "        x = self.CROP_to_latent(x)\n",
    "        print(\"Post projection: \", x.shape)\n",
    "        x = self.p(x)\n",
    "        # x = F.pad(x, [0,self.padding, 0,self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.norm(self.conv0(self.norm(x)))\n",
    "        x1 = self.mlp0(x1)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv1(self.norm(x)))\n",
    "        x1 = self.mlp1(x1)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv2(self.norm(x)))\n",
    "        x1 = self.mlp2(x1)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv3(self.norm(x)))\n",
    "        x1 = self.mlp3(x1)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "        print(\"Pre crop back: \", x.shape)\n",
    "        x = self.CROP_back(x)\n",
    "        print(\"Post crop back: \", x.shape)\n",
    "\n",
    "        # x = x[..., :-self.padding, :-self.padding] # pad the domain if input is non-periodic\n",
    "        x = self.q(x)\n",
    "        #x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float, device=device)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float, device=device)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe52cb88-cd8d-4169-8c25-0d5e93b49c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre projection:  torch.Size([2, 1, 128, 128])\n",
      "Post projection:  torch.Size([2, 1, 64, 64])\n",
      "Pre crop back:  torch.Size([2, 64, 64, 64])\n",
      "Post crop back:  torch.Size([2, 64, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes1 = 32 # modes have to be less than half the latent size\n",
    "modes2 = 16\n",
    "width = 64\n",
    "latent_size = 64 # Latent size has to be smaller or equal to smallest input dimension\n",
    "in_size = 128 # The in_size is dynamically changed for different input!\n",
    "batch_size = 2\n",
    "time_steps = 1\n",
    "\n",
    "crop_model = CROP_FNO2d(modes1, modes2, width, in_size, latent_size, time_steps).to(device)\n",
    "\n",
    "x_data = torch.rand((batch_size, time_steps, in_size, in_size, )).type(torch.float32).to(device)\n",
    "\n",
    "#x_data_transformed = torch.rand((32, 64, 64, 1)).type(torch.float32).to(device)\n",
    "\n",
    "crop_model(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150f3b06-a947-482b-8d1c-abc23055f00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488eb7a-e6f8-4b56-b01a-1b8306767918",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FNO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0647e853-7e0e-4213-abe5-2ee797ad0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adopted From:\n",
    "@author: Zongyi Li\n",
    "This file is the Fourier Neural Operator for 2D problem such as the Navier-Stokes equation discussed in Section 5.3 in the [paper](https://arxiv.org/pdf/2010.08895.pdf),\n",
    "which uses a recurrent structure to propagates in time.\n",
    "\"\"\"\n",
    "# https://github.com/wenhangao21/ICLR25-CROP/blob/main/original_code_and_trained_models/Sec5_1_NS_with_low_Reynolds/FNO/fno.py \n",
    "\n",
    "import torch.nn.functional as F\n",
    "from timeit import default_timer\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "\n",
    "\n",
    "class Spectral_weights(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        dtype = torch.cfloat\n",
    "        self.kernel_size_Y = 2*modes1 - 1\n",
    "        self.kernel_size_X = modes2\n",
    "        self.W = nn.ParameterDict({\n",
    "            'y0_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, modes1 - 1, 1, dtype=dtype)),\n",
    "            'yposx_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, self.kernel_size_Y, self.kernel_size_X - 1, dtype=dtype)),\n",
    "            '00_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, 1, 1, dtype=torch.float))\n",
    "        })\n",
    "        self.eval_build = True\n",
    "        self.reset_parameters()\n",
    "        self.get_weight()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for v in self.W.values():\n",
    "            nn.init.kaiming_uniform_(v, a=math.sqrt(5))\n",
    "            \n",
    "    def get_weight(self):\n",
    "        if self.training:\n",
    "            self.eval_build = True\n",
    "        elif self.eval_build:\n",
    "            self.eval_build = False\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        self.weights = torch.cat([self.W[\"y0_modes\"], self.W[\"00_modes\"].cfloat(), self.W[\"y0_modes\"].flip(dims=(-2, )).conj()], dim=-2)\n",
    "        self.weights = torch.cat([self.weights, self.W[\"yposx_modes\"]], dim=-1)\n",
    "        self.weights = self.weights.view(self.in_channels, self.out_channels,\n",
    "                                         self.kernel_size_Y, self.kernel_size_X)\n",
    "        \n",
    "\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "        self.spectral_weight = Spectral_weights(in_channels=in_channels, out_channels=out_channels, modes1=modes1, modes2=modes2)\n",
    "        self.get_weight()\n",
    "\n",
    "    def get_weight(self):\n",
    "        self.spectral_weight.get_weight()\n",
    "        self.weights = self.spectral_weight.weights\n",
    "        \n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # get the index of the zero frequency and construct weight\n",
    "        freq0_y = (torch.fft.fftshift(torch.fft.fftfreq(x.shape[-2])) == 0).nonzero().item()\n",
    "        self.get_weight()\n",
    "        # Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.fftshift(torch.fft.rfft2(x), dim=-2)\n",
    "        x_ft = x_ft[..., (freq0_y - self.modes1 + 1):(freq0_y + self.modes1), :self.modes2]\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-2), x.size(-1) // 2 + 1, dtype=torch.cfloat,\n",
    "                             device=x.device)              \n",
    "        out_ft[..., (freq0_y - self.modes1 + 1):(freq0_y + self.modes1), :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft, self.weights)\n",
    "\n",
    "\n",
    "        # Return to physical space\n",
    "        x = torch.fft.irfft2(torch.fft.ifftshift(out_ft, dim=-2), s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp1 = nn.Conv2d(in_channels, mid_channels, 1)\n",
    "        self.mlp2 = nn.Conv2d(mid_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.mlp2(x)\n",
    "        return x\n",
    "\n",
    "class FNO2d_custom(nn.Module):\n",
    "    def __init__(self, modes1, modes2, width):\n",
    "        super(FNO2d_custom, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        input shape: (batchsize, x=64, y=64, c=12)\n",
    "        output: the solution of the next timestep\n",
    "        output shape: (batchsize, x=64, y=64, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 8 # pad the domain if input is non-periodic\n",
    "\n",
    "        self.p = nn.Linear(12, self.width) # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.mlp0 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp1 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp2 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp3 = MLP(self.width, self.width, self.width)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.norm = nn.InstanceNorm2d(self.width)\n",
    "        self.q = MLP(self.width, 1, self.width * 4) # output channel is 1: u(x, y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.p(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        # x = F.pad(x, [0,self.padding, 0,self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.norm(self.conv0(self.norm(x)))\n",
    "        x1 = self.mlp0(x1)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv1(self.norm(x)))\n",
    "        x1 = self.mlp1(x1)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv2(self.norm(x)))\n",
    "        x1 = self.mlp2(x1)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv3(self.norm(x)))\n",
    "        x1 = self.mlp3(x1)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        # x = x[..., :-self.padding, :-self.padding] # pad the domain if input is non-periodic\n",
    "        x = self.q(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float, device=device)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float, device=device)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea30069-dc19-431b-b3b9-d37a47861171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 64, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes1 = 12\n",
    "modes2 = 12\n",
    "width = 20\n",
    "in_size = 32\n",
    "\n",
    "fno_model = FNO2d_custom(modes1, modes2, width).to(device)\n",
    "\n",
    "x_data = torch.rand((batch_size, 2*in_size, 2*in_size, 10)).type(torch.float32).to(device)\n",
    "\n",
    "#x_data_transformed = torch.rand((32, 64, 64, 1)).type(torch.float32).to(device)\n",
    "\n",
    "fno_model(x_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ac361-f5a0-4b76-9d91-b0dc10b22526",
   "metadata": {},
   "source": [
    "# Original CNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4555df63-d742-46ce-ac70-5e83bc07dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator_aliasing.models.CNO2d_original_version.CNOModule import CNO\n",
    "from operator_aliasing.models.CNO2d_original_version.training.filtered_networks import LReLu, LReLu_regular, LReLu_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e29f246-904b-4956-bd3a-5fa685a847db",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = LReLu_torch(in_channels =1, #In _channels is not used in these settings\n",
    "                                            out_channels         = 1,                   \n",
    "                                            in_size               = 32,                       \n",
    "                                            out_size              = 32,                       \n",
    "                                            in_sampling_rate      = 32,               \n",
    "                                            out_sampling_rate     = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a4a31c5-f530-471e-b1b3-f13764031e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_data = torch.rand((batch_size, time_steps, 2*in_size, 2*in_size, )).type(torch.float32).to(device)\n",
    "cno.lift.inter_CNOBlock.activation.activation(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a52ea07-bac6-4656-bfb8-39ff67d1e480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 80, 80])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes1 = 16\n",
    "modes2 = 16\n",
    "width = 64\n",
    "latent_size = 256\n",
    "in_size = 40\n",
    "batch_size = 2\n",
    "time_steps = 1\n",
    "\n",
    "x_data = torch.rand((batch_size, time_steps, 2*in_size, 2*in_size, )).type(torch.float32).to(device)\n",
    "\n",
    "cno = CNO(\n",
    "                 in_dim = time_steps,                    # Number of input channels.\n",
    "                 in_size=in_size,                   # Input spatial size\n",
    "                 N_layers=1,                  # Number of (D) or (U) blocks in the network\n",
    "                 N_res = 1,                 # Number of (R) blocks per level (except the neck)\n",
    "                 N_res_neck = 6,            # Number of (R) blocks in the neck\n",
    "                 channel_multiplier = 32,   # How the number of channels evolve?\n",
    "                 conv_kernel=3,             # Size of all the kernels\n",
    "                 cutoff_den = 2.0001,       # Filter property 1.\n",
    "                 filter_size=6,             # Filter property 2.\n",
    "                 lrelu_upsampling = 2,      # Filter property 3.\n",
    "                 half_width_mult  = 0.8,    # Filter property 4.\n",
    "                 radial = False,            # Filter property 5. Is filter radial?\n",
    "                 batch_norm = True,         # Add BN? We do not add BN in lifting/projection layer\n",
    "                 out_dim = 1,               # Target dimension\n",
    "                 out_size = 1,              # If out_size is 1, Then out_size = in_size. Else must be int\n",
    "                 expand_input = False,      # Start with original in_size, or expand it (pad zeros in the spectrum)\n",
    "                 latent_lift_proj_dim = 64, # Intermediate latent dimension in the lifting/projection layer\n",
    "                 add_inv = True,            # Add invariant block (I) after the intermediate connections?\n",
    "                 activation = 'cno_lrelu_torch'   # Activation function can be 'cno_lrelu' or 'lrelu'\n",
    "                #activation='lrelu'                \n",
    ").to(device)\n",
    "\n",
    "cno(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8467394a-0b41-4d38-932f-b6fab76d4c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-lift torch.Size([2, 1, 320, 320])\n",
      "pre upsample: torch.Size([2, 64, 320, 320])\n",
      "post upsample: torch.Size([2, 64, 160, 160])\n",
      "post downsample: torch.Size([2, 64, 80, 80])\n",
      "post downsample 2.0: torch.Size([2, 64, 80, 80])\n",
      "post-lift torch.Size([2, 16, 80, 80])\n",
      "pre-activation: torch.Size([2, 16, 80, 80])\n",
      "pre upsample: torch.Size([2, 16, 80, 80])\n",
      "post upsample: torch.Size([2, 16, 160, 160])\n",
      "post downsample: torch.Size([2, 16, 80, 80])\n",
      "post downsample 2.0: torch.Size([2, 16, 80, 80])\n",
      "post-activation: torch.Size([2, 16, 80, 80])\n",
      "pre upsample: torch.Size([2, 32, 80, 80])\n",
      "post upsample: torch.Size([2, 32, 160, 160])\n",
      "post downsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 16, 40, 40])\n",
      "post upsample: torch.Size([2, 16, 80, 80])\n",
      "post downsample: torch.Size([2, 16, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 16, 80, 80])\n",
      "pre upsample: torch.Size([2, 16, 80, 80])\n",
      "post upsample: torch.Size([2, 16, 160, 160])\n",
      "post downsample: torch.Size([2, 16, 80, 80])\n",
      "post downsample 2.0: torch.Size([2, 16, 80, 80])\n",
      "pre-projection torch.Size([2, 32, 80, 80])\n",
      "pre upsample: torch.Size([2, 64, 80, 80])\n",
      "post upsample: torch.Size([2, 64, 160, 160])\n",
      "post downsample: torch.Size([2, 64, 80, 80])\n",
      "post downsample 2.0: torch.Size([2, 64, 80, 80])\n",
      "post-projection torch.Size([2, 1, 80, 80])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 80, 80])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = torch.rand((batch_size, time_steps, in_size*4, in_size*4, )).type(torch.float32).to(device)\n",
    "cno(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28fc8cf6-20d5-4c0e-8d17-f7a7aec51766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the filters is borrowed from paper \"Alias-Free Generative Adversarial Networks (StyleGAN3)\" https://nvlabs.github.io/stylegan3/\n",
    "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
    "#\n",
    "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
    "# and proprietary rights in and to this software, related documentation\n",
    "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
    "# distribution of this software and related documentation without an express\n",
    "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from torch.nn import LeakyReLU as LReLu\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "#Depending on in_size, out_size, the CNOBlock can be:\n",
    "#   -- (D) Block\n",
    "#   -- (U) Block\n",
    "#   -- (I) Block\n",
    "\n",
    "class CNOBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 in_size,\n",
    "                 out_size,\n",
    "                 cutoff_den = 2.0001,\n",
    "                 conv_kernel = 3,\n",
    "                 filter_size = 6,\n",
    "                 lrelu_upsampling = 2,\n",
    "                 half_width_mult  = 0.8,\n",
    "                 radial = False,\n",
    "                 batch_norm = True,\n",
    "                 activation = 'cno_lrelu'\n",
    "                 ):\n",
    "        super(CNOBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.in_size  = in_size\n",
    "        self.out_size = out_size\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        #---------- Filter properties -----------\n",
    "        self.citically_sampled = False #We use w_c = s/2.0001 --> NOT critically sampled\n",
    "\n",
    "        if cutoff_den == 2.0:\n",
    "            self.citically_sampled = True\n",
    "        self.in_cutoff  = self.in_size / cutoff_den\n",
    "        self.out_cutoff = self.out_size / cutoff_den\n",
    "        \n",
    "        self.in_halfwidth =  half_width_mult*self.in_size - self.in_size / cutoff_den\n",
    "        self.out_halfwidth = half_width_mult*self.out_size - self.out_size / cutoff_den\n",
    "        \n",
    "        #-----------------------------------------\n",
    "\n",
    "        # We apply Conv -> BN (optional) -> Activation\n",
    "        # Up/Downsampling happens inside Activation\n",
    "        \n",
    "        pad = (self.conv_kernel-1)//2\n",
    "        self.convolution = torch.nn.Conv2d(in_channels = self.in_channels, out_channels=self.out_channels, \n",
    "                                           kernel_size=self.conv_kernel, \n",
    "                                           padding = pad)\n",
    "    \n",
    "        if self.batch_norm:\n",
    "            self.batch_norm  = nn.BatchNorm2d(self.out_channels)\n",
    "        self.activation = LReLu() \n",
    "        if activation == \"cno_lrelu_torch\":\n",
    "            self.activation = LReLu_torch(in_channels           = self.out_channels, #In _channels is not used in these settings\n",
    "                                            out_channels          = self.out_channels,                   \n",
    "                                            in_size               = self.in_size,                       \n",
    "                                            out_size              = self.out_size,                       \n",
    "                                            in_sampling_rate      = self.in_size,               \n",
    "                                            out_sampling_rate     = self.out_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convolution(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Contains CNOBlock -> Convolution -> BN\n",
    "\n",
    "class LiftProjectBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 in_size,\n",
    "                 out_size,\n",
    "                 latent_dim = 64,\n",
    "                 cutoff_den = 2.0001,\n",
    "                 conv_kernel = 3,\n",
    "                 filter_size = 6,\n",
    "                 lrelu_upsampling = 2,\n",
    "                 half_width_mult  = 0.8,\n",
    "                 radial = False,\n",
    "                 batch_norm = True,\n",
    "                 activation = 'cno_lrelu'\n",
    "                 ):\n",
    "        super(LiftProjectBlock, self).__init__()\n",
    "    \n",
    "        self.inter_CNOBlock = CNOBlock(in_channels = in_channels,\n",
    "                                    out_channels = latent_dim,\n",
    "                                    in_size = in_size,\n",
    "                                    out_size = out_size,\n",
    "                                    cutoff_den = cutoff_den,\n",
    "                                    conv_kernel = conv_kernel,\n",
    "                                    filter_size = filter_size,\n",
    "                                    lrelu_upsampling = lrelu_upsampling,\n",
    "                                    half_width_mult  = half_width_mult,\n",
    "                                    radial = radial,\n",
    "                                    batch_norm = batch_norm,\n",
    "                                    activation = activation)\n",
    "        \n",
    "        pad = (conv_kernel-1)//2\n",
    "        self.convolution = torch.nn.Conv2d(in_channels = latent_dim, out_channels=out_channels, \n",
    "                                           kernel_size=conv_kernel, stride = 1, \n",
    "                                           padding = pad)\n",
    "        \n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm  = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.inter_CNOBlock(x)\n",
    "        \n",
    "        x = self.convolution(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x)\n",
    "        return x\n",
    "        \n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Residual Block containts:\n",
    "    # Convolution -> BN -> Activation -> Convolution -> BN -> SKIP CONNECTION\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels,\n",
    "                 size,\n",
    "                 cutoff_den = 2.0001,\n",
    "                 conv_kernel = 3,\n",
    "                 filter_size = 6,\n",
    "                 lrelu_upsampling = 2,\n",
    "                 half_width_mult  = 0.8,\n",
    "                 radial = False,\n",
    "                 batch_norm = True,\n",
    "                 activation = 'cno_lrelu'\n",
    "                 ):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.size  = size\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        #---------- Filter properties -----------\n",
    "        self.citically_sampled = False #We use w_c = s/2.0001 --> NOT critically sampled\n",
    "\n",
    "        if cutoff_den == 2.0:\n",
    "            self.citically_sampled = True\n",
    "        self.cutoff  = self.size / cutoff_den        \n",
    "        self.halfwidth =  half_width_mult*self.size - self.size / cutoff_den\n",
    "        \n",
    "        #-----------------------------------------\n",
    "        \n",
    "        pad = (self.conv_kernel-1)//2\n",
    "        self.convolution1 = torch.nn.Conv2d(in_channels = self.channels, out_channels=self.channels, \n",
    "                                           kernel_size=self.conv_kernel, stride = 1, \n",
    "                                           padding = pad)\n",
    "        self.convolution2 = torch.nn.Conv2d(in_channels = self.channels, out_channels=self.channels, \n",
    "                                           kernel_size=self.conv_kernel, stride = 1, \n",
    "                                           padding = pad)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            self.batch_norm1  = nn.BatchNorm2d(self.channels)\n",
    "            self.batch_norm2  = nn.BatchNorm2d(self.channels)\n",
    "        self.activation = LReLu() \n",
    "        if activation == \"cno_lrelu_torch\":\n",
    "            print(\"custom activation\")\n",
    "            self.activation = LReLu_torch(in_channels           = self.channels, #In _channels is not used in these settings\n",
    "                                            out_channels          = self.channels,                   \n",
    "                                            in_size               = self.size,                       \n",
    "                                            out_size              = self.size,                       \n",
    "                                            in_sampling_rate      = self.size,               \n",
    "                                            out_sampling_rate     = self.size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution1(x)\n",
    "        if self.batch_norm:\n",
    "            out = self.batch_norm1(out)\n",
    "        print(\"pre-activation:\",out.shape)\n",
    "        out = self.activation(out)\n",
    "        print(\"post-activation:\",out.shape)\n",
    "        out = self.convolution2(out)\n",
    "        if self.batch_norm:\n",
    "            out = self.batch_norm2(out)\n",
    "        \n",
    "        return x + out\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "#CNO NETWORK:\n",
    "class CNO(nn.Module):\n",
    "    def __init__(self,  \n",
    "                 in_dim,                    # Number of input channels.\n",
    "                 in_size,                   # Input spatial size\n",
    "                 N_layers,                  # Number of (D) or (U) blocks in the network\n",
    "                 N_res = 1,                 # Number of (R) blocks per level (except the neck)\n",
    "                 N_res_neck = 6,            # Number of (R) blocks in the neck\n",
    "                 channel_multiplier = 32,   # How the number of channels evolve?\n",
    "                 conv_kernel=3,             # Size of all the kernels\n",
    "                 cutoff_den = 2.0001,       # Filter property 1.\n",
    "                 filter_size=6,             # Filter property 2.\n",
    "                 lrelu_upsampling = 2,      # Filter property 3.\n",
    "                 half_width_mult  = 0.8,    # Filter property 4.\n",
    "                 radial = False,            # Filter property 5. Is filter radial?\n",
    "                 batch_norm = True,         # Add BN? We do not add BN in lifting/projection layer\n",
    "                 out_dim = 1,               # Target dimension\n",
    "                 out_size = 1,              # If out_size is 1, Then out_size = in_size. Else must be int\n",
    "                 expand_input = False,      # Start with original in_size, or expand it (pad zeros in the spectrum)\n",
    "                 latent_lift_proj_dim = 64, # Intermediate latent dimension in the lifting/projection layer\n",
    "                 add_inv = True,            # Add invariant block (I) after the intermediate connections?\n",
    "                 activation = 'cno_lrelu'   # Activation function can be 'cno_lrelu' or 'lrelu'\n",
    "                ):\n",
    "        \n",
    "        super(CNO, self).__init__()\n",
    "\n",
    "        ###################### Define the parameters & specifications #################################################\n",
    "\n",
    "        \n",
    "        # Number od (D) & (U) Blocks\n",
    "        self.N_layers = int(N_layers)\n",
    "        \n",
    "        # Input is lifted to the half on channel_multiplier dimension\n",
    "        self.lift_dim = channel_multiplier//2         \n",
    "        self.out_dim  = out_dim\n",
    "        \n",
    "        #Should we add invariant layers in the decoder?\n",
    "        self.add_inv = add_inv\n",
    "        \n",
    "        # The growth of the channels : d_e parametee\n",
    "        self.channel_multiplier = channel_multiplier        \n",
    "        \n",
    "        # Is the filter radial? We always use NOT radial\n",
    "        if radial ==0:\n",
    "            self.radial = False\n",
    "        else:\n",
    "            self.radial = True\n",
    "        \n",
    "        ###################### Define evolution of the number features ################################################\n",
    "\n",
    "        # How the features in Encoder evolve (number of features)\n",
    "        self.encoder_features = [self.lift_dim]\n",
    "        for i in range(self.N_layers):\n",
    "            self.encoder_features.append(2 ** i *   self.channel_multiplier)\n",
    "        \n",
    "        # How the features in Decoder evolve (number of features)\n",
    "        self.decoder_features_in = self.encoder_features[1:]\n",
    "        self.decoder_features_in.reverse()\n",
    "        self.decoder_features_out = self.encoder_features[:-1]\n",
    "        self.decoder_features_out.reverse()\n",
    "\n",
    "        for i in range(1, self.N_layers):\n",
    "            self.decoder_features_in[i] = 2*self.decoder_features_in[i] #Pad the outputs of the resnets\n",
    "        \n",
    "        self.inv_features = self.decoder_features_in\n",
    "        self.inv_features.append(self.encoder_features[0] + self.decoder_features_out[-1])\n",
    "        \n",
    "        ###################### Define evolution of sampling rates #####################################################\n",
    "        \n",
    "        if not expand_input:\n",
    "            latent_size = in_size # No change in in_size\n",
    "        else:\n",
    "            down_exponent = 2 ** N_layers\n",
    "            latent_size = in_size - (in_size % down_exponent) + down_exponent # Jump from 64 to 72, for example\n",
    "        \n",
    "        #Are inputs and outputs of the same size? If not, how should the size of the decoder evolve?\n",
    "        if out_size == 1:\n",
    "            latent_size_out = latent_size\n",
    "        else:\n",
    "            if not expand_input:\n",
    "                latent_size_out = out_size # No change in in_size\n",
    "            else:\n",
    "                down_exponent = 2 ** N_layers\n",
    "                latent_size_out = out_size - (out_size % down_exponent) + down_exponent # Jump from 64 to 72, for example\n",
    "        \n",
    "        self.encoder_sizes = []\n",
    "        self.decoder_sizes = []\n",
    "        for i in range(self.N_layers + 1):\n",
    "            self.encoder_sizes.append(latent_size // 2 ** i)\n",
    "            self.decoder_sizes.append(latent_size_out // 2 ** (self.N_layers - i))\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################### Define Projection & Lift ##############################################################\n",
    "    \n",
    "        self.lift = LiftProjectBlock(in_channels  = in_dim,\n",
    "                                     out_channels = self.encoder_features[0],\n",
    "                                     in_size      = in_size,\n",
    "                                     out_size     = self.encoder_sizes[0],\n",
    "                                     latent_dim   = latent_lift_proj_dim,\n",
    "                                     cutoff_den   = cutoff_den,\n",
    "                                     conv_kernel  = conv_kernel,\n",
    "                                     filter_size  = filter_size,\n",
    "                                     lrelu_upsampling  = lrelu_upsampling,\n",
    "                                     half_width_mult   = half_width_mult,\n",
    "                                     radial            = radial,\n",
    "                                     batch_norm        = False,\n",
    "                                     activation = activation)\n",
    "        _out_size = out_size\n",
    "        if out_size == 1:\n",
    "            _out_size = in_size\n",
    "            \n",
    "        self.project = LiftProjectBlock(in_channels  = self.encoder_features[0] + self.decoder_features_out[-1],\n",
    "                                        out_channels = out_dim,\n",
    "                                        in_size      = self.decoder_sizes[-1],\n",
    "                                        out_size     = _out_size,\n",
    "                                        latent_dim   = latent_lift_proj_dim,\n",
    "                                        cutoff_den   = cutoff_den,\n",
    "                                        conv_kernel  = conv_kernel,\n",
    "                                        filter_size  = filter_size,\n",
    "                                        lrelu_upsampling  = lrelu_upsampling,\n",
    "                                        half_width_mult   = half_width_mult,\n",
    "                                        radial            = radial,\n",
    "                                        batch_norm        = False,\n",
    "                                        activation = activation) \n",
    "\n",
    "        ###################### Define U & D blocks ###################################################################\n",
    "\n",
    "        self.encoder         = nn.ModuleList([(CNOBlock(in_channels  = self.encoder_features[i],\n",
    "                                                        out_channels = self.encoder_features[i+1],\n",
    "                                                        in_size      = self.encoder_sizes[i],\n",
    "                                                        out_size     = self.encoder_sizes[i+1],\n",
    "                                                        cutoff_den   = cutoff_den,\n",
    "                                                        conv_kernel  = conv_kernel,\n",
    "                                                        filter_size  = filter_size,\n",
    "                                                        lrelu_upsampling = lrelu_upsampling,\n",
    "                                                        half_width_mult  = half_width_mult,\n",
    "                                                        radial = radial,\n",
    "                                                        batch_norm = batch_norm,\n",
    "                                                        activation = activation))                                  \n",
    "                                               for i in range(self.N_layers)])\n",
    "        \n",
    "        # After the ResNets are executed, the sizes of encoder and decoder might not match (if out_size>1)\n",
    "        # We must ensure that the sizes are the same, by aplying CNO Blocks\n",
    "        self.ED_expansion     = nn.ModuleList([(CNOBlock(in_channels = self.encoder_features[i],\n",
    "                                                        out_channels = self.encoder_features[i],\n",
    "                                                        in_size      = self.encoder_sizes[i],\n",
    "                                                        out_size     = self.decoder_sizes[self.N_layers - i],\n",
    "                                                        cutoff_den   = cutoff_den,\n",
    "                                                        conv_kernel  = conv_kernel,\n",
    "                                                        filter_size  = filter_size,\n",
    "                                                        lrelu_upsampling = lrelu_upsampling,\n",
    "                                                        half_width_mult  = half_width_mult,\n",
    "                                                        radial = radial,\n",
    "                                                        batch_norm = batch_norm,\n",
    "                                                        activation = activation))                                  \n",
    "                                               for i in range(self.N_layers + 1)])\n",
    "        \n",
    "        self.decoder         = nn.ModuleList([(CNOBlock(in_channels  = self.decoder_features_in[i],\n",
    "                                                        out_channels = self.decoder_features_out[i],\n",
    "                                                        in_size      = self.decoder_sizes[i],\n",
    "                                                        out_size     = self.decoder_sizes[i+1],\n",
    "                                                        cutoff_den   = cutoff_den,\n",
    "                                                        conv_kernel  = conv_kernel,\n",
    "                                                        filter_size  = filter_size,\n",
    "                                                        lrelu_upsampling = lrelu_upsampling,\n",
    "                                                        half_width_mult  = half_width_mult,\n",
    "                                                        radial = radial,\n",
    "                                                        batch_norm = batch_norm,\n",
    "                                                        activation = activation))                                  \n",
    "                                               for i in range(self.N_layers)])\n",
    "        \n",
    "        \n",
    "        self.decoder_inv    = nn.ModuleList([(CNOBlock(in_channels  =  self.inv_features[i],\n",
    "                                                        out_channels = self.inv_features[i],\n",
    "                                                        in_size      = self.decoder_sizes[i],\n",
    "                                                        out_size     = self.decoder_sizes[i],\n",
    "                                                        cutoff_den   = cutoff_den,\n",
    "                                                        conv_kernel  = conv_kernel,\n",
    "                                                        filter_size  = filter_size,\n",
    "                                                        lrelu_upsampling = lrelu_upsampling,\n",
    "                                                        half_width_mult  = half_width_mult,\n",
    "                                                        radial = radial,\n",
    "                                                        batch_norm = batch_norm,\n",
    "                                                        activation = activation))                                  \n",
    "                                               for i in range(self.N_layers + 1)])\n",
    "        \n",
    "        \n",
    "        ####################### Define ResNets Blocks ################################################################\n",
    "\n",
    "        # Here, we define ResNet Blocks. \n",
    "        # We also define the BatchNorm layers applied BEFORE the ResNet blocks \n",
    "        \n",
    "        # Operator UNet:\n",
    "        # Outputs of the middle networks are patched (or padded) to corresponding sets of feature maps in the decoder \n",
    "\n",
    "        self.res_nets = []\n",
    "        self.N_res = int(N_res)\n",
    "        self.N_res_neck = int(N_res_neck)\n",
    "\n",
    "        # Define the ResNet blocks & BatchNorm\n",
    "        for l in range(self.N_layers):\n",
    "            for i in range(self.N_res):\n",
    "                self.res_nets.append(ResidualBlock(channels = self.encoder_features[l],\n",
    "                                                   size     = self.encoder_sizes[l],\n",
    "                                                   cutoff_den = cutoff_den,\n",
    "                                                   conv_kernel = conv_kernel,\n",
    "                                                   filter_size = filter_size,\n",
    "                                                   lrelu_upsampling = lrelu_upsampling,\n",
    "                                                   half_width_mult  = half_width_mult,\n",
    "                                                   radial = radial,\n",
    "                                                   batch_norm = batch_norm,\n",
    "                                                   activation = activation))\n",
    "        for i in range(self.N_res_neck):\n",
    "            self.res_nets.append(ResidualBlock(channels = self.encoder_features[self.N_layers],\n",
    "                                               size     = self.encoder_sizes[self.N_layers],\n",
    "                                               cutoff_den = cutoff_den,\n",
    "                                               conv_kernel = conv_kernel,\n",
    "                                               filter_size = filter_size,\n",
    "                                               lrelu_upsampling = lrelu_upsampling,\n",
    "                                               half_width_mult  = half_width_mult,\n",
    "                                               radial = radial,\n",
    "                                               batch_norm = batch_norm,\n",
    "                                               activation = activation))\n",
    "        \n",
    "        self.res_nets = torch.nn.Sequential(*self.res_nets)    \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "                 \n",
    "        #Execute Lift ---------------------------------------------------------\n",
    "        print(\"pre-lift\", x.shape)\n",
    "        x = self.lift(x)\n",
    "        print(\"post-lift\", x.shape)\n",
    "        skip = []\n",
    "        \n",
    "        # Execute Encoder -----------------------------------------------------\n",
    "        for i in range(self.N_layers):\n",
    "            \n",
    "            #Apply ResNet & save the result\n",
    "            y = x\n",
    "            for j in range(self.N_res):\n",
    "                y = self.res_nets[i*self.N_res + j](y)\n",
    "            skip.append(y)\n",
    "            \n",
    "            # Apply (D) block\n",
    "            x = self.encoder[i](x)   \n",
    "        \n",
    "        #----------------------------------------------------------------------\n",
    "        \n",
    "        # Apply the deepest ResNet (bottle neck)\n",
    "        for j in range(self.N_res_neck):\n",
    "            x = self.res_nets[-j-1](x)\n",
    "\n",
    "        # Execute Decoder -----------------------------------------------------\n",
    "        for i in range(self.N_layers):\n",
    "            \n",
    "            # Apply (I) block (ED_expansion) & cat if needed\n",
    "            if i == 0:\n",
    "                x = self.ED_expansion[self.N_layers - i](x) #BottleNeck : no cat\n",
    "            else:\n",
    "                x = torch.cat((x, self.ED_expansion[self.N_layers - i](skip[-i])),1)\n",
    "            \n",
    "            if self.add_inv:\n",
    "                x = self.decoder_inv[i](x)\n",
    "            # Apply (U) block\n",
    "            x = self.decoder[i](x)\n",
    "        # Cat & Execute Projetion ---------------------------------------------\n",
    "        \n",
    "        x = torch.cat((x, self.ED_expansion[0](skip[0])),1)\n",
    "        print(\"pre-projection\", x.shape)\n",
    "        x = self.project(x)\n",
    "        print(\"post-projection\", x.shape)\n",
    "        \n",
    "        del skip\n",
    "        del y\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def get_n_params(self):\n",
    "        pp = 0\n",
    "        \n",
    "        for p in list(self.parameters()):\n",
    "            nn = 1\n",
    "            for s in list(p.size()):\n",
    "                nn = nn * s\n",
    "            pp += nn\n",
    "        return pp\n",
    "\n",
    "    def print_size(self):\n",
    "        nparams = 0\n",
    "        nbytes = 0\n",
    "\n",
    "        for param in self.parameters():\n",
    "            nparams += param.numel()\n",
    "            nbytes += param.data.element_size() * param.numel()\n",
    "\n",
    "        print(f'Total number of model parameters: {nparams}')\n",
    "\n",
    "        return nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c31d42f-7842-4657-871f-2c3505c35b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-lift torch.Size([2, 1, 108, 108])\n",
      "post-lift torch.Size([2, 16, 108, 108])\n",
      "pre-activation: torch.Size([2, 16, 108, 108])\n",
      "post-activation: torch.Size([2, 16, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-projection torch.Size([2, 32, 108, 108])\n",
      "post-projection torch.Size([2, 1, 108, 108])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 108, 108])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes1 = 16\n",
    "modes2 = 16\n",
    "width = 64\n",
    "in_size = 27\n",
    "batch_size = 2\n",
    "time_steps = 1\n",
    "\n",
    "x_data = torch.rand((batch_size, time_steps, 2*in_size* 2, 2*in_size*2, )).type(torch.float32).to(device)\n",
    "\n",
    "cno = CNO(\n",
    "                 in_dim = time_steps,                    # Number of input channels.\n",
    "                 in_size=in_size,                   # Input spatial size\n",
    "                 N_layers=1,                  # Number of (D) or (U) blocks in the network\n",
    "                 N_res = 1,                 # Number of (R) blocks per level (except the neck)\n",
    "                 N_res_neck = 6,            # Number of (R) blocks in the neck\n",
    "                 channel_multiplier = 32,   # How the number of channels evolve?\n",
    "                 conv_kernel=3,             # Size of all the kernels\n",
    "                 cutoff_den = 2.0001,       # Filter property 1.\n",
    "                 filter_size=6,             # Filter property 2.\n",
    "                 lrelu_upsampling = 2,      # Filter property 3.\n",
    "                 half_width_mult  = 0.8,    # Filter property 4.\n",
    "                 radial = False,            # Filter property 5. Is filter radial?\n",
    "                 batch_norm = True,         # Add BN? We do not add BN in lifting/projection layer\n",
    "                 out_dim = 1,               # Target dimension\n",
    "                 out_size = 1,              # If out_size is 1, Then out_size = in_size. Else must be int\n",
    "                 expand_input = False,      # Start with original in_size, or expand it (pad zeros in the spectrum)\n",
    "                 latent_lift_proj_dim = 38, # Intermediate latent dimension in the lifting/projection layer (channels)\n",
    "                 add_inv = True,            # Add invariant block (I) after the intermediate connections?\n",
    "                 activation = 'cno_lrelu'   # Activation function can be 'cno_lrelu' or 'lrelu'\n",
    "                ).to(device)\n",
    "\n",
    "cno(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07dd13c7-e1b4-492c-b193-229c7033cbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 108, 108])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cd4493a-bf30-4566-9e7e-c4dc81edd46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = torch.rand((batch_size, time_steps, 128, 128, )).type(torch.float32).to(device)\n",
    "cno.lift.inter_CNOBlock.activation(x_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dffb17-f4c9-43ef-a143-7cb41e5f632c",
   "metadata": {},
   "source": [
    "# Multi-resolution inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b98d1ae-d72c-4366-8f20-13805df26776",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.rand((32, 1, 128, 128)).type(torch.float32).to(device)\n",
    "y_data = torch.ones((32, 1, 128, 128)).type(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60ba03e0-ac10-4832-b050-6fe9e0d2556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_layers = 1\n",
    "N_res = 4\n",
    "N_res_neck = 4\n",
    "channel_multiplier = 16\n",
    "\n",
    "s = 128\n",
    "\n",
    "model_args = {\n",
    "    'in_dim': 1,  # Number of input channels.\n",
    "    'out_dim': 1,  # Number of output channels.\n",
    "    #'size': s,  # Input and Output spatial size (required )\n",
    "    'n_layers': N_layers,  # Number of (D) or (U) blocks in the network\n",
    "    'n_res': N_res,  # Number of (R) blocks per level (except the neck)\n",
    "    'n_res_neck': N_res_neck,  # Number of (R) blocks in the neck\n",
    "    'channel_multiplier': channel_multiplier,  # How num channels evolve?\n",
    "    'use_bn': False,\n",
    "}\n",
    "cno = CNO2d(**model_args)\n",
    "cno = cno.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeaa06b-b6d7-4a7b-8760-e13cb8d1f0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edc0fb5a-7f09-445b-ba80-879dfc513b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_modes = 16\n",
    "starting_modes = (max_modes, max_modes)\n",
    "model = FNO(\n",
    "    max_n_modes=(max_modes, max_modes),\n",
    "    n_modes=starting_modes,\n",
    "    hidden_channels=32,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a53c54-ff1c-48c5-b75d-b20f62eaa869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 64, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fno_architecture = {\n",
    "    'width': 64,\n",
    "    'modes': 16,\n",
    "    'FourierF': 0,  # Num Fourier Features in the input channels. Default is 0.\n",
    "    'n_layers': 4,  # Number of Fourier layers\n",
    "    'padding': 0,\n",
    "    'include_grid': 0,\n",
    "    'retrain': 4,  # Random seed\n",
    "}\n",
    "\n",
    "fno = FNO2d(fno_architecture, device=device).to(device)\n",
    "\n",
    "x_data_transformed = torch.rand((32, 64, 64, 1)).type(torch.float32).to(device)\n",
    "\n",
    "fno(x_data_transformed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18a4bb01-f31d-4cc5-a975-7b75c4a69a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 128, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efab6a55-6c71-482b-b348-b11bc4d5a9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac531f1-6e07-4f1f-a5f7-f3806ac1ff75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 128, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cno(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca9727f1-0933-4977-9666-0674ac7f1c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 64, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_small = torch.rand((32, 1, 64, 64)).type(torch.float32).to(device)\n",
    "y_data_small = torch.ones((32, 1, 64, 64)).type(torch.float32).to(device)\n",
    "cno(x_data_small).shape\n",
    "\n",
    "# model(x_data_small).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b074778c-5bd4-41a9-b02f-fb07f61020fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256, 256])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_large = torch.rand((1, 1, 256, 256)).type(torch.float32).to(device)\n",
    "y_data_large = torch.ones((1, 1, 256, 256)).type(torch.float32).to(device)\n",
    "cno(x_data_large).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2160caf-b28d-4410-9930-f337806923d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 0  ######### Train Loss: 0.9055739879608155  ######### Relative L1 Test Norm: 57.7748908996582\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 1  ######### Train Loss: 0.25167717672884465  ######### Relative L1 Test Norm: 52.63320207595825\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 2  ######### Train Loss: 0.1598238818347454  ######### Relative L1 Test Norm: 21.002609848976135\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 3  ######### Train Loss: 0.0730876948684454  ######### Relative L1 Test Norm: 20.69439399242401\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 4  ######### Train Loss: 0.03981395326554775  ######### Relative L1 Test Norm: 14.589525878429413\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 5  ######### Train Loss: 0.02466009221971035  ######### Relative L1 Test Norm: 15.191275537014008\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 6  ######### Train Loss: 0.01775022381916642  ######### Relative L1 Test Norm: 11.939327001571655\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 7  ######### Train Loss: 0.010724165989086033  ######### Relative L1 Test Norm: 11.165721952915192\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 8  ######### Train Loss: 0.019835104001685977  ######### Relative L1 Test Norm: 20.73923969268799\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 9  ######### Train Loss: 0.03328498362097889  ######### Relative L1 Test Norm: 14.320962846279144\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 10  ######### Train Loss: 0.019196551712229847  ######### Relative L1 Test Norm: 5.934997051954269\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 11  ######### Train Loss: 0.013836816023103892  ######### Relative L1 Test Norm: 16.565542936325073\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 12  ######### Train Loss: 0.0204890004824847  ######### Relative L1 Test Norm: 5.3831963539123535\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 13  ######### Train Loss: 0.016350756445899606  ######### Relative L1 Test Norm: 8.301013350486755\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 14  ######### Train Loss: 0.04209347409196198  ######### Relative L1 Test Norm: 22.714921236038208\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 15  ######### Train Loss: 0.023178669367916883  ######### Relative L1 Test Norm: 14.617516994476318\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 16  ######### Train Loss: 0.01646250020712614  ######### Relative L1 Test Norm: 15.028542816638947\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 17  ######### Train Loss: 0.014920321735553443  ######### Relative L1 Test Norm: 7.615252673625946\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 18  ######### Train Loss: 0.014300728915259242  ######### Relative L1 Test Norm: 14.405225813388824\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 19  ######### Train Loss: 0.019231191952712834  ######### Relative L1 Test Norm: 5.602485746145248\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 20  ######### Train Loss: 0.007936568465083838  ######### Relative L1 Test Norm: 8.887517511844635\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 21  ######### Train Loss: 0.007859216444194317  ######### Relative L1 Test Norm: 7.601627886295319\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 22  ######### Train Loss: 0.00781525990460068  ######### Relative L1 Test Norm: 11.896973729133606\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 23  ######### Train Loss: 0.007785650342702866  ######### Relative L1 Test Norm: 9.838278353214264\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 24  ######### Train Loss: 0.0070966253988444805  ######### Relative L1 Test Norm: 7.581246435642242\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 25  ######### Train Loss: 0.003634190559387207  ######### Relative L1 Test Norm: 5.930107891559601\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 26  ######### Train Loss: 0.0030647005885839464  ######### Relative L1 Test Norm: 5.152249783277512\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 27  ######### Train Loss: 0.002791524026542902  ######### Relative L1 Test Norm: 4.967260152101517\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 28  ######### Train Loss: 0.002452390524558723  ######### Relative L1 Test Norm: 5.203385561704636\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 29  ######### Train Loss: 0.002589865354821086  ######### Relative L1 Test Norm: 5.083554685115814\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 30  ######### Train Loss: 0.002473056106828153  ######### Relative L1 Test Norm: 5.031180769205093\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 31  ######### Train Loss: 0.0024922051932662727  ######### Relative L1 Test Norm: 5.007603973150253\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 32  ######### Train Loss: 0.002501706825569272  ######### Relative L1 Test Norm: 4.987777262926102\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 33  ######### Train Loss: 0.0024460564134642484  ######### Relative L1 Test Norm: 5.030503153800964\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 34  ######### Train Loss: 0.002482384000904858  ######### Relative L1 Test Norm: 4.898721486330032\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 35  ######### Train Loss: 0.0024414320243522523  ######### Relative L1 Test Norm: 4.857206642627716\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 36  ######### Train Loss: 0.0024523244705051185  ######### Relative L1 Test Norm: 4.832018822431564\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 37  ######### Train Loss: 0.0024294022703543307  ######### Relative L1 Test Norm: 5.031657665967941\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 38  ######### Train Loss: 0.002463870961219072  ######### Relative L1 Test Norm: 5.045852154493332\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 39  ######### Train Loss: 0.002392797847278416  ######### Relative L1 Test Norm: 4.860679030418396\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 40  ######### Train Loss: 0.0022901520365849136  ######### Relative L1 Test Norm: 4.6938157081604\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 41  ######### Train Loss: 0.002199833421036601  ######### Relative L1 Test Norm: 4.678664833307266\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 42  ######### Train Loss: 0.0021741229575127363  ######### Relative L1 Test Norm: 4.649824023246765\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 43  ######### Train Loss: 0.0021493012318387628  ######### Relative L1 Test Norm: 4.62138819694519\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 44  ######### Train Loss: 0.0021238300716504453  ######### Relative L1 Test Norm: 4.5931116938591\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 45  ######### Train Loss: 0.0021034441655501724  ######### Relative L1 Test Norm: 4.578712552785873\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 46  ######### Train Loss: 0.002089642523787916  ######### Relative L1 Test Norm: 4.563655346632004\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 47  ######### Train Loss: 0.002075689984485507  ######### Relative L1 Test Norm: 4.548003762960434\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 48  ######### Train Loss: 0.002061313670128584  ######### Relative L1 Test Norm: 4.532163918018341\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 49  ######### Train Loss: 0.0020467838272452354  ######### Relative L1 Test Norm: 4.515728771686554\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# --------------------------------------\n",
    "# REPLACE THIS PART BY YOUR DATALOADER\n",
    "# --------------------------------------\n",
    "\n",
    "n_train = 100  # number of training samples\n",
    "\n",
    "x_data = (\n",
    "    torch.rand((256, 1, 128, 128))\n",
    "    .type(torch.float32)\n",
    "    .reshape(256, 128, 128, 1)\n",
    ")\n",
    "y_data = (\n",
    "    torch.ones((256, 1, 128, 128))\n",
    "    .type(torch.float32)\n",
    "    .reshape(256, 128, 128, 1)\n",
    ")\n",
    "\n",
    "input_function_train = x_data[:n_train, :]\n",
    "output_function_train = y_data[:n_train, :]\n",
    "input_function_test = x_data[n_train:, :]\n",
    "output_function_test = y_data[n_train:, :]\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "training_set = DataLoader(\n",
    "    TensorDataset(input_function_train, output_function_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "testing_set = DataLoader(\n",
    "    TensorDataset(input_function_test, output_function_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# Define the hyperparameters and the model:\n",
    "# ---------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "step_size = 15\n",
    "gamma = 0.5\n",
    "\n",
    "N_layers = 4\n",
    "N_res = 4\n",
    "N_res_neck = 4\n",
    "channel_multiplier = 16\n",
    "\n",
    "s = 128\n",
    "\n",
    "cno = CNO2d(**model_args)\n",
    "\n",
    "# -----------\n",
    "# TRAIN:\n",
    "# -----------\n",
    "\n",
    "cno = cno.to(device)\n",
    "\n",
    "# cno = model.to(device)\n",
    "\n",
    "cno = fno.to(device)\n",
    "\n",
    "optimizer = AdamW(cno.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=step_size, gamma=gamma\n",
    ")\n",
    "\n",
    "loss = nn.L1Loss()\n",
    "freq_print = 1\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_mse = 0.0\n",
    "    for _step, (input_data, output_data) in enumerate(training_set):\n",
    "        input_batch = input_data.to(device)\n",
    "        output_batch = output_data.to(device)\n",
    "        print(input_batch.shape)\n",
    "        optimizer.zero_grad()\n",
    "        output_pred_batch = cno(input_batch)\n",
    "        loss_f = loss(output_pred_batch, output_batch)\n",
    "        loss_f.backward()\n",
    "        optimizer.step()\n",
    "        train_mse += loss_f.item()\n",
    "    train_mse /= len(training_set)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cno.eval()\n",
    "        test_relative_l2 = 0.0\n",
    "        for _step, (input_data, output_data) in enumerate(testing_set):\n",
    "            input_batch = input_data.to(device)\n",
    "            output_batch = output_data.to(device)\n",
    "            output_pred_batch = cno(input_batch)\n",
    "            loss_f = (\n",
    "                torch.mean(abs(output_pred_batch - output_batch))\n",
    "                / torch.mean(abs(output_batch))\n",
    "            ) ** 0.5 * 100\n",
    "            test_relative_l2 += loss_f.item()\n",
    "        test_relative_l2 /= len(testing_set)\n",
    "\n",
    "    if epoch % freq_print == 0:\n",
    "        print(\n",
    "            '######### Epoch:',\n",
    "            epoch,\n",
    "            ' ######### Train Loss:',\n",
    "            train_mse,\n",
    "            ' ######### Relative L1 Test Norm:',\n",
    "            test_relative_l2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d46ebe86-8493-48a9-910c-2398603c2476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_preds(\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    # data_transform: DataProcessor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Return model predictions.\"\"\"\n",
    "    model_preds = []\n",
    "    for _idx, sample in enumerate(test_loader):  # resolution 128\n",
    "        model_input = sample['x'].to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(model_input)\n",
    "            model_preds.append(out)\n",
    "    return torch.cat(model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef00da32-4c24-42a2-872c-af6c0c891123",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _idx, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtest_loader\u001b[49m):  \u001b[38;5;66;03m# resolution 128\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for _idx, sample in enumerate(test_loader):  # resolution 128\n",
    "    model_input = sample['x'].to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(model_input)\n",
    "        model_preds.append(out)\n",
    "return torch.cat(model_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "operator_alias",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
