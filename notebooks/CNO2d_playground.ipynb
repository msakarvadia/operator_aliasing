{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b41f01e-9bb8-4ef5-9e8b-35896d86188e",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "- https://github.com/camlab-ethz/ConvolutionalNeuralOperator/tree/main/CNO2d_vanilla_torch_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660548ef-5a3d-4b9a-b0f7-3a0310dd9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from neuralop.models import FNO\n",
    "\n",
    "from operator_aliasing.models.cno2d import CNO2d\n",
    "from operator_aliasing.models.cno1d import CNO1d\n",
    "from operator_aliasing.models.FNOModules import FNO2d\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd47e5-c52c-4d58-adcb-af2518b13714",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436c41fe-609c-4e2b-913c-19de9dbec3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/wenhangao21/ICLR25-CROP/blob/main/original_code_and_trained_models/Sec5_1_NS_with_low_Reynolds/CROP/CROP.py\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "#from utilities3 import *\n",
    "from timeit import default_timer\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "\n",
    "\n",
    "class Spectral_weights(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        dtype = torch.cfloat\n",
    "        self.kernel_size_Y = 2*modes1 - 1\n",
    "        self.kernel_size_X = modes2\n",
    "        self.W = nn.ParameterDict({\n",
    "            'y0_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, modes1 - 1, 1, dtype=dtype)),\n",
    "            'yposx_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, self.kernel_size_Y, self.kernel_size_X - 1, dtype=dtype)),\n",
    "            '00_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, 1, 1, dtype=torch.float))\n",
    "        })\n",
    "        self.eval_build = True\n",
    "        self.reset_parameters()\n",
    "        self.get_weight()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for v in self.W.values():\n",
    "            nn.init.kaiming_uniform_(v, a=math.sqrt(5))\n",
    "            \n",
    "    def get_weight(self):\n",
    "        if self.training:\n",
    "            self.eval_build = True\n",
    "        elif self.eval_build:\n",
    "            self.eval_build = False\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        self.weights = torch.cat([self.W[\"y0_modes\"], self.W[\"00_modes\"].cfloat(), self.W[\"y0_modes\"].flip(dims=(-2, )).conj()], dim=-2)\n",
    "        self.weights = torch.cat([self.weights, self.W[\"yposx_modes\"]], dim=-1)\n",
    "        self.weights = self.weights.view(self.in_channels, self.out_channels,\n",
    "                                         self.kernel_size_Y, self.kernel_size_X)\n",
    "        \n",
    "\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "        self.spectral_weight = Spectral_weights(in_channels=in_channels, out_channels=out_channels, modes1=modes1, modes2=modes2)\n",
    "        self.get_weight()\n",
    "\n",
    "    def get_weight(self):\n",
    "        self.spectral_weight.get_weight()\n",
    "        self.weights = self.spectral_weight.weights\n",
    "        \n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # get the index of the zero frequency and construct weight\n",
    "        freq0_y = (torch.fft.fftshift(torch.fft.fftfreq(x.shape[-2])) == 0).nonzero().item()\n",
    "        self.get_weight()\n",
    "        # Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.fftshift(torch.fft.rfft2(x), dim=-2)\n",
    "        x_ft = x_ft[..., (freq0_y - self.modes1 + 1):(freq0_y + self.modes1), :self.modes2]\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-2), x.size(-1) // 2 + 1, dtype=torch.cfloat,\n",
    "                             device=x.device)              \n",
    "        out_ft[..., (freq0_y - self.modes1 + 1):(freq0_y + self.modes1), :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft, self.weights)\n",
    "\n",
    "\n",
    "        # Return to physical space\n",
    "        x = torch.fft.irfft2(torch.fft.ifftshift(out_ft, dim=-2), s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp1 = nn.Conv2d(in_channels, mid_channels, 1)\n",
    "        self.mlp2 = nn.Conv2d(mid_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.mlp2(x)\n",
    "        return x\n",
    "        \n",
    "class Crop_to_latent_size(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(Crop_to_latent_size, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.temp_size = min(in_size, out_size)\n",
    "    def forward(self, u1):\n",
    "        B, C, _, _ = u1.shape\n",
    "        fu1 = torch.fft.rfft2(u1, norm=\"ortho\")\n",
    "        fu1_recover = torch.zeros((B, C, self.out_size, self.out_size // 2 + 1), dtype=torch.complex64, device=u1.device)\n",
    "        fu1_recover[:, :, :self.temp_size // 2, :self.temp_size // 2 + 1] = fu1[:, :, :self.temp_size // 2, :self.temp_size // 2 + 1]\n",
    "        fu1_recover[:, :, -self.temp_size // 2:, :self.temp_size // 2 + 1] = fu1[:, :, -self.temp_size // 2:, :self.temp_size // 2 + 1]\n",
    "        # Inverse FFT and scaling\n",
    "        u1_recover = torch.fft.irfft2(fu1_recover, norm=\"ortho\") * (self.out_size / self.in_size)\n",
    "        return u1_recover\n",
    "\n",
    "class CROP_FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2, width, in_size, latent_size, time_steps):\n",
    "        super(CROP_FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        input shape: (batchsize, x=64, y=64, c=12)\n",
    "        output: the solution of the next timestep\n",
    "        output shape: (batchsize, x=64, y=64, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.in_size = in_size\n",
    "        self.latent_size = latent_size\n",
    "        self.padding = 8 # pad the domain if input is non-periodic\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        self.CROP_to_latent = Crop_to_latent_size(self.in_size, self.latent_size)\n",
    "        self.CROP_back = Crop_to_latent_size(self.latent_size, self.in_size)\n",
    "        self.p = nn.Conv2d(self.time_steps, self.width, 1) # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.mlp0 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp1 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp2 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp3 = MLP(self.width, self.width, self.width)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.norm = nn.InstanceNorm2d(self.width)\n",
    "        self.q = MLP(self.width, 1, self.width * 4) # output channel is 1: u(x, y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.permute(0, 3, 1, 2)\n",
    "        print(\"Pre projection: \", x.shape)\n",
    "        x = self.CROP_to_latent(x)\n",
    "        print(\"Post projection: \", x.shape)\n",
    "        x = self.p(x)\n",
    "        # x = F.pad(x, [0,self.padding, 0,self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.norm(self.conv0(self.norm(x)))\n",
    "        x1 = self.mlp0(x1)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv1(self.norm(x)))\n",
    "        x1 = self.mlp1(x1)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv2(self.norm(x)))\n",
    "        x1 = self.mlp2(x1)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv3(self.norm(x)))\n",
    "        x1 = self.mlp3(x1)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "        print(\"Pre crop back: \", x.shape)\n",
    "        x = self.CROP_back(x)\n",
    "        print(\"Post crop back: \", x.shape)\n",
    "\n",
    "        # x = x[..., :-self.padding, :-self.padding] # pad the domain if input is non-periodic\n",
    "        x = self.q(x)\n",
    "        #x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float, device=device)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float, device=device)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe52cb88-cd8d-4169-8c25-0d5e93b49c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre projection:  torch.Size([2, 1, 128, 128])\n",
      "Post projection:  torch.Size([2, 1, 64, 64])\n",
      "Pre crop back:  torch.Size([2, 64, 64, 64])\n",
      "Post crop back:  torch.Size([2, 64, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes1 = 32 # modes have to be less than half the latent size\n",
    "modes2 = 16\n",
    "width = 64\n",
    "latent_size = 64 # Latent size has to be smaller or equal to smallest input dimension\n",
    "in_size = 128 # The in_size is dynamically changed for different input!\n",
    "batch_size = 2\n",
    "time_steps = 1\n",
    "\n",
    "crop_model = CROP_FNO2d(modes1, modes2, width, in_size, latent_size, time_steps).to(device)\n",
    "\n",
    "x_data = torch.rand((batch_size, time_steps, in_size, in_size, )).type(torch.float32).to(device)\n",
    "\n",
    "#x_data_transformed = torch.rand((32, 64, 64, 1)).type(torch.float32).to(device)\n",
    "\n",
    "crop_model(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150f3b06-a947-482b-8d1c-abc23055f00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488eb7a-e6f8-4b56-b01a-1b8306767918",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FNO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0647e853-7e0e-4213-abe5-2ee797ad0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adopted From:\n",
    "@author: Zongyi Li\n",
    "This file is the Fourier Neural Operator for 2D problem such as the Navier-Stokes equation discussed in Section 5.3 in the [paper](https://arxiv.org/pdf/2010.08895.pdf),\n",
    "which uses a recurrent structure to propagates in time.\n",
    "\"\"\"\n",
    "# https://github.com/wenhangao21/ICLR25-CROP/blob/main/original_code_and_trained_models/Sec5_1_NS_with_low_Reynolds/FNO/fno.py \n",
    "\n",
    "import torch.nn.functional as F\n",
    "from timeit import default_timer\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "\n",
    "\n",
    "class Spectral_weights(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        dtype = torch.cfloat\n",
    "        self.kernel_size_Y = 2*modes1 - 1\n",
    "        self.kernel_size_X = modes2\n",
    "        self.W = nn.ParameterDict({\n",
    "            'y0_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, modes1 - 1, 1, dtype=dtype)),\n",
    "            'yposx_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, self.kernel_size_Y, self.kernel_size_X - 1, dtype=dtype)),\n",
    "            '00_modes': torch.nn.Parameter(torch.empty(in_channels, out_channels, 1, 1, dtype=torch.float))\n",
    "        })\n",
    "        self.eval_build = True\n",
    "        self.reset_parameters()\n",
    "        self.get_weight()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for v in self.W.values():\n",
    "            nn.init.kaiming_uniform_(v, a=math.sqrt(5))\n",
    "            \n",
    "    def get_weight(self):\n",
    "        if self.training:\n",
    "            self.eval_build = True\n",
    "        elif self.eval_build:\n",
    "            self.eval_build = False\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        self.weights = torch.cat([self.W[\"y0_modes\"], self.W[\"00_modes\"].cfloat(), self.W[\"y0_modes\"].flip(dims=(-2, )).conj()], dim=-2)\n",
    "        self.weights = torch.cat([self.weights, self.W[\"yposx_modes\"]], dim=-1)\n",
    "        self.weights = self.weights.view(self.in_channels, self.out_channels,\n",
    "                                         self.kernel_size_Y, self.kernel_size_X)\n",
    "        \n",
    "\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "        self.spectral_weight = Spectral_weights(in_channels=in_channels, out_channels=out_channels, modes1=modes1, modes2=modes2)\n",
    "        self.get_weight()\n",
    "\n",
    "    def get_weight(self):\n",
    "        self.spectral_weight.get_weight()\n",
    "        self.weights = self.spectral_weight.weights\n",
    "        \n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # get the index of the zero frequency and construct weight\n",
    "        freq0_y = (torch.fft.fftshift(torch.fft.fftfreq(x.shape[-2])) == 0).nonzero().item()\n",
    "        self.get_weight()\n",
    "        # Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.fftshift(torch.fft.rfft2(x), dim=-2)\n",
    "        x_ft = x_ft[..., (freq0_y - self.modes1 + 1):(freq0_y + self.modes1), :self.modes2]\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-2), x.size(-1) // 2 + 1, dtype=torch.cfloat,\n",
    "                             device=x.device)              \n",
    "        out_ft[..., (freq0_y - self.modes1 + 1):(freq0_y + self.modes1), :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft, self.weights)\n",
    "\n",
    "\n",
    "        # Return to physical space\n",
    "        x = torch.fft.irfft2(torch.fft.ifftshift(out_ft, dim=-2), s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp1 = nn.Conv2d(in_channels, mid_channels, 1)\n",
    "        self.mlp2 = nn.Conv2d(mid_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.mlp2(x)\n",
    "        return x\n",
    "\n",
    "class FNO2d_custom(nn.Module):\n",
    "    def __init__(self, modes1, modes2, width):\n",
    "        super(FNO2d_custom, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        input shape: (batchsize, x=64, y=64, c=12)\n",
    "        output: the solution of the next timestep\n",
    "        output shape: (batchsize, x=64, y=64, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 8 # pad the domain if input is non-periodic\n",
    "\n",
    "        self.p = nn.Linear(12, self.width) # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.mlp0 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp1 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp2 = MLP(self.width, self.width, self.width)\n",
    "        self.mlp3 = MLP(self.width, self.width, self.width)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.norm = nn.InstanceNorm2d(self.width)\n",
    "        self.q = MLP(self.width, 1, self.width * 4) # output channel is 1: u(x, y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.p(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        # x = F.pad(x, [0,self.padding, 0,self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.norm(self.conv0(self.norm(x)))\n",
    "        x1 = self.mlp0(x1)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv1(self.norm(x)))\n",
    "        x1 = self.mlp1(x1)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv2(self.norm(x)))\n",
    "        x1 = self.mlp2(x1)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.norm(self.conv3(self.norm(x)))\n",
    "        x1 = self.mlp3(x1)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        # x = x[..., :-self.padding, :-self.padding] # pad the domain if input is non-periodic\n",
    "        x = self.q(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float, device=device)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float, device=device)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea30069-dc19-431b-b3b9-d37a47861171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 64, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes1 = 12\n",
    "modes2 = 12\n",
    "width = 20\n",
    "in_size = 32\n",
    "\n",
    "fno_model = FNO2d_custom(modes1, modes2, width).to(device)\n",
    "\n",
    "x_data = torch.rand((batch_size, 2*in_size, 2*in_size, 10)).type(torch.float32).to(device)\n",
    "\n",
    "#x_data_transformed = torch.rand((32, 64, 64, 1)).type(torch.float32).to(device)\n",
    "\n",
    "fno_model(x_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ac361-f5a0-4b76-9d91-b0dc10b22526",
   "metadata": {},
   "source": [
    "# Original CNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4555df63-d742-46ce-ac70-5e83bc07dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator_aliasing.models.CNO2d_original_version.CNOModule import CNO\n",
    "from operator_aliasing.models.CNO2d_original_version.training.filtered_networks import LReLu, LReLu_regular, LReLu_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e29f246-904b-4956-bd3a-5fa685a847db",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = LReLu_torch(in_channels =1, #In _channels is not used in these settings\n",
    "                                            out_channels         = 1,                   \n",
    "                                            in_size               = 32,                       \n",
    "                                            out_size              = 32,                       \n",
    "                                            in_sampling_rate      = 32,               \n",
    "                                            out_sampling_rate     = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a4a31c5-f530-471e-b1b3-f13764031e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_data = torch.rand((batch_size, time_steps, 2*in_size, 2*in_size, )).type(torch.float32).to(device)\n",
    "cno.lift.inter_CNOBlock.activation.activation(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a52ea07-bac6-4656-bfb8-39ff67d1e480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 74, 74])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes1 = 16\n",
    "modes2 = 16\n",
    "width = 64\n",
    "latent_size = 256\n",
    "in_size = 37\n",
    "batch_size = 2\n",
    "time_steps = 1\n",
    "\n",
    "x_data = torch.rand((batch_size, time_steps, 2*in_size, 2*in_size, )).type(torch.float32).to(device)\n",
    "\n",
    "cno = CNO(\n",
    "                 in_dim = time_steps,                    # Number of input channels.\n",
    "                 in_size=in_size,                   # Input spatial size\n",
    "                 N_layers=1,                  # Number of (D) or (U) blocks in the network\n",
    "                 N_res = 1,                 # Number of (R) blocks per level (except the neck)\n",
    "                 N_res_neck = 6,            # Number of (R) blocks in the neck\n",
    "                 channel_multiplier = 32,   # How the number of channels evolve?\n",
    "                 conv_kernel=3,             # Size of all the kernels\n",
    "                 cutoff_den = 2.0001,       # Filter property 1.\n",
    "                 filter_size=6,             # Filter property 2.\n",
    "                 lrelu_upsampling = 2,      # Filter property 3.\n",
    "                 half_width_mult  = 0.8,    # Filter property 4.\n",
    "                 radial = False,            # Filter property 5. Is filter radial?\n",
    "                 batch_norm = True,         # Add BN? We do not add BN in lifting/projection layer\n",
    "                 out_dim = 1,               # Target dimension\n",
    "                 out_size = 1,              # If out_size is 1, Then out_size = in_size. Else must be int\n",
    "                 expand_input = False,      # Start with original in_size, or expand it (pad zeros in the spectrum)\n",
    "                 latent_lift_proj_dim = 64, # Intermediate latent dimension in the lifting/projection layer\n",
    "                 add_inv = True,            # Add invariant block (I) after the intermediate connections?\n",
    "                 activation = 'cno_lrelu_torch'   # Activation function can be 'cno_lrelu' or 'lrelu'\n",
    "                #activation='lrelu'                \n",
    ").to(device)\n",
    "\n",
    "cno(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8467394a-0b41-4d38-932f-b6fab76d4c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-lift torch.Size([2, 1, 320, 320])\n",
      "pre upsample: torch.Size([2, 64, 320, 320])\n",
      "post upsample: torch.Size([2, 64, 160, 160])\n",
      "post downsample: torch.Size([2, 64, 80, 80])\n",
      "post downsample 2.0: torch.Size([2, 64, 80, 80])\n",
      "post-lift torch.Size([2, 16, 80, 80])\n",
      "pre-activation: torch.Size([2, 16, 80, 80])\n",
      "pre upsample: torch.Size([2, 16, 80, 80])\n",
      "post upsample: torch.Size([2, 16, 160, 160])\n",
      "post downsample: torch.Size([2, 16, 80, 80])\n",
      "post downsample 2.0: torch.Size([2, 16, 80, 80])\n",
      "post-activation: torch.Size([2, 16, 80, 80])\n",
      "pre upsample: torch.Size([2, 32, 80, 80])\n",
      "post upsample: torch.Size([2, 32, 160, 160])\n",
      "post downsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "post-activation: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 32, 40, 40])\n",
      "post upsample: torch.Size([2, 32, 80, 80])\n",
      "post downsample: torch.Size([2, 32, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 32, 40, 40])\n",
      "pre upsample: torch.Size([2, 16, 40, 40])\n",
      "post upsample: torch.Size([2, 16, 80, 80])\n",
      "post downsample: torch.Size([2, 16, 40, 40])\n",
      "post downsample 2.0: torch.Size([2, 16, 80, 80])\n",
      "pre upsample: torch.Size([2, 16, 80, 80])\n",
      "post upsample: torch.Size([2, 16, 160, 160])\n",
      "post downsample: torch.Size([2, 16, 80, 80])\n",
      "post downsample 2.0: torch.Size([2, 16, 80, 80])\n",
      "pre-projection torch.Size([2, 32, 80, 80])\n",
      "pre upsample: torch.Size([2, 64, 80, 80])\n",
      "post upsample: torch.Size([2, 64, 160, 160])\n",
      "post downsample: torch.Size([2, 64, 80, 80])\n",
      "post downsample 2.0: torch.Size([2, 64, 80, 80])\n",
      "post-projection torch.Size([2, 1, 80, 80])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 80, 80])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = torch.rand((batch_size, time_steps, in_size*4, in_size*4, )).type(torch.float32).to(device)\n",
    "cno(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd362724-ac32-4c45-8cd8-aa2cb992765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CNO1d code has been modified from a tutorial featured in the \n",
    "# ETH Zurich course \"AI in the Sciences and Engineering.\"\n",
    "# Git page for this course: https://github.com/bogdanraonic3/AI_Science_Engineering \n",
    "\n",
    "# For up/downsampling, the antialias interpolation functions from the \n",
    "# torch library are utilized, limiting the ability to design\n",
    "# your own low-pass filters at present.\n",
    "\n",
    "# While acknowledging this suboptimal setup, the performance of CNO1d remains commendable. \n",
    "# Additionally, a training script is available, offering a solid foundation for personal projects.\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# CNO LReLu activation fucntion\n",
    "# CNO building block (CNOBlock) → Conv1d - BatchNorm - Activation\n",
    "# Lift/Project Block (Important for embeddings)\n",
    "# Residual Block → Conv1d - BatchNorm - Activation - Conv1d - BatchNorm - Skip Connection\n",
    "# ResNet → Stacked ResidualBlocks (several blocks applied iteratively)\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# Activation Function:\n",
    "#---------------------\n",
    "\n",
    "class CNO_LReLu(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_size,\n",
    "                out_size\n",
    "                ):\n",
    "        super(CNO_LReLu, self).__init__()\n",
    "\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x.unsqueeze(2), size = (1,2 * self.in_size), mode = \"bicubic\", antialias = True)\n",
    "        x = self.act(x)\n",
    "        x = F.interpolate(x, size = (1,self.out_size), mode = \"bicubic\", antialias = True)\n",
    "        return x[:,:,0]\n",
    "\n",
    "#--------------------\n",
    "# CNO Block:\n",
    "#--------------------\n",
    "\n",
    "class CNOBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                in_size,\n",
    "                out_size,\n",
    "                use_bn = True\n",
    "                ):\n",
    "        super(CNOBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.in_size  = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        #-----------------------------------------\n",
    "\n",
    "        # We apply Conv -> BN (optional) -> Activation\n",
    "        # Up/Downsampling happens inside Activation\n",
    "\n",
    "        self.convolution = torch.nn.Conv1d(in_channels = self.in_channels,\n",
    "                                            out_channels= self.out_channels,\n",
    "                                            kernel_size = 3,\n",
    "                                            padding     = 1)\n",
    "\n",
    "        if use_bn:\n",
    "            self.batch_norm  = nn.BatchNorm1d(self.out_channels)\n",
    "        else:\n",
    "            self.batch_norm  = nn.Identity()\n",
    "        self.act           = CNO_LReLu(in_size  = self.in_size,\n",
    "                                        out_size = self.out_size)\n",
    "    def forward(self, x):\n",
    "        x = self.convolution(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return self.act(x)\n",
    "    \n",
    "#--------------------\n",
    "# Lift/Project Block:\n",
    "#--------------------\n",
    "\n",
    "class LiftProjectBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                size,\n",
    "                latent_dim = 64\n",
    "                ):\n",
    "        super(LiftProjectBlock, self).__init__()\n",
    "\n",
    "        self.inter_CNOBlock = CNOBlock(in_channels       = in_channels,\n",
    "                                        out_channels     = latent_dim,\n",
    "                                        in_size          = size,\n",
    "                                        out_size         = size,\n",
    "                                        use_bn           = False)\n",
    "\n",
    "        self.convolution = torch.nn.Conv1d(in_channels  = latent_dim,\n",
    "                                            out_channels = out_channels,\n",
    "                                            kernel_size  = 3,\n",
    "                                            padding      = 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inter_CNOBlock(x)\n",
    "        x = self.convolution(x)\n",
    "        return x\n",
    "\n",
    "#--------------------\n",
    "# Residual Block:\n",
    "#--------------------\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                channels,\n",
    "                size,\n",
    "                use_bn = True\n",
    "                ):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.size     = size\n",
    "\n",
    "        #-----------------------------------------\n",
    "\n",
    "        # We apply Conv -> BN (optional) -> Activation -> Conv -> BN (optional) -> Skip Connection\n",
    "        # Up/Downsampling happens inside Activation\n",
    "\n",
    "        self.convolution1 = torch.nn.Conv1d(in_channels = self.channels,\n",
    "                                            out_channels= self.channels,\n",
    "                                            kernel_size = 3,\n",
    "                                            padding     = 1)\n",
    "        self.convolution2 = torch.nn.Conv1d(in_channels = self.channels,\n",
    "                                            out_channels= self.channels,\n",
    "                                            kernel_size = 3,\n",
    "                                            padding     = 1)\n",
    "\n",
    "        if use_bn:\n",
    "            self.batch_norm1  = nn.BatchNorm1d(self.channels)\n",
    "            self.batch_norm2  = nn.BatchNorm1d(self.channels)\n",
    "\n",
    "        else:\n",
    "            self.batch_norm1  = nn.Identity()\n",
    "            self.batch_norm2  = nn.Identity()\n",
    "\n",
    "        self.act           = CNO_LReLu(in_size  = self.size,\n",
    "                                        out_size = self.size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.convolution2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        return x + out\n",
    "\n",
    "#--------------------\n",
    "# ResNet:\n",
    "#--------------------\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                channels,\n",
    "                size,\n",
    "                num_blocks,\n",
    "                use_bn = True\n",
    "                ):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        self.res_nets = []\n",
    "        for _ in range(self.num_blocks):\n",
    "            self.res_nets.append(ResidualBlock(channels = channels,\n",
    "                                                size = size,\n",
    "                                                use_bn = use_bn))\n",
    "\n",
    "        self.res_nets = torch.nn.Sequential(*self.res_nets)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.res_nets[i](x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#--------------------\n",
    "# CNO:\n",
    "#--------------------\n",
    "\n",
    "class CNO1d(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_dim,                    # Number of input channels.\n",
    "                out_dim,                   # Number of input channels.\n",
    "                size,                      # Input and Output spatial size (required )\n",
    "                N_layers,                  # Number of (D) or (U) blocks in the network\n",
    "                N_res = 4,                 # Number of (R) blocks per level (except the neck)\n",
    "                N_res_neck = 4,            # Number of (R) blocks in the neck\n",
    "                channel_multiplier = 16,   # How the number of channels evolve?\n",
    "                use_bn = True,             # Add BN? We do not add BN in lifting/projection layer\n",
    "                ):\n",
    "\n",
    "        super(CNO1d, self).__init__()\n",
    "\n",
    "        self.N_layers = int(N_layers)         # Number od (D) & (U) Blocks\n",
    "        self.lift_dim = channel_multiplier//2 # Input is lifted to the half of channel_multiplier dimension\n",
    "        self.in_dim   = in_dim\n",
    "        self.out_dim  = out_dim\n",
    "        self.channel_multiplier = channel_multiplier  # The growth of the channels\n",
    "\n",
    "        ######## Num of channels/features - evolution ########\n",
    "\n",
    "        self.encoder_features = [self.lift_dim] # How the features in Encoder evolve (number of features)\n",
    "        for i in range(self.N_layers):\n",
    "            self.encoder_features.append(2 ** i *   self.channel_multiplier)\n",
    "\n",
    "        self.decoder_features_in = self.encoder_features[1:] # How the features in Decoder evolve (number of features)\n",
    "        self.decoder_features_in.reverse()\n",
    "        self.decoder_features_out = self.encoder_features[:-1]\n",
    "        self.decoder_features_out.reverse()\n",
    "\n",
    "        for i in range(1, self.N_layers):\n",
    "            self.decoder_features_in[i] = 2*self.decoder_features_in[i] #Pad the outputs of the resnets (we must multiply by 2 then)\n",
    "\n",
    "        ######## Spatial sizes of channels - evolution ########\n",
    "\n",
    "        self.encoder_sizes = []\n",
    "        self.decoder_sizes = []\n",
    "        for i in range(self.N_layers + 1):\n",
    "            self.encoder_sizes.append(size // 2 ** i)\n",
    "            self.decoder_sizes.append(size // 2 ** (self.N_layers - i))\n",
    "\n",
    "\n",
    "        ######## Define Lift and Project blocks ########\n",
    "\n",
    "        self.lift   = LiftProjectBlock(in_channels = in_dim,\n",
    "                                        out_channels = self.encoder_features[0],\n",
    "                                        size = size)\n",
    "\n",
    "        self.project   = LiftProjectBlock(in_channels = self.encoder_features[0] + self.decoder_features_out[-1],\n",
    "                                            out_channels = out_dim,\n",
    "                                            size = size)\n",
    "\n",
    "        ######## Define Encoder, ED Linker and Decoder networks ########\n",
    "\n",
    "        self.encoder         = nn.ModuleList([(CNOBlock(in_channels  = self.encoder_features[i],\n",
    "                                                        out_channels = self.encoder_features[i+1],\n",
    "                                                        in_size      = self.encoder_sizes[i],\n",
    "                                                        out_size     = self.encoder_sizes[i+1],\n",
    "                                                        use_bn       = use_bn))\n",
    "                                                for i in range(self.N_layers)])\n",
    "\n",
    "        # After the ResNets are executed, the sizes of encoder and decoder might not match (if out_size>1)\n",
    "        # We must ensure that the sizes are the same, by aplying CNO Blocks\n",
    "        self.ED_expansion     = nn.ModuleList([(CNOBlock(in_channels = self.encoder_features[i],\n",
    "                                                        out_channels = self.encoder_features[i],\n",
    "                                                        in_size      = self.encoder_sizes[i],\n",
    "                                                        out_size     = self.decoder_sizes[self.N_layers - i],\n",
    "                                                        use_bn       = use_bn))\n",
    "                                                for i in range(self.N_layers + 1)])\n",
    "\n",
    "        self.decoder         = nn.ModuleList([(CNOBlock(in_channels  = self.decoder_features_in[i],\n",
    "                                                        out_channels = self.decoder_features_out[i],\n",
    "                                                        in_size      = self.decoder_sizes[i],\n",
    "                                                        out_size     = self.decoder_sizes[i+1],\n",
    "                                                        use_bn       = use_bn))\n",
    "                                                for i in range(self.N_layers)])\n",
    "\n",
    "        ####################### Define ResNets Blocks ################################################################\n",
    "\n",
    "        # Here, we define ResNet Blocks.\n",
    "\n",
    "        # Operator UNet:\n",
    "        # Outputs of the middle networks are patched (or padded) to corresponding sets of feature maps in the decoder\n",
    "\n",
    "        self.res_nets = []\n",
    "        self.N_res = int(N_res)\n",
    "        self.N_res_neck = int(N_res_neck)\n",
    "\n",
    "        # Define the ResNet networks (before the neck)\n",
    "        for l in range(self.N_layers):\n",
    "            self.res_nets.append(ResNet(channels = self.encoder_features[l],\n",
    "                                        size = self.encoder_sizes[l],\n",
    "                                        num_blocks = self.N_res,\n",
    "                                        use_bn = use_bn))\n",
    "\n",
    "        self.res_net_neck = ResNet(channels = self.encoder_features[self.N_layers],\n",
    "                                    size = self.encoder_sizes[self.N_layers],\n",
    "                                    num_blocks = self.N_res_neck,\n",
    "                                    use_bn = use_bn)\n",
    "\n",
    "        self.res_nets = torch.nn.Sequential(*self.res_nets)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.lift(x) #Execute Lift\n",
    "        skip = []\n",
    "\n",
    "        # Execute Encoder\n",
    "        for i in range(self.N_layers):\n",
    "\n",
    "            #Apply ResNet & save the result\n",
    "            y = self.res_nets[i](x)\n",
    "            skip.append(y)\n",
    "\n",
    "            # Apply (D) block\n",
    "            x = self.encoder[i](x)\n",
    "            \n",
    "        # Apply the deepest ResNet (bottle neck)\n",
    "        x = self.res_net_neck(x)\n",
    "\n",
    "        # Execute Decode\n",
    "        for i in range(self.N_layers):\n",
    "\n",
    "            # Apply (I) block (ED_expansion) & cat if needed\n",
    "            if i == 0:\n",
    "                x = self.ED_expansion[self.N_layers - i](x) #BottleNeck : no cat\n",
    "            else:\n",
    "                x = torch.cat((x, self.ED_expansion[self.N_layers - i](skip[-i])),1)\n",
    "\n",
    "            # Apply (U) block\n",
    "            x = self.decoder[i](x)\n",
    "\n",
    "        # Cat & Execute Projetion\n",
    "        x = torch.cat((x, self.ED_expansion[0](skip[0])),1)\n",
    "        x = self.project(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d505b6-28bf-46a0-a09f-7a29cc05ae62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 27])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width = 64\n",
    "in_size = 27\n",
    "batch_size = 2\n",
    "time_steps = 1\n",
    "\n",
    "x_data = torch.rand((batch_size, time_steps, 2*in_size*2, )).type(torch.float32).to(device)\n",
    "\n",
    "model= CNO1d(\n",
    "                time_steps,                    # Number of input channels.\n",
    "                time_steps,                   # Number of input channels.\n",
    "                in_size,                      # Input and Output spatial size (required )\n",
    "                N_layers=4,                  # Number of (D) or (U) blocks in the network\n",
    "                N_res = 4,                 # Number of (R) blocks per level (except the neck)\n",
    "                N_res_neck = 4,            # Number of (R) blocks in the neck\n",
    "                channel_multiplier = 16,   # How the number of channels evolve?\n",
    "                use_bn = True,             # Add BN? We do not add BN in lifting/projection layer\n",
    "                ).to(device)\n",
    "model_output = model(x_data)\n",
    "model_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c9366e0-513a-485e-89c2-b6aa726d0f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 375, 188]) (375, 375)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 375, 375])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def resize(x, out_size, permute=False):\n",
    "    if permute:\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "    f = torch.fft.rfft2(x, norm='backward')\n",
    "    f_z = torch.zeros((*x.shape[:-2], out_size[0], out_size[1]//2 + 1), dtype=f.dtype, device=f.device)\n",
    "    # 2k+1 -> (2k+1 + 1) // 2 = k+1 and (2k+1)//2 = k\n",
    "    top_freqs1 = min((f.shape[-2] + 1) // 2, (out_size[0] + 1) // 2)\n",
    "    top_freqs2 = min(f.shape[-1], out_size[1] // 2 + 1)\n",
    "    # 2k -> (2k + 1) // 2 = k and (2k)//2 = k\n",
    "    bot_freqs1 = min(f.shape[-2] // 2, out_size[0] // 2)\n",
    "    bot_freqs2 = min(f.shape[-1], out_size[1] // 2 + 1)\n",
    "    f_z[..., :top_freqs1, :top_freqs2] = f[..., :top_freqs1, :top_freqs2]\n",
    "    f_z[..., -bot_freqs1:, :bot_freqs2] = f[..., -bot_freqs1:, :bot_freqs2]\n",
    "    # x_z = torch.fft.ifft2(f_z, s=out_size).real\n",
    "    x_z = torch.fft.irfft2(f_z, s=out_size).real\n",
    "    print(f_z.shape, out_size)\n",
    "    x_z = x_z * (out_size[0] / x.shape[-2]) * (out_size[1] / x.shape[-1])\n",
    " \n",
    "    # f_z[..., -f.shape[-2]//2:, :f.shape[-1]] = f[..., :f.shape[-2]//2+1, :]\n",
    " \n",
    "    if permute:\n",
    "        x_z = x_z.permute(0, 2, 3, 1)\n",
    "    return x_z\n",
    "    \n",
    "x_data = torch.rand((batch_size, time_steps, 2*in_size*2, 2*in_size*2))\n",
    "resize(x_data, (375,375), permute=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dd245a5-3304-4b76-89b5-36dedd50d9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 39, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 39, 39])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Crop_to_latent_size(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(Crop_to_latent_size, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.temp_size = min(in_size, out_size)\n",
    "    def forward(self, u1):\n",
    "        B, C, _, _ = u1.shape\n",
    "        fu1 = torch.fft.rfft2(u1, norm=\"ortho\")\n",
    "        fu1_recover = torch.zeros((B, C, self.out_size, self.out_size // 2 + 1), dtype=torch.complex64, device=u1.device)\n",
    "        fu1_recover[:, :, :self.temp_size // 2, :self.temp_size // 2 + 1] = fu1[:, :, :self.temp_size // 2, :self.temp_size // 2 + 1]\n",
    "        fu1_recover[:, :, -self.temp_size // 2:, :self.temp_size // 2 + 1] = fu1[:, :, -self.temp_size // 2:, :self.temp_size // 2 + 1]\n",
    "        # Inverse FFT and scaling\n",
    "        print(fu1_recover.shape)\n",
    "        u1_recover = torch.fft.irfft2(fu1_recover, norm=\"ortho\", s=(self.out_size,self.out_size)) * (self.out_size / self.in_size)\n",
    "        return u1_recover\n",
    "\n",
    "in_size = 37\n",
    "x_data = torch.rand((batch_size, time_steps, in_size, in_size))\n",
    "proj = Crop_to_latent_size(in_size, 39)\n",
    "proj(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3e379-caec-4fd2-bc59-70d81b393449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(x, out_size, permute=False):\n",
    "    if permute:\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "    f = torch.fft.rfft2(x, norm='backward')\n",
    "    f_z = torch.zeros((*x.shape[:-2], out_size[0], out_size[1]//2 + 1), dtype=f.dtype, device=f.device)\n",
    "\n",
    "    \n",
    "    # 2k+1 -> (2k+1 + 1) // 2 = k+1 and (2k+1)//2 = k\n",
    "    top_freqs1 = min((f.shape[-2] + 1) // 2, (out_size[0] + 1) // 2)\n",
    "    top_freqs2 = min(f.shape[-1], out_size[1] // 2 + 1)\n",
    "    # 2k -> (2k + 1) // 2 = k and (2k)//2 = k\n",
    "    bot_freqs1 = min(f.shape[-2] // 2, out_size[0] // 2)\n",
    "    bot_freqs2 = min(f.shape[-1], out_size[1] // 2 + 1)\n",
    "\n",
    "    \n",
    "    f_z[..., :top_freqs1, :top_freqs2] = f[..., :top_freqs1, :top_freqs2]\n",
    "    f_z[..., -bot_freqs1:, :bot_freqs2] = f[..., -bot_freqs1:, :bot_freqs2]\n",
    "    # x_z = torch.fft.ifft2(f_z, s=out_size).real\n",
    "    x_z = torch.fft.irfft2(f_z, s=out_size).real\n",
    "    x_z = x_z * (out_size[0] / x.shape[-2]) * (out_size[1] / x.shape[-1])\n",
    " \n",
    "    # f_z[..., -f.shape[-2]//2:, :f.shape[-1]] = f[..., :f.shape[-2]//2+1, :]\n",
    " \n",
    "    if permute:\n",
    "        x_z = x_z.permute(0, 2, 3, 1)\n",
    "    return x_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d022d94-9cf8-4c7e-8f61-35e4df79a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_1D(x, out_size, permute=False):\n",
    "    # based on https://github.com/camlab-ethz/ConvolutionalNeuralOperator/blob/main/CNO2d_original_version/Error_Distribution_VaryingResolution.py#L119C1-L141C15\n",
    "    if permute:\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "    f = torch.fft.rfftn(x, norm='backward', s = x.shape[-1])\n",
    "\n",
    "    f_z = torch.zeros((*x.shape[:-1], out_size[0] // 2 + 1), dtype=f.dtype, device=f.device)\n",
    "    # 2k+1 -> (2k+1 + 1) // 2 = k+1 and (2k+1)//2 = k\n",
    "    top_freqs1 = min((f.shape[-1]), (out_size[0] // 2 + 1))\n",
    "    f_z[..., :top_freqs1] = f[..., :top_freqs1]\n",
    "    x_z = torch.fft.irfftn(f_z,s=out_size[0]).real\n",
    "    x_z = x_z * (out_size[0] / x.shape[-2]) #* (out_size[1] / x.shape[-1])\n",
    "\n",
    "    if permute:\n",
    "        x_z = x_z.permute(0, 2, 3, 1)\n",
    "    return x_z\n",
    "\n",
    "time_steps = 1\n",
    "in_size = 100\n",
    "batch_size = 1\n",
    "x_data = torch.rand((batch_size, time_steps, 1, in_size))\n",
    "upsample = resize_1D(x_data, (320,), permute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a533848e-1ad1-453a-8368-69c264dd4612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 320])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b83c031c-9bef-403f-ae9d-40aadcc09f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from operator_aliasing.utils import get_energy_curve_1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "original_spectrum = get_energy_curve_1d(x_data)\n",
    "\n",
    "upsample_spectrum = get_energy_curve_1d(upsample)\n",
    "\n",
    "\n",
    "plt.plot(original_spectrum)\n",
    "plt.plot(upsample_spectrum)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28fc8cf6-20d5-4c0e-8d17-f7a7aec51766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the filters is borrowed from paper \"Alias-Free Generative Adversarial Networks (StyleGAN3)\" https://nvlabs.github.io/stylegan3/\n",
    "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
    "#\n",
    "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
    "# and proprietary rights in and to this software, related documentation\n",
    "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
    "# distribution of this software and related documentation without an express\n",
    "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from torch.nn import LeakyReLU as LReLu\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "#Depending on in_size, out_size, the CNOBlock can be:\n",
    "#   -- (D) Block\n",
    "#   -- (U) Block\n",
    "#   -- (I) Block\n",
    "\n",
    "class CNOBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 in_size,\n",
    "                 out_size,\n",
    "                 cutoff_den = 2.0001,\n",
    "                 conv_kernel = 3,\n",
    "                 filter_size = 6,\n",
    "                 lrelu_upsampling = 2,\n",
    "                 half_width_mult  = 0.8,\n",
    "                 radial = False,\n",
    "                 batch_norm = True,\n",
    "                 activation = 'cno_lrelu'\n",
    "                 ):\n",
    "        super(CNOBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.in_size  = in_size\n",
    "        self.out_size = out_size\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        #---------- Filter properties -----------\n",
    "        self.citically_sampled = False #We use w_c = s/2.0001 --> NOT critically sampled\n",
    "\n",
    "        if cutoff_den == 2.0:\n",
    "            self.citically_sampled = True\n",
    "        self.in_cutoff  = self.in_size / cutoff_den\n",
    "        self.out_cutoff = self.out_size / cutoff_den\n",
    "        \n",
    "        self.in_halfwidth =  half_width_mult*self.in_size - self.in_size / cutoff_den\n",
    "        self.out_halfwidth = half_width_mult*self.out_size - self.out_size / cutoff_den\n",
    "        \n",
    "        #-----------------------------------------\n",
    "\n",
    "        # We apply Conv -> BN (optional) -> Activation\n",
    "        # Up/Downsampling happens inside Activation\n",
    "        \n",
    "        pad = (self.conv_kernel-1)//2\n",
    "        self.convolution = torch.nn.Conv2d(in_channels = self.in_channels, out_channels=self.out_channels, \n",
    "                                           kernel_size=self.conv_kernel, \n",
    "                                           padding = pad)\n",
    "    \n",
    "        if self.batch_norm:\n",
    "            self.batch_norm  = nn.BatchNorm2d(self.out_channels)\n",
    "        self.activation = LReLu() \n",
    "        if activation == \"cno_lrelu_torch\":\n",
    "            self.activation = LReLu_torch(in_channels           = self.out_channels, #In _channels is not used in these settings\n",
    "                                            out_channels          = self.out_channels,                   \n",
    "                                            in_size               = self.in_size,                       \n",
    "                                            out_size              = self.out_size,                       \n",
    "                                            in_sampling_rate      = self.in_size,               \n",
    "                                            out_sampling_rate     = self.out_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convolution(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Contains CNOBlock -> Convolution -> BN\n",
    "\n",
    "class LiftProjectBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 in_size,\n",
    "                 out_size,\n",
    "                 latent_dim = 64,\n",
    "                 cutoff_den = 2.0001,\n",
    "                 conv_kernel = 3,\n",
    "                 filter_size = 6,\n",
    "                 lrelu_upsampling = 2,\n",
    "                 half_width_mult  = 0.8,\n",
    "                 radial = False,\n",
    "                 batch_norm = True,\n",
    "                 activation = 'cno_lrelu'\n",
    "                 ):\n",
    "        super(LiftProjectBlock, self).__init__()\n",
    "    \n",
    "        self.inter_CNOBlock = CNOBlock(in_channels = in_channels,\n",
    "                                    out_channels = latent_dim,\n",
    "                                    in_size = in_size,\n",
    "                                    out_size = out_size,\n",
    "                                    cutoff_den = cutoff_den,\n",
    "                                    conv_kernel = conv_kernel,\n",
    "                                    filter_size = filter_size,\n",
    "                                    lrelu_upsampling = lrelu_upsampling,\n",
    "                                    half_width_mult  = half_width_mult,\n",
    "                                    radial = radial,\n",
    "                                    batch_norm = batch_norm,\n",
    "                                    activation = activation)\n",
    "        \n",
    "        pad = (conv_kernel-1)//2\n",
    "        self.convolution = torch.nn.Conv2d(in_channels = latent_dim, out_channels=out_channels, \n",
    "                                           kernel_size=conv_kernel, stride = 1, \n",
    "                                           padding = pad)\n",
    "        \n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm  = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.inter_CNOBlock(x)\n",
    "        \n",
    "        x = self.convolution(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x)\n",
    "        return x\n",
    "        \n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Residual Block containts:\n",
    "    # Convolution -> BN -> Activation -> Convolution -> BN -> SKIP CONNECTION\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels,\n",
    "                 size,\n",
    "                 cutoff_den = 2.0001,\n",
    "                 conv_kernel = 3,\n",
    "                 filter_size = 6,\n",
    "                 lrelu_upsampling = 2,\n",
    "                 half_width_mult  = 0.8,\n",
    "                 radial = False,\n",
    "                 batch_norm = True,\n",
    "                 activation = 'cno_lrelu'\n",
    "                 ):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.size  = size\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        #---------- Filter properties -----------\n",
    "        self.citically_sampled = False #We use w_c = s/2.0001 --> NOT critically sampled\n",
    "\n",
    "        if cutoff_den == 2.0:\n",
    "            self.citically_sampled = True\n",
    "        self.cutoff  = self.size / cutoff_den        \n",
    "        self.halfwidth =  half_width_mult*self.size - self.size / cutoff_den\n",
    "        \n",
    "        #-----------------------------------------\n",
    "        \n",
    "        pad = (self.conv_kernel-1)//2\n",
    "        self.convolution1 = torch.nn.Conv2d(in_channels = self.channels, out_channels=self.channels, \n",
    "                                           kernel_size=self.conv_kernel, stride = 1, \n",
    "                                           padding = pad)\n",
    "        self.convolution2 = torch.nn.Conv2d(in_channels = self.channels, out_channels=self.channels, \n",
    "                                           kernel_size=self.conv_kernel, stride = 1, \n",
    "                                           padding = pad)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            self.batch_norm1  = nn.BatchNorm2d(self.channels)\n",
    "            self.batch_norm2  = nn.BatchNorm2d(self.channels)\n",
    "        self.activation = LReLu() \n",
    "        if activation == \"cno_lrelu_torch\":\n",
    "            print(\"custom activation\")\n",
    "            self.activation = LReLu_torch(in_channels           = self.channels, #In _channels is not used in these settings\n",
    "                                            out_channels          = self.channels,                   \n",
    "                                            in_size               = self.size,                       \n",
    "                                            out_size              = self.size,                       \n",
    "                                            in_sampling_rate      = self.size,               \n",
    "                                            out_sampling_rate     = self.size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution1(x)\n",
    "        if self.batch_norm:\n",
    "            out = self.batch_norm1(out)\n",
    "        print(\"pre-activation:\",out.shape)\n",
    "        out = self.activation(out)\n",
    "        print(\"post-activation:\",out.shape)\n",
    "        out = self.convolution2(out)\n",
    "        if self.batch_norm:\n",
    "            out = self.batch_norm2(out)\n",
    "        \n",
    "        return x + out\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "#CNO NETWORK:\n",
    "class CNO(nn.Module):\n",
    "    def __init__(self,  \n",
    "                 in_dim,                    # Number of input channels.\n",
    "                 in_size,                   # Input spatial size\n",
    "                 N_layers,                  # Number of (D) or (U) blocks in the network\n",
    "                 N_res = 1,                 # Number of (R) blocks per level (except the neck)\n",
    "                 N_res_neck = 6,            # Number of (R) blocks in the neck\n",
    "                 channel_multiplier = 32,   # How the number of channels evolve?\n",
    "                 conv_kernel=3,             # Size of all the kernels\n",
    "                 cutoff_den = 2.0001,       # Filter property 1.\n",
    "                 filter_size=6,             # Filter property 2.\n",
    "                 lrelu_upsampling = 2,      # Filter property 3.\n",
    "                 half_width_mult  = 0.8,    # Filter property 4.\n",
    "                 radial = False,            # Filter property 5. Is filter radial?\n",
    "                 batch_norm = True,         # Add BN? We do not add BN in lifting/projection layer\n",
    "                 out_dim = 1,               # Target dimension\n",
    "                 out_size = 1,              # If out_size is 1, Then out_size = in_size. Else must be int\n",
    "                 expand_input = False,      # Start with original in_size, or expand it (pad zeros in the spectrum)\n",
    "                 latent_lift_proj_dim = 64, # Intermediate latent dimension in the lifting/projection layer\n",
    "                 add_inv = True,            # Add invariant block (I) after the intermediate connections?\n",
    "                 activation = 'cno_lrelu'   # Activation function can be 'cno_lrelu' or 'lrelu'\n",
    "                ):\n",
    "        \n",
    "        super(CNO, self).__init__()\n",
    "\n",
    "        ###################### Define the parameters & specifications #################################################\n",
    "\n",
    "        \n",
    "        # Number od (D) & (U) Blocks\n",
    "        self.N_layers = int(N_layers)\n",
    "        \n",
    "        # Input is lifted to the half on channel_multiplier dimension\n",
    "        self.lift_dim = channel_multiplier//2         \n",
    "        self.out_dim  = out_dim\n",
    "        \n",
    "        #Should we add invariant layers in the decoder?\n",
    "        self.add_inv = add_inv\n",
    "        \n",
    "        # The growth of the channels : d_e parametee\n",
    "        self.channel_multiplier = channel_multiplier        \n",
    "        \n",
    "        # Is the filter radial? We always use NOT radial\n",
    "        if radial ==0:\n",
    "            self.radial = False\n",
    "        else:\n",
    "            self.radial = True\n",
    "        \n",
    "        ###################### Define evolution of the number features ################################################\n",
    "\n",
    "        # How the features in Encoder evolve (number of features)\n",
    "        self.encoder_features = [self.lift_dim]\n",
    "        for i in range(self.N_layers):\n",
    "            self.encoder_features.append(2 ** i *   self.channel_multiplier)\n",
    "        \n",
    "        # How the features in Decoder evolve (number of features)\n",
    "        self.decoder_features_in = self.encoder_features[1:]\n",
    "        self.decoder_features_in.reverse()\n",
    "        self.decoder_features_out = self.encoder_features[:-1]\n",
    "        self.decoder_features_out.reverse()\n",
    "\n",
    "        for i in range(1, self.N_layers):\n",
    "            self.decoder_features_in[i] = 2*self.decoder_features_in[i] #Pad the outputs of the resnets\n",
    "        \n",
    "        self.inv_features = self.decoder_features_in\n",
    "        self.inv_features.append(self.encoder_features[0] + self.decoder_features_out[-1])\n",
    "        \n",
    "        ###################### Define evolution of sampling rates #####################################################\n",
    "        \n",
    "        if not expand_input:\n",
    "            latent_size = in_size # No change in in_size\n",
    "        else:\n",
    "            down_exponent = 2 ** N_layers\n",
    "            latent_size = in_size - (in_size % down_exponent) + down_exponent # Jump from 64 to 72, for example\n",
    "        \n",
    "        #Are inputs and outputs of the same size? If not, how should the size of the decoder evolve?\n",
    "        if out_size == 1:\n",
    "            latent_size_out = latent_size\n",
    "        else:\n",
    "            if not expand_input:\n",
    "                latent_size_out = out_size # No change in in_size\n",
    "            else:\n",
    "                down_exponent = 2 ** N_layers\n",
    "                latent_size_out = out_size - (out_size % down_exponent) + down_exponent # Jump from 64 to 72, for example\n",
    "        \n",
    "        self.encoder_sizes = []\n",
    "        self.decoder_sizes = []\n",
    "        for i in range(self.N_layers + 1):\n",
    "            self.encoder_sizes.append(latent_size // 2 ** i)\n",
    "            self.decoder_sizes.append(latent_size_out // 2 ** (self.N_layers - i))\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################### Define Projection & Lift ##############################################################\n",
    "    \n",
    "        self.lift = LiftProjectBlock(in_channels  = in_dim,\n",
    "                                     out_channels = self.encoder_features[0],\n",
    "                                     in_size      = in_size,\n",
    "                                     out_size     = self.encoder_sizes[0],\n",
    "                                     latent_dim   = latent_lift_proj_dim,\n",
    "                                     cutoff_den   = cutoff_den,\n",
    "                                     conv_kernel  = conv_kernel,\n",
    "                                     filter_size  = filter_size,\n",
    "                                     lrelu_upsampling  = lrelu_upsampling,\n",
    "                                     half_width_mult   = half_width_mult,\n",
    "                                     radial            = radial,\n",
    "                                     batch_norm        = False,\n",
    "                                     activation = activation)\n",
    "        _out_size = out_size\n",
    "        if out_size == 1:\n",
    "            _out_size = in_size\n",
    "            \n",
    "        self.project = LiftProjectBlock(in_channels  = self.encoder_features[0] + self.decoder_features_out[-1],\n",
    "                                        out_channels = out_dim,\n",
    "                                        in_size      = self.decoder_sizes[-1],\n",
    "                                        out_size     = _out_size,\n",
    "                                        latent_dim   = latent_lift_proj_dim,\n",
    "                                        cutoff_den   = cutoff_den,\n",
    "                                        conv_kernel  = conv_kernel,\n",
    "                                        filter_size  = filter_size,\n",
    "                                        lrelu_upsampling  = lrelu_upsampling,\n",
    "                                        half_width_mult   = half_width_mult,\n",
    "                                        radial            = radial,\n",
    "                                        batch_norm        = False,\n",
    "                                        activation = activation) \n",
    "\n",
    "        ###################### Define U & D blocks ###################################################################\n",
    "\n",
    "        self.encoder         = nn.ModuleList([(CNOBlock(in_channels  = self.encoder_features[i],\n",
    "                                                        out_channels = self.encoder_features[i+1],\n",
    "                                                        in_size      = self.encoder_sizes[i],\n",
    "                                                        out_size     = self.encoder_sizes[i+1],\n",
    "                                                        cutoff_den   = cutoff_den,\n",
    "                                                        conv_kernel  = conv_kernel,\n",
    "                                                        filter_size  = filter_size,\n",
    "                                                        lrelu_upsampling = lrelu_upsampling,\n",
    "                                                        half_width_mult  = half_width_mult,\n",
    "                                                        radial = radial,\n",
    "                                                        batch_norm = batch_norm,\n",
    "                                                        activation = activation))                                  \n",
    "                                               for i in range(self.N_layers)])\n",
    "        \n",
    "        # After the ResNets are executed, the sizes of encoder and decoder might not match (if out_size>1)\n",
    "        # We must ensure that the sizes are the same, by aplying CNO Blocks\n",
    "        self.ED_expansion     = nn.ModuleList([(CNOBlock(in_channels = self.encoder_features[i],\n",
    "                                                        out_channels = self.encoder_features[i],\n",
    "                                                        in_size      = self.encoder_sizes[i],\n",
    "                                                        out_size     = self.decoder_sizes[self.N_layers - i],\n",
    "                                                        cutoff_den   = cutoff_den,\n",
    "                                                        conv_kernel  = conv_kernel,\n",
    "                                                        filter_size  = filter_size,\n",
    "                                                        lrelu_upsampling = lrelu_upsampling,\n",
    "                                                        half_width_mult  = half_width_mult,\n",
    "                                                        radial = radial,\n",
    "                                                        batch_norm = batch_norm,\n",
    "                                                        activation = activation))                                  \n",
    "                                               for i in range(self.N_layers + 1)])\n",
    "        \n",
    "        self.decoder         = nn.ModuleList([(CNOBlock(in_channels  = self.decoder_features_in[i],\n",
    "                                                        out_channels = self.decoder_features_out[i],\n",
    "                                                        in_size      = self.decoder_sizes[i],\n",
    "                                                        out_size     = self.decoder_sizes[i+1],\n",
    "                                                        cutoff_den   = cutoff_den,\n",
    "                                                        conv_kernel  = conv_kernel,\n",
    "                                                        filter_size  = filter_size,\n",
    "                                                        lrelu_upsampling = lrelu_upsampling,\n",
    "                                                        half_width_mult  = half_width_mult,\n",
    "                                                        radial = radial,\n",
    "                                                        batch_norm = batch_norm,\n",
    "                                                        activation = activation))                                  \n",
    "                                               for i in range(self.N_layers)])\n",
    "        \n",
    "        \n",
    "        self.decoder_inv    = nn.ModuleList([(CNOBlock(in_channels  =  self.inv_features[i],\n",
    "                                                        out_channels = self.inv_features[i],\n",
    "                                                        in_size      = self.decoder_sizes[i],\n",
    "                                                        out_size     = self.decoder_sizes[i],\n",
    "                                                        cutoff_den   = cutoff_den,\n",
    "                                                        conv_kernel  = conv_kernel,\n",
    "                                                        filter_size  = filter_size,\n",
    "                                                        lrelu_upsampling = lrelu_upsampling,\n",
    "                                                        half_width_mult  = half_width_mult,\n",
    "                                                        radial = radial,\n",
    "                                                        batch_norm = batch_norm,\n",
    "                                                        activation = activation))                                  \n",
    "                                               for i in range(self.N_layers + 1)])\n",
    "        \n",
    "        \n",
    "        ####################### Define ResNets Blocks ################################################################\n",
    "\n",
    "        # Here, we define ResNet Blocks. \n",
    "        # We also define the BatchNorm layers applied BEFORE the ResNet blocks \n",
    "        \n",
    "        # Operator UNet:\n",
    "        # Outputs of the middle networks are patched (or padded) to corresponding sets of feature maps in the decoder \n",
    "\n",
    "        self.res_nets = []\n",
    "        self.N_res = int(N_res)\n",
    "        self.N_res_neck = int(N_res_neck)\n",
    "\n",
    "        # Define the ResNet blocks & BatchNorm\n",
    "        for l in range(self.N_layers):\n",
    "            for i in range(self.N_res):\n",
    "                self.res_nets.append(ResidualBlock(channels = self.encoder_features[l],\n",
    "                                                   size     = self.encoder_sizes[l],\n",
    "                                                   cutoff_den = cutoff_den,\n",
    "                                                   conv_kernel = conv_kernel,\n",
    "                                                   filter_size = filter_size,\n",
    "                                                   lrelu_upsampling = lrelu_upsampling,\n",
    "                                                   half_width_mult  = half_width_mult,\n",
    "                                                   radial = radial,\n",
    "                                                   batch_norm = batch_norm,\n",
    "                                                   activation = activation))\n",
    "        for i in range(self.N_res_neck):\n",
    "            self.res_nets.append(ResidualBlock(channels = self.encoder_features[self.N_layers],\n",
    "                                               size     = self.encoder_sizes[self.N_layers],\n",
    "                                               cutoff_den = cutoff_den,\n",
    "                                               conv_kernel = conv_kernel,\n",
    "                                               filter_size = filter_size,\n",
    "                                               lrelu_upsampling = lrelu_upsampling,\n",
    "                                               half_width_mult  = half_width_mult,\n",
    "                                               radial = radial,\n",
    "                                               batch_norm = batch_norm,\n",
    "                                               activation = activation))\n",
    "        \n",
    "        self.res_nets = torch.nn.Sequential(*self.res_nets)    \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "                 \n",
    "        #Execute Lift ---------------------------------------------------------\n",
    "        print(\"pre-lift\", x.shape)\n",
    "        x = self.lift(x)\n",
    "        print(\"post-lift\", x.shape)\n",
    "        skip = []\n",
    "        \n",
    "        # Execute Encoder -----------------------------------------------------\n",
    "        for i in range(self.N_layers):\n",
    "            \n",
    "            #Apply ResNet & save the result\n",
    "            y = x\n",
    "            for j in range(self.N_res):\n",
    "                y = self.res_nets[i*self.N_res + j](y)\n",
    "            skip.append(y)\n",
    "            \n",
    "            # Apply (D) block\n",
    "            x = self.encoder[i](x)   \n",
    "        \n",
    "        #----------------------------------------------------------------------\n",
    "        \n",
    "        # Apply the deepest ResNet (bottle neck)\n",
    "        for j in range(self.N_res_neck):\n",
    "            x = self.res_nets[-j-1](x)\n",
    "\n",
    "        # Execute Decoder -----------------------------------------------------\n",
    "        for i in range(self.N_layers):\n",
    "            \n",
    "            # Apply (I) block (ED_expansion) & cat if needed\n",
    "            if i == 0:\n",
    "                x = self.ED_expansion[self.N_layers - i](x) #BottleNeck : no cat\n",
    "            else:\n",
    "                x = torch.cat((x, self.ED_expansion[self.N_layers - i](skip[-i])),1)\n",
    "            \n",
    "            if self.add_inv:\n",
    "                x = self.decoder_inv[i](x)\n",
    "            # Apply (U) block\n",
    "            x = self.decoder[i](x)\n",
    "        # Cat & Execute Projetion ---------------------------------------------\n",
    "        \n",
    "        x = torch.cat((x, self.ED_expansion[0](skip[0])),1)\n",
    "        print(\"pre-projection\", x.shape)\n",
    "        x = self.project(x)\n",
    "        print(\"post-projection\", x.shape)\n",
    "        \n",
    "        del skip\n",
    "        del y\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def get_n_params(self):\n",
    "        pp = 0\n",
    "        \n",
    "        for p in list(self.parameters()):\n",
    "            nn = 1\n",
    "            for s in list(p.size()):\n",
    "                nn = nn * s\n",
    "            pp += nn\n",
    "        return pp\n",
    "\n",
    "    def print_size(self):\n",
    "        nparams = 0\n",
    "        nbytes = 0\n",
    "\n",
    "        for param in self.parameters():\n",
    "            nparams += param.numel()\n",
    "            nbytes += param.data.element_size() * param.numel()\n",
    "\n",
    "        print(f'Total number of model parameters: {nparams}')\n",
    "\n",
    "        return nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c31d42f-7842-4657-871f-2c3505c35b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-lift torch.Size([2, 1, 108, 108])\n",
      "post-lift torch.Size([2, 16, 108, 108])\n",
      "pre-activation: torch.Size([2, 16, 108, 108])\n",
      "post-activation: torch.Size([2, 16, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-activation: torch.Size([2, 32, 108, 108])\n",
      "post-activation: torch.Size([2, 32, 108, 108])\n",
      "pre-projection torch.Size([2, 32, 108, 108])\n",
      "post-projection torch.Size([2, 1, 108, 108])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 108, 108])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes1 = 16\n",
    "modes2 = 16\n",
    "width = 64\n",
    "in_size = 27\n",
    "batch_size = 2\n",
    "time_steps = 1\n",
    "\n",
    "x_data = torch.rand((batch_size, time_steps, 2*in_size* 2, 2*in_size*2, )).type(torch.float32).to(device)\n",
    "\n",
    "cno = CNO(\n",
    "                 in_dim = time_steps,                    # Number of input channels.\n",
    "                 in_size=in_size,                   # Input spatial size\n",
    "                 N_layers=1,                  # Number of (D) or (U) blocks in the network\n",
    "                 N_res = 1,                 # Number of (R) blocks per level (except the neck)\n",
    "                 N_res_neck = 6,            # Number of (R) blocks in the neck\n",
    "                 channel_multiplier = 32,   # How the number of channels evolve?\n",
    "                 conv_kernel=3,             # Size of all the kernels\n",
    "                 cutoff_den = 2.0001,       # Filter property 1.\n",
    "                 filter_size=6,             # Filter property 2.\n",
    "                 lrelu_upsampling = 2,      # Filter property 3.\n",
    "                 half_width_mult  = 0.8,    # Filter property 4.\n",
    "                 radial = False,            # Filter property 5. Is filter radial?\n",
    "                 batch_norm = True,         # Add BN? We do not add BN in lifting/projection layer\n",
    "                 out_dim = 1,               # Target dimension\n",
    "                 out_size = 1,              # If out_size is 1, Then out_size = in_size. Else must be int\n",
    "                 expand_input = False,      # Start with original in_size, or expand it (pad zeros in the spectrum)\n",
    "                 latent_lift_proj_dim = 38, # Intermediate latent dimension in the lifting/projection layer (channels)\n",
    "                 add_inv = True,            # Add invariant block (I) after the intermediate connections?\n",
    "                 activation = 'cno_lrelu'   # Activation function can be 'cno_lrelu' or 'lrelu'\n",
    "                ).to(device)\n",
    "\n",
    "cno(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07dd13c7-e1b4-492c-b193-229c7033cbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 108, 108])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cd4493a-bf30-4566-9e7e-c4dc81edd46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = torch.rand((batch_size, time_steps, 128, 128, )).type(torch.float32).to(device)\n",
    "cno.lift.inter_CNOBlock.activation(x_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dffb17-f4c9-43ef-a143-7cb41e5f632c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multi-resolution inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b98d1ae-d72c-4366-8f20-13805df26776",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.rand((32, 1, 128, 128)).type(torch.float32).to(device)\n",
    "y_data = torch.ones((32, 1, 128, 128)).type(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60ba03e0-ac10-4832-b050-6fe9e0d2556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_layers = 1\n",
    "N_res = 4\n",
    "N_res_neck = 4\n",
    "channel_multiplier = 16\n",
    "\n",
    "s = 128\n",
    "\n",
    "model_args = {\n",
    "    'in_dim': 1,  # Number of input channels.\n",
    "    'out_dim': 1,  # Number of output channels.\n",
    "    #'size': s,  # Input and Output spatial size (required )\n",
    "    'n_layers': N_layers,  # Number of (D) or (U) blocks in the network\n",
    "    'n_res': N_res,  # Number of (R) blocks per level (except the neck)\n",
    "    'n_res_neck': N_res_neck,  # Number of (R) blocks in the neck\n",
    "    'channel_multiplier': channel_multiplier,  # How num channels evolve?\n",
    "    'use_bn': False,\n",
    "}\n",
    "cno = CNO2d(**model_args)\n",
    "cno = cno.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aeaa06b-b6d7-4a7b-8760-e13cb8d1f0d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [32, 10, 1, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m cno \u001b[38;5;241m=\u001b[39m cno\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m x_data_transformed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m))\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mcno\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data_transformed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/operator_aliasing/models/cno1d.py:328\u001b[0m, in \u001b[0;36mCNO1d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    326\u001b[0m original_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    327\u001b[0m x \u001b[38;5;241m=\u001b[39m resize(x, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize,))\n\u001b[0;32m--> 328\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Execute Lift\u001b[39;00m\n\u001b[1;32m    329\u001b[0m skip \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Execute Encoder\u001b[39;00m\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/operator_aliasing/models/cno1d.py:131\u001b[0m, in \u001b[0;36mLiftProjectBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 131\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minter_CNOBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvolution(x)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/operator_aliasing/models/cno1d.py:101\u001b[0m, in \u001b[0;36mCNOBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 101\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm(x)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [32, 10, 1, 128]"
     ]
    }
   ],
   "source": [
    "N_layers = 1\n",
    "N_res = 4\n",
    "N_res_neck = 4\n",
    "channel_multiplier = 16\n",
    "\n",
    "s = 128\n",
    "\n",
    "model_args = {\n",
    "    'in_dim': 10,  # Number of input channels.\n",
    "    'out_dim': 1,  # Number of output channels.\n",
    "    'size': s,  # Input and Output spatial size (required )\n",
    "    'N_layers': N_layers,  # Number of (D) or (U) blocks in the network\n",
    "    'N_res': N_res,  # Number of (R) blocks per level (except the neck)\n",
    "    'N_res_neck': N_res_neck,  # Number of (R) blocks in the neck\n",
    "    'channel_multiplier': channel_multiplier,  # How num channels evolve?\n",
    "    'use_bn': False,\n",
    "}\n",
    "cno = CNO1d(**model_args)\n",
    "cno = cno.to(device)\n",
    "\n",
    "x_data_transformed = torch.rand((32, 10,1 16)).type(torch.float32).to(device)\n",
    "cno(x_data_transformed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edc0fb5a-7f09-445b-ba80-879dfc513b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_modes = 16\n",
    "starting_modes = (max_modes, max_modes)\n",
    "model = FNO(\n",
    "    max_n_modes=(max_modes, max_modes),\n",
    "    n_modes=starting_modes,\n",
    "    hidden_channels=32,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a53c54-ff1c-48c5-b75d-b20f62eaa869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 64, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fno_architecture = {\n",
    "    'width': 64,\n",
    "    'modes': 16,\n",
    "    'FourierF': 0,  # Num Fourier Features in the input channels. Default is 0.\n",
    "    'n_layers': 4,  # Number of Fourier layers\n",
    "    'padding': 0,\n",
    "    'include_grid': 0,\n",
    "    'retrain': 4,  # Random seed\n",
    "}\n",
    "\n",
    "fno = FNO2d(fno_architecture, device=device).to(device)\n",
    "\n",
    "x_data_transformed = torch.rand((32, 64, 64, 1)).type(torch.float32).to(device)\n",
    "\n",
    "fno(x_data_transformed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18a4bb01-f31d-4cc5-a975-7b75c4a69a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 128, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efab6a55-6c71-482b-b348-b11bc4d5a9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac531f1-6e07-4f1f-a5f7-f3806ac1ff75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 128, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cno(x_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca9727f1-0933-4977-9666-0674ac7f1c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 64, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_small = torch.rand((32, 1, 64, 64)).type(torch.float32).to(device)\n",
    "y_data_small = torch.ones((32, 1, 64, 64)).type(torch.float32).to(device)\n",
    "cno(x_data_small).shape\n",
    "\n",
    "# model(x_data_small).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b074778c-5bd4-41a9-b02f-fb07f61020fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256, 256])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_large = torch.rand((1, 1, 256, 256)).type(torch.float32).to(device)\n",
    "y_data_large = torch.ones((1, 1, 256, 256)).type(torch.float32).to(device)\n",
    "cno(x_data_large).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2160caf-b28d-4410-9930-f337806923d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 0  ######### Train Loss: 0.9055739879608155  ######### Relative L1 Test Norm: 57.7748908996582\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 1  ######### Train Loss: 0.25167717672884465  ######### Relative L1 Test Norm: 52.63320207595825\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 2  ######### Train Loss: 0.1598238818347454  ######### Relative L1 Test Norm: 21.002609848976135\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 3  ######### Train Loss: 0.0730876948684454  ######### Relative L1 Test Norm: 20.69439399242401\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 4  ######### Train Loss: 0.03981395326554775  ######### Relative L1 Test Norm: 14.589525878429413\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 5  ######### Train Loss: 0.02466009221971035  ######### Relative L1 Test Norm: 15.191275537014008\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 6  ######### Train Loss: 0.01775022381916642  ######### Relative L1 Test Norm: 11.939327001571655\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 7  ######### Train Loss: 0.010724165989086033  ######### Relative L1 Test Norm: 11.165721952915192\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 8  ######### Train Loss: 0.019835104001685977  ######### Relative L1 Test Norm: 20.73923969268799\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 9  ######### Train Loss: 0.03328498362097889  ######### Relative L1 Test Norm: 14.320962846279144\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 10  ######### Train Loss: 0.019196551712229847  ######### Relative L1 Test Norm: 5.934997051954269\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 11  ######### Train Loss: 0.013836816023103892  ######### Relative L1 Test Norm: 16.565542936325073\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 12  ######### Train Loss: 0.0204890004824847  ######### Relative L1 Test Norm: 5.3831963539123535\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 13  ######### Train Loss: 0.016350756445899606  ######### Relative L1 Test Norm: 8.301013350486755\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 14  ######### Train Loss: 0.04209347409196198  ######### Relative L1 Test Norm: 22.714921236038208\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 15  ######### Train Loss: 0.023178669367916883  ######### Relative L1 Test Norm: 14.617516994476318\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 16  ######### Train Loss: 0.01646250020712614  ######### Relative L1 Test Norm: 15.028542816638947\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 17  ######### Train Loss: 0.014920321735553443  ######### Relative L1 Test Norm: 7.615252673625946\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 18  ######### Train Loss: 0.014300728915259242  ######### Relative L1 Test Norm: 14.405225813388824\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 19  ######### Train Loss: 0.019231191952712834  ######### Relative L1 Test Norm: 5.602485746145248\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 20  ######### Train Loss: 0.007936568465083838  ######### Relative L1 Test Norm: 8.887517511844635\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 21  ######### Train Loss: 0.007859216444194317  ######### Relative L1 Test Norm: 7.601627886295319\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 22  ######### Train Loss: 0.00781525990460068  ######### Relative L1 Test Norm: 11.896973729133606\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 23  ######### Train Loss: 0.007785650342702866  ######### Relative L1 Test Norm: 9.838278353214264\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 24  ######### Train Loss: 0.0070966253988444805  ######### Relative L1 Test Norm: 7.581246435642242\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 25  ######### Train Loss: 0.003634190559387207  ######### Relative L1 Test Norm: 5.930107891559601\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 26  ######### Train Loss: 0.0030647005885839464  ######### Relative L1 Test Norm: 5.152249783277512\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 27  ######### Train Loss: 0.002791524026542902  ######### Relative L1 Test Norm: 4.967260152101517\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 28  ######### Train Loss: 0.002452390524558723  ######### Relative L1 Test Norm: 5.203385561704636\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 29  ######### Train Loss: 0.002589865354821086  ######### Relative L1 Test Norm: 5.083554685115814\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 30  ######### Train Loss: 0.002473056106828153  ######### Relative L1 Test Norm: 5.031180769205093\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 31  ######### Train Loss: 0.0024922051932662727  ######### Relative L1 Test Norm: 5.007603973150253\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 32  ######### Train Loss: 0.002501706825569272  ######### Relative L1 Test Norm: 4.987777262926102\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 33  ######### Train Loss: 0.0024460564134642484  ######### Relative L1 Test Norm: 5.030503153800964\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 34  ######### Train Loss: 0.002482384000904858  ######### Relative L1 Test Norm: 4.898721486330032\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 35  ######### Train Loss: 0.0024414320243522523  ######### Relative L1 Test Norm: 4.857206642627716\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 36  ######### Train Loss: 0.0024523244705051185  ######### Relative L1 Test Norm: 4.832018822431564\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 37  ######### Train Loss: 0.0024294022703543307  ######### Relative L1 Test Norm: 5.031657665967941\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 38  ######### Train Loss: 0.002463870961219072  ######### Relative L1 Test Norm: 5.045852154493332\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 39  ######### Train Loss: 0.002392797847278416  ######### Relative L1 Test Norm: 4.860679030418396\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 40  ######### Train Loss: 0.0022901520365849136  ######### Relative L1 Test Norm: 4.6938157081604\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 41  ######### Train Loss: 0.002199833421036601  ######### Relative L1 Test Norm: 4.678664833307266\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 42  ######### Train Loss: 0.0021741229575127363  ######### Relative L1 Test Norm: 4.649824023246765\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 43  ######### Train Loss: 0.0021493012318387628  ######### Relative L1 Test Norm: 4.62138819694519\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 44  ######### Train Loss: 0.0021238300716504453  ######### Relative L1 Test Norm: 4.5931116938591\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 45  ######### Train Loss: 0.0021034441655501724  ######### Relative L1 Test Norm: 4.578712552785873\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 46  ######### Train Loss: 0.002089642523787916  ######### Relative L1 Test Norm: 4.563655346632004\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 47  ######### Train Loss: 0.002075689984485507  ######### Relative L1 Test Norm: 4.548003762960434\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 48  ######### Train Loss: 0.002061313670128584  ######### Relative L1 Test Norm: 4.532163918018341\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "torch.Size([10, 128, 128, 1])\n",
      "######### Epoch: 49  ######### Train Loss: 0.0020467838272452354  ######### Relative L1 Test Norm: 4.515728771686554\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# --------------------------------------\n",
    "# REPLACE THIS PART BY YOUR DATALOADER\n",
    "# --------------------------------------\n",
    "\n",
    "n_train = 100  # number of training samples\n",
    "\n",
    "x_data = (\n",
    "    torch.rand((256, 1, 128, 128))\n",
    "    .type(torch.float32)\n",
    "    .reshape(256, 128, 128, 1)\n",
    ")\n",
    "y_data = (\n",
    "    torch.ones((256, 1, 128, 128))\n",
    "    .type(torch.float32)\n",
    "    .reshape(256, 128, 128, 1)\n",
    ")\n",
    "\n",
    "input_function_train = x_data[:n_train, :]\n",
    "output_function_train = y_data[:n_train, :]\n",
    "input_function_test = x_data[n_train:, :]\n",
    "output_function_test = y_data[n_train:, :]\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "training_set = DataLoader(\n",
    "    TensorDataset(input_function_train, output_function_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "testing_set = DataLoader(\n",
    "    TensorDataset(input_function_test, output_function_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# Define the hyperparameters and the model:\n",
    "# ---------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "step_size = 15\n",
    "gamma = 0.5\n",
    "\n",
    "N_layers = 4\n",
    "N_res = 4\n",
    "N_res_neck = 4\n",
    "channel_multiplier = 16\n",
    "\n",
    "s = 128\n",
    "\n",
    "cno = CNO2d(**model_args)\n",
    "\n",
    "# -----------\n",
    "# TRAIN:\n",
    "# -----------\n",
    "\n",
    "cno = cno.to(device)\n",
    "\n",
    "# cno = model.to(device)\n",
    "\n",
    "cno = fno.to(device)\n",
    "\n",
    "optimizer = AdamW(cno.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=step_size, gamma=gamma\n",
    ")\n",
    "\n",
    "loss = nn.L1Loss()\n",
    "freq_print = 1\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_mse = 0.0\n",
    "    for _step, (input_data, output_data) in enumerate(training_set):\n",
    "        input_batch = input_data.to(device)\n",
    "        output_batch = output_data.to(device)\n",
    "        print(input_batch.shape)\n",
    "        optimizer.zero_grad()\n",
    "        output_pred_batch = cno(input_batch)\n",
    "        loss_f = loss(output_pred_batch, output_batch)\n",
    "        loss_f.backward()\n",
    "        optimizer.step()\n",
    "        train_mse += loss_f.item()\n",
    "    train_mse /= len(training_set)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cno.eval()\n",
    "        test_relative_l2 = 0.0\n",
    "        for _step, (input_data, output_data) in enumerate(testing_set):\n",
    "            input_batch = input_data.to(device)\n",
    "            output_batch = output_data.to(device)\n",
    "            output_pred_batch = cno(input_batch)\n",
    "            loss_f = (\n",
    "                torch.mean(abs(output_pred_batch - output_batch))\n",
    "                / torch.mean(abs(output_batch))\n",
    "            ) ** 0.5 * 100\n",
    "            test_relative_l2 += loss_f.item()\n",
    "        test_relative_l2 /= len(testing_set)\n",
    "\n",
    "    if epoch % freq_print == 0:\n",
    "        print(\n",
    "            '######### Epoch:',\n",
    "            epoch,\n",
    "            ' ######### Train Loss:',\n",
    "            train_mse,\n",
    "            ' ######### Relative L1 Test Norm:',\n",
    "            test_relative_l2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d46ebe86-8493-48a9-910c-2398603c2476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_preds(\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    # data_transform: DataProcessor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Return model predictions.\"\"\"\n",
    "    model_preds = []\n",
    "    for _idx, sample in enumerate(test_loader):  # resolution 128\n",
    "        model_input = sample['x'].to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(model_input)\n",
    "            model_preds.append(out)\n",
    "    return torch.cat(model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef00da32-4c24-42a2-872c-af6c0c891123",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _idx, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtest_loader\u001b[49m):  \u001b[38;5;66;03m# resolution 128\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for _idx, sample in enumerate(test_loader):  # resolution 128\n",
    "    model_input = sample['x'].to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(model_input)\n",
    "        model_preds.append(out)\n",
    "return torch.cat(model_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "operator_alias",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
