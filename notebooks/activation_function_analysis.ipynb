{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d399fc27-ec7b-445c-bd5f-93ac1bca8fb1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e98cabb-752e-4f29-8c2e-190639abd46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "# Specific to NERSC: Set up kernel using: https://docs.nersc.gov/services/jupyter/how-to-guides/\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# might have issues with too many files being opened at once\n",
    "# this will prevent that\n",
    "import torch.multiprocessing\n",
    "import torch.nn.functional as f\n",
    "from neuralop import H1Loss\n",
    "from neuralop import LpLoss\n",
    "from neuralop.data.datasets.darcy import DarcyDataset\n",
    "from neuralop.data.transforms.data_processors import IncrementalDataProcessor\n",
    "from neuralop.models import FNO\n",
    "from neuralop.training import AdamW\n",
    "from neuralop.training.incremental import IncrementalFNOTrainer\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.utils import get_project_root\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from operator_aliasing.utils import get_energy_curve\n",
    "from operator_aliasing.utils import get_model_preds\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "root_dir = get_project_root() / 'neuralop/data/datasets/data'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device=}')\n",
    "\n",
    "FIG_DIR = '../figures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24acd538-c496-4db3-901f-443ef8c47327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdaf-filter==0.1.1 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from hdaf-filter==0.1.1) (3.10.3)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from hdaf-filter==0.1.1) (1.26.4)\n",
      "Requirement already satisfied: setuptools>=57.4.0 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from hdaf-filter==0.1.1) (80.8.0)\n",
      "Requirement already satisfied: tifffile>=2021.11.2 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from hdaf-filter==0.1.1) (2025.5.10)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from matplotlib>=3.2.2->hdaf-filter==0.1.1) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from matplotlib>=3.2.2->hdaf-filter==0.1.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from matplotlib>=3.2.2->hdaf-filter==0.1.1) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from matplotlib>=3.2.2->hdaf-filter==0.1.1) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from matplotlib>=3.2.2->hdaf-filter==0.1.1) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from matplotlib>=3.2.2->hdaf-filter==0.1.1) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from matplotlib>=3.2.2->hdaf-filter==0.1.1) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from matplotlib>=3.2.2->hdaf-filter==0.1.1) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /pscratch/sd/m/mansisak/operator_aliasing/env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->hdaf-filter==0.1.1) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install a filtering library and include a versioning work around\n",
    "%pip install hdaf-filter==0.1.1\n",
    "\n",
    "# Monkeypatch\n",
    "from hdaf_filter import hdaf\n",
    "from scipy.special import factorial\n",
    "\n",
    "np.math = type('math', (), {})()\n",
    "np.math.factorial = factorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f56e9-c98d-4b66-a82a-bfeaed16bb11",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5af40d8-508e-4885-adef-e6384a6474f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 16 with 32 samples \n",
      "Loading test db for resolution 32 with 32 samples \n",
      "Loading test db for resolution 64 with 32 samples \n",
      "Loading test db for resolution 128 with 32 samples \n",
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 100 samples \n",
      "Loading test db for resolution 64 with 100 samples \n",
      "Loading test db for resolution 128 with 100 samples \n"
     ]
    }
   ],
   "source": [
    "# first download data\n",
    "data = DarcyDataset(\n",
    "    root_dir=root_dir,\n",
    "    n_train=100,\n",
    "    n_tests=[32, 32, 32, 32],\n",
    "    batch_size=16,\n",
    "    test_batch_sizes=[16, 16, 16, 16],\n",
    "    train_resolution=128,  # change resolution to download different data\n",
    "    test_resolutions=[16, 32, 64, 128],\n",
    ")\n",
    "\n",
    "# load darcy flow dataset\n",
    "\n",
    "\n",
    "def load_darcy_flow_small(\n",
    "    n_train,\n",
    "    n_tests,\n",
    "    data_root=root_dir,\n",
    "    test_resolutions=(16, 32),\n",
    "    train_resolution=16,\n",
    "):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    batch_size = 16\n",
    "    test_batch_sizes = [batch_size] * len(test_resolutions)\n",
    "\n",
    "    dataset = DarcyDataset(\n",
    "        root_dir=data_root,\n",
    "        n_train=n_train,\n",
    "        n_tests=n_tests,\n",
    "        batch_size=batch_size,\n",
    "        test_batch_sizes=test_batch_sizes,\n",
    "        train_resolution=train_resolution,\n",
    "        test_resolutions=test_resolutions,\n",
    "        encode_input=False,\n",
    "        encode_output=True,\n",
    "        channel_dim=1,\n",
    "        encoding='channel-wise',\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    # return dataloaders for backwards compat\n",
    "    train_loader = DataLoader(\n",
    "        dataset.train_db,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    test_loaders = {}\n",
    "    for res, test_bsize in zip(test_resolutions, test_batch_sizes):\n",
    "        test_loaders[res] = DataLoader(\n",
    "            dataset.test_dbs[res],\n",
    "            batch_size=test_bsize,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "\n",
    "    return train_loader, test_loaders, dataset.data_processor\n",
    "\n",
    "\n",
    "train_resolution = 16\n",
    "train_loader, test_loaders, output_encoder = load_darcy_flow_small(\n",
    "    n_train=1000,\n",
    "    # batch_size=16,\n",
    "    train_resolution=train_resolution,\n",
    "    test_resolutions=[16, 32, 64, 128],\n",
    "    n_tests=[100, 100, 100, 100],\n",
    "    # test_batch_sizes=[32, 32, 32, 32],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0db487be-647d-4c50-a70f-9fd36c6f4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _idx, batch in enumerate(test_loaders[16]):\n",
    "    low_res_batch = batch\n",
    "    break\n",
    "\n",
    "for _idx, batch in enumerate(test_loaders[128]):\n",
    "    high_res_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e7088f-1d7e-4ca9-a0ca-ed804fc645ac",
   "metadata": {},
   "source": [
    "# Train FNO at multiple resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c593f-1964-4a16-97ba-b08c554afb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 100 samples \n",
      "Loading test db for resolution 64 with 100 samples \n",
      "Loading test db for resolution 128 with 100 samples \n",
      "Original Incre Res: change index to 0\n",
      "Original Incre Res: change sub to 2\n",
      "Original Incre Res: change res to 8\n",
      "Incre Res Update: change index to 1\n",
      "Incre Res Update: change sub to 1\n",
      "Incre Res Update: change res to 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2092808/218557674.py:117: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 100 samples \n",
      "Loading test db for resolution 64 with 100 samples \n",
      "Loading test db for resolution 128 with 100 samples \n",
      "Original Incre Res: change index to 0\n",
      "Original Incre Res: change sub to 2\n",
      "Original Incre Res: change res to 8\n",
      "Incre Res Update: change index to 1\n",
      "Incre Res Update: change sub to 1\n",
      "Incre Res Update: change res to 16\n",
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 100 samples \n",
      "Loading test db for resolution 64 with 100 samples \n",
      "Loading test db for resolution 128 with 100 samples \n",
      "Original Incre Res: change index to 0\n",
      "Original Incre Res: change sub to 2\n",
      "Original Incre Res: change res to 8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "class NoAct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NoAct, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\"\"\"\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        'train_resolution',\n",
    "        'test_resolution',\n",
    "        'loss_type',\n",
    "        'loss',\n",
    "        'max_modes',\n",
    "    ],\n",
    ")\n",
    "modes = {}\n",
    "for max_modes in [16]:\n",
    "    trained_models = {}  # at different resolutions\n",
    "    for train_resolution in [16, 32, 64, 128]:\n",
    "        # get data\n",
    "        train_loader, test_loaders, output_encoder = load_darcy_flow_small(\n",
    "            n_train=1000,\n",
    "            # batch_size=16,\n",
    "            train_resolution=train_resolution,\n",
    "            test_resolutions=[16, 32, 64, 128],\n",
    "            n_tests=[100, 100, 100, 100],\n",
    "            # test_batch_sizes=[32, 32, 32, 32],\n",
    "        )\n",
    "\n",
    "        # incrementally vary modes\n",
    "        incremental = True\n",
    "        starting_modes = (max_modes, max_modes)\n",
    "        if incremental:\n",
    "            starting_modes = (2, 2)\n",
    "\n",
    "        # Set up model\n",
    "        model = FNO(\n",
    "            max_n_modes=(max_modes, max_modes),\n",
    "            n_modes=starting_modes,\n",
    "            hidden_channels=32,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            # non_linearity=NoAct(),\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        n_params = count_model_params(model)\n",
    "\n",
    "        # optimizer + data\n",
    "        optimizer = AdamW(model.parameters(), lr=8e-3, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=30\n",
    "        )\n",
    "\n",
    "        data_transform = IncrementalDataProcessor(\n",
    "            in_normalizer=None,\n",
    "            out_normalizer=None,\n",
    "            device=device,\n",
    "            subsampling_rates=[2, 1],\n",
    "            dataset_resolution=16,\n",
    "            dataset_indices=[2, 3],\n",
    "            epoch_gap=10,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        data_transform = data_transform.to(device)\n",
    "\n",
    "        l2loss = LpLoss(d=2, p=2)\n",
    "        h1loss = H1Loss(d=2)\n",
    "        train_loss = h1loss\n",
    "        eval_losses = {'h1': h1loss, 'l2': l2loss}\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Finally pass all of these to the Trainer\n",
    "        trainer = IncrementalFNOTrainer(\n",
    "            model=model,\n",
    "            n_epochs=20,\n",
    "            data_processor=data_transform,\n",
    "            device=device,\n",
    "            verbose=False,\n",
    "            incremental_loss_gap=False,\n",
    "            incremental_grad=True,\n",
    "            incremental_grad_eps=0.9999,\n",
    "            incremental_loss_eps=0.001,\n",
    "            incremental_buffer=5,\n",
    "            incremental_max_iter=1,\n",
    "            incremental_grad_max_iter=2,\n",
    "        )\n",
    "\n",
    "        # train\n",
    "        end_stats = trainer.train(\n",
    "            train_loader,\n",
    "            test_loaders,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            regularizer=False,\n",
    "            training_loss=train_loss,\n",
    "            eval_losses=eval_losses,\n",
    "        )\n",
    "\n",
    "        trained_models[train_resolution] = trainer.model\n",
    "\n",
    "        # save stats\n",
    "        for k, v in end_stats.items():\n",
    "            s = k.split('_')\n",
    "\n",
    "            if 'h1' in s or ('l2' in s):\n",
    "                row = {\n",
    "                    'train_resolution': train_resolution,\n",
    "                    'test_resolution': s[0],\n",
    "                    'loss_type': s[1],\n",
    "                    'loss': v.item(),\n",
    "                    'max_modes': max_modes,\n",
    "                }\n",
    "                df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    modes[max_modes] = trained_models\n",
    "    # Define colormap from green (high engagement) to red (low engagement)\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "        'green_red',\n",
    "        ['green', 'yellow', 'red'],\n",
    "        N=100,\n",
    "    )\n",
    "    for loss_type in ['l2', 'h1']:\n",
    "        heat = df[\n",
    "            (df.loss_type == loss_type) & (df.max_modes == max_modes)\n",
    "        ].pivot(\n",
    "            index='train_resolution',\n",
    "            columns='test_resolution',\n",
    "            values='loss',\n",
    "        )\n",
    "        heat = heat.loc[:, ['16', '32', '64', '128']]\n",
    "        sns.heatmap(heat, cmap=cmap)\n",
    "        plt.title(f'{loss_type=} {max_modes=}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec22578-3369-4266-b3be-29e4c028fba9",
   "metadata": {},
   "source": [
    "# Plot Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f19befd0-3988-48d9-b67b-38a53cb1e0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f81791bb5b0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spectrum = get_energy_curve(high_res_batch['x'].squeeze())\n",
    "plt.plot(spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11332965-f39e-4ab2-a190-e7e601656995",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrum_func in [get_energy_curve]:\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(8, 4), sharey='row')\n",
    "\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    for idx, s in enumerate([16, 32, 64, 128]):\n",
    "        ## Define some variables\n",
    "        T = 500  # number of time steps\n",
    "        samples = 50\n",
    "        # s = 16 # resolution of the dataset\n",
    "\n",
    "        # additional paramaters for the dataset\n",
    "        Re = 5000\n",
    "        index = 1\n",
    "        T = 100\n",
    "        dataset_name = 'Darcy Flow'\n",
    "\n",
    "        train_loader, test_loaders, output_encoder = load_darcy_flow_small(\n",
    "            n_train=1000,\n",
    "            # batch_size=16,\n",
    "            train_resolution=s,\n",
    "            test_resolutions=[16, 32, 64, 128],\n",
    "            n_tests=[100, 100, 100, 100],\n",
    "            # test_batch_sizes=[32, 32, 32, 32],\n",
    "        )\n",
    "\n",
    "        # It is important to note that we want the last two dimensions\n",
    "        # to represent the spatial dimensions\n",
    "        # So in some cases one might have to permute the\n",
    "        # dataset after squeezing\n",
    "        # the initial dimensions as well\n",
    "\n",
    "        # squeeze the dataset to remove the empty channel dimension\n",
    "        dataset_pred = train_loader.dataset[:samples]['x'].squeeze()\n",
    "        dataset_pred_y = train_loader.dataset[:samples]['y'].squeeze()\n",
    "        model_preds_train = get_model_preds(\n",
    "            train_loader, model, data_transform=data_transform\n",
    "        ).squeeze()\n",
    "        model_preds_test = get_model_preds(\n",
    "            test_loaders[s], model, data_transform=data_transform\n",
    "        ).squeeze()\n",
    "        # Shape of the dataset\n",
    "        shape = dataset_pred.shape\n",
    "\n",
    "        # Generate the spectrum of the dataset\n",
    "        # We reshape our samples into the form expected by ``spectrum_2d``:\n",
    "        # ``(n_samples, h, w)``\n",
    "        train_sp_x = spectrum_func(dataset_pred.reshape(samples * 1, s, s))\n",
    "        train_sp_y = spectrum_func(dataset_pred_y.reshape(samples * 1, s, s))\n",
    "        train_sp_model_pred = spectrum_func(\n",
    "            model_preds_train[:samples].reshape(samples * 1, s, s)\n",
    "        )\n",
    "\n",
    "        test_sp_x = spectrum_func(\n",
    "            test_loaders[s]\n",
    "            .dataset[:samples]['x']\n",
    "            .squeeze()\n",
    "            .reshape(samples * 1, s, s)\n",
    "        )\n",
    "        test_sp_y = spectrum_func(\n",
    "            test_loaders[s]\n",
    "            .dataset[:samples]['y']\n",
    "            .squeeze()\n",
    "            .reshape(samples * 1, s, s)\n",
    "        )\n",
    "        test_sp_model_pred = spectrum_func(\n",
    "            model_preds_test[:samples].reshape(samples * 1, s, s)\n",
    "        )\n",
    "        # Configure pyplot and generate the plot\n",
    "\n",
    "        ax[idx].set_yscale('log')\n",
    "\n",
    "        length = dataset_pred.shape[-1]  # the resolution length of the dataset\n",
    "        buffer = 10  # just add a buffer to the plot\n",
    "        k = np.arange(length + buffer) * 1.0\n",
    "        ax[idx].plot(train_sp_x, linestyle=':', label='train_x')\n",
    "        ax[idx].plot(train_sp_y, linestyle='-', label='train_y')\n",
    "        ax[idx].plot(test_sp_x, linestyle=':', label='test_x')\n",
    "        ax[idx].plot(test_sp_y, linestyle='-', label='test_y')\n",
    "        ax[idx].plot(\n",
    "            test_sp_model_pred, linestyle='-', label='test_model_pred'\n",
    "        )\n",
    "        ax[idx].plot(\n",
    "            train_sp_model_pred, linestyle='-', label='train_model_pred'\n",
    "        )\n",
    "\n",
    "        ax[idx].set_title(f'Resolution: {s}')\n",
    "        fig.suptitle(f'Spectrum of {dataset_name} Dataset')\n",
    "\n",
    "    fig.supxlabel('wavenumber')\n",
    "    fig.supylabel('energy')\n",
    "\n",
    "    # show the figure\n",
    "    leg = plt.legend(loc='best')\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.show()\n",
    "    fig.savefig(\n",
    "        f'{FIG_DIR}/{spectrum_func.__name__}_cross_resolution_spectral_analysis.pdf',\n",
    "        bbox_inches='tight',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0876d-4a90-4e63-ab46-37742b78d75b",
   "metadata": {},
   "source": [
    "# filter training experiment to test super-resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca57a7-1ac3-4948-b12c-54496b6e03ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = data_transform.to(device)\n",
    "for _idx, sample in enumerate(test_loaders[16]):  # resolution 128\n",
    "    model_input = data_transform.preprocess(sample)\n",
    "\n",
    "    # grab a batch\n",
    "    model_input = sample['x']  # [0, :, :, :].unsqueeze(dim=0)\n",
    "    ground_truth = sample['y']  # [0, :, :, :].unsqueeze(dim=0)\n",
    "    break\n",
    "\n",
    "# grab image to initialize filter with\n",
    "filter_initializer_img = model_input[0, 0, :, :].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c9cd4-44b9-46b7-a41c-5c33aac7f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img_and_power_spectrum(image: np.array):\n",
    "    \"\"\"Show image and its corresponding energy spectrum.\"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(5, 2))\n",
    "\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title('Spatial Domain')\n",
    "\n",
    "    spectrum = spectrum_func(torch.tensor(image).unsqueeze(dim=0))\n",
    "    axs[1].plot(spectrum)\n",
    "    axs[1].set_title('spectrum')\n",
    "    axs[1].set_ylabel('Energy')\n",
    "    axs[1].set_xlabel('Wavenumber')\n",
    "    # axs[1].set_xscale(\"log\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_img_and_power_spectrum(filter_initializer_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb7319-d3bb-4461-8e61-cf3bab6b2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img_and_power_spectrum(filter_initializer_img)\n",
    "show_img_and_power_spectrum(ground_truth[0, 0, :, :].cpu())\n",
    "\n",
    "# initialize filter\n",
    "filter_obj = hdaf.filt(filter_initializer_img)\n",
    "\n",
    "low_freq_limit = 3\n",
    "low_pass_filter = filter_obj.get_filter('low_pass', low_freq_limit)\n",
    "plt.imshow(low_pass_filter[0])\n",
    "plt.show()\n",
    "low_pass_filter = torch.tensor(low_pass_filter, device=device)\n",
    "\n",
    "# fft batch\n",
    "batch_fourier = torch.fft.fftn(ground_truth, dim=(-2, -1))\n",
    "\n",
    "# center batch, filter\n",
    "filter_centered = torch.fft.fftshift(low_pass_filter)\n",
    "fourier_centered = torch.fft.fftshift(batch_fourier)\n",
    "\n",
    "# apply filter\n",
    "filtered_batch = fourier_centered * filter_centered\n",
    "\n",
    "# visualize a filtered image in FFT space\n",
    "plt.imshow(np.log(np.abs(1 + filtered_batch[0, 0, :, :].cpu())))\n",
    "plt.show()\n",
    "\n",
    "# convert batch back to spatial domain\n",
    "batch = torch.real(\n",
    "    torch.fft.ifftn(torch.fft.ifftshift(filtered_batch), dim=(-1, -2))\n",
    ")\n",
    "show_img_and_power_spectrum(batch[0, 0, :, :].cpu())\n",
    "\n",
    "# show diff from original and filtered\n",
    "\n",
    "show_img_and_power_spectrum(\n",
    "    ground_truth[0, 0, :, :].cpu() - batch[0, 0, :, :].cpu()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389da9eb-120b-44a0-beb1-952f6be87df3",
   "metadata": {},
   "source": [
    "# Playing w/ spectral conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e1985f-d58b-4dbf-8b37-a94c7babb5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "out_channels = 1\n",
    "modes1 = 8\n",
    "modes2 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5154f148-0617-4de4-9186-d2e8ba89f858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass SpectralConv2d(nn.Module):\\n    def __init__(\\n        self,\\n        in_channels,  # Number of input channels\\n        out_channels,  # Number of output channels\\n        modes1,  # Number of Fourier modes to multiply in the first dimension\\n        modes2,\\n    ):  # Number of Fourier modes to multiply in the second dimension\\n        super(SpectralConv2d, self).__init__()\\n\\n        self.in_channels = in_channels\\n        self.out_channels = out_channels\\n        self.modes1 = modes1\\n        self.modes2 = modes2\\n\\n        self.scale = 1 / (in_channels * out_channels)\\n        self.weights1 = nn.Parameter(\\n            self.scale\\n            * torch.rand(\\n                in_channels,\\n                out_channels,\\n                self.modes1,\\n                self.modes2,\\n                dtype=torch.cfloat,\\n            )\\n        )\\n        self.weights2 = nn.Parameter(\\n            self.scale\\n            * torch.rand(\\n                in_channels,\\n                out_channels,\\n                self.modes1,\\n                self.modes2,\\n                dtype=torch.cfloat,\\n            )\\n        )\\n\\n    def forward(self, x):\\n        batchsize = x.shape[0]\\n        # Compute Fourier coeffcients\\n        x_ft = torch.fft.rfft2(x)\\n\\n        # Multiply relevant Fourier modes\\n        out_ft = torch.zeros(\\n            batchsize,\\n            self.out_channels,\\n            x.size(-2),\\n            x.size(-1) // 2 + 1,\\n            dtype=torch.cfloat,\\n            device=x.device,\\n        )\\n        out_ft[:, :, : self.modes1, : self.modes2] = self.compl_mul2d(\\n            x_ft[:, :, : self.modes1, : self.modes2], self.weights1\\n        )\\n        out_ft[:, :, -self.modes1 :, : self.modes2] = self.compl_mul2d(\\n            x_ft[:, :, -self.modes1 :, : self.modes2], self.weights2\\n        )\\n\\n        # Return to physical space\\n        print(f'{out_ft.shape=}, {(x.size(-2), x.size(-1))=}')\\n        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\\n        return x\\n\\n    def compl_mul2d(self, input, weights):\\n        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\\n        print(f'{input.shape=}')\\n        return torch.einsum('bixy,ioxy->boxy', input, weights)\\n\\n\\nin_channels = 1\\nout_channels = 1\\nmodes1 = 8\\nmodes2 = 8\\nspectral_conv = SpectralConv2d(\\n    in_channels=in_channels,\\n    out_channels=out_channels,\\n    modes1=modes1,\\n    modes2=modes2,\\n)\\nspectral_conv.forward(low_res_batch['x']).shape\\nspectral_conv.forward(high_res_batch['x']).shape\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,  # Number of input channels\n",
    "        out_channels,  # Number of output channels\n",
    "        modes1,  # Number of Fourier modes to multiply in the first dimension\n",
    "        modes2,\n",
    "    ):  # Number of Fourier modes to multiply in the second dimension\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = 1 / (in_channels * out_channels)\n",
    "        self.weights1 = nn.Parameter(\n",
    "            self.scale\n",
    "            * torch.rand(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                self.modes1,\n",
    "                self.modes2,\n",
    "                dtype=torch.cfloat,\n",
    "            )\n",
    "        )\n",
    "        self.weights2 = nn.Parameter(\n",
    "            self.scale\n",
    "            * torch.rand(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                self.modes1,\n",
    "                self.modes2,\n",
    "                dtype=torch.cfloat,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        # Compute Fourier coeffcients\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(\n",
    "            batchsize,\n",
    "            self.out_channels,\n",
    "            x.size(-2),\n",
    "            x.size(-1) // 2 + 1,\n",
    "            dtype=torch.cfloat,\n",
    "            device=x.device,\n",
    "        )\n",
    "        out_ft[:, :, : self.modes1, : self.modes2] = self.compl_mul2d(\n",
    "            x_ft[:, :, : self.modes1, : self.modes2], self.weights1\n",
    "        )\n",
    "        out_ft[:, :, -self.modes1 :, : self.modes2] = self.compl_mul2d(\n",
    "            x_ft[:, :, -self.modes1 :, : self.modes2], self.weights2\n",
    "        )\n",
    "\n",
    "        # Return to physical space\n",
    "        print(f'{out_ft.shape=}, {(x.size(-2), x.size(-1))=}')\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) \n",
    "        # -> (batch, out_channel, x,y)\n",
    "        print(f'{input.shape=}')\n",
    "        return torch.einsum('bixy,ioxy->boxy', input, weights)\n",
    "\n",
    "\n",
    "\n",
    "spectral_conv = SpectralConv2d(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    modes1=modes1,\n",
    "    modes2=modes2,\n",
    ")\n",
    "spectral_conv.forward(low_res_batch['x']).shape\n",
    "spectral_conv.forward(high_res_batch['x']).shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81471c9c-d46d-4000-8473-81ac760f67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralop.layers.spectral_convolution import SpectralConv\n",
    "\n",
    "modes1 = 400\n",
    "modes2 = 400\n",
    "fourier_layer = SpectralConv(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    n_modes=(modes1, modes2),\n",
    ")\n",
    "fourier_layer.forward(high_res_batch['x']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00d664-be07-422e-9213-006a0e48728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_modes = 400\n",
    "starting_modes = (max_modes, max_modes)\n",
    "model = FNO(\n",
    "    max_n_modes=(max_modes, max_modes),\n",
    "    n_modes=starting_modes,\n",
    "    hidden_channels=32,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    ")\n",
    "model(high_res_batch['x']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07ff44-9023-459b-b707-7c920056e8e8",
   "metadata": {},
   "source": [
    "# Testing Activation Functions\n",
    "\n",
    "- try style gan activation function (CNO), and see what happens\n",
    "\n",
    "- https://github.com/camlab-ethz/AI_Science_Engineering/blob/main/Tutorial%2006%20-%20Operator%20Learing%20-%20Convolutional%20Neural%20Operator.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c786fc03-67e4-4671-8401-5c90ee5db652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNOLReLu(nn.Module):\n",
    "    \"\"\"Same activation function as StyleGAN3: meant to prevent aliasing.\"\"\"\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        \"\"\"Initilize StyleGan Relu.\"\"\"\n",
    "        super(__class__, self).__init__()\n",
    "\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Upsample, activation, downsample.\"\"\"\n",
    "        x = f.interpolate(\n",
    "            x.unsqueeze(2),\n",
    "            size=(1, 2 * self.in_size),\n",
    "            mode='bicubic',\n",
    "            antialias=True,\n",
    "        )\n",
    "        x = self.act(x)\n",
    "        x = f.interpolate(\n",
    "            x, size=(1, self.out_size), mode='bicubic', antialias=True\n",
    "        )\n",
    "\n",
    "        return x[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6df74-377f-433d-ba15-7c6e94945045",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_modes = 16\n",
    "starting_modes = (max_modes, max_modes)\n",
    "model = FNO(\n",
    "    max_n_modes=(max_modes, max_modes),\n",
    "    n_modes=starting_modes,\n",
    "    hidden_channels=32,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    # non_linearity=NoAct(),\n",
    ")\n",
    "\n",
    "model(high_res_batch['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078dfad-7719-47b8-834e-7d72f38fdb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fno_blocks.convs[0](high_res_batch['x']).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "operator_alias",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
