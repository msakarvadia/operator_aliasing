{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2553ea-f934-452c-b4f4-19f5c9ddbb5a",
   "metadata": {},
   "source": [
    "# Imports / Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124d5e8e-ced5-4528-b587-103ab269e4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "# Specific to NERSC: Set up kernel using: https://docs.nersc.gov/services/jupyter/how-to-guides/\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# might have issues with too many files being opened at once\n",
    "# this will prevent that\n",
    "import torch.multiprocessing\n",
    "import torch.nn.functional as f\n",
    "from neuralop import H1Loss\n",
    "from neuralop import LpLoss\n",
    "from neuralop.data.datasets.darcy import DarcyDataset\n",
    "from neuralop.models import FNO\n",
    "from neuralop.training import AdamW\n",
    "from neuralop.training.incremental import IncrementalFNOTrainer\n",
    "from neuralop.utils import get_project_root\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from operator_aliasing.utils import filter_batch\n",
    "from operator_aliasing.utils import get_2d_low_pass_filter\n",
    "from operator_aliasing.utils import get_energy_curve\n",
    "from operator_aliasing.utils import get_model_preds\n",
    "from operator_aliasing.utils import lowpass_filter_dataloader\n",
    "from operator_aliasing.utils import get_2d_low_pass_filter\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "root_dir = get_project_root() / 'neuralop/data/datasets/data'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device=}')\n",
    "\n",
    "FIG_DIR = '../figures'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239dcc5a-322d-4746-8f3c-6202d78dc0e0",
   "metadata": {},
   "source": [
    "# Basic Filter to NeuralOp Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae6a662-c2d3-4c06-9dd1-ba86ca1332b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 16 with 32 samples \n",
      "Loading test db for resolution 32 with 32 samples \n",
      "Loading test db for resolution 64 with 32 samples \n",
      "Loading test db for resolution 128 with 32 samples \n",
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 100 samples \n",
      "Loading test db for resolution 64 with 100 samples \n",
      "Loading test db for resolution 128 with 100 samples \n"
     ]
    }
   ],
   "source": [
    "# first download data\n",
    "data = DarcyDataset(\n",
    "    root_dir=root_dir,\n",
    "    n_train=100,\n",
    "    n_tests=[32, 32, 32, 32],\n",
    "    batch_size=16,\n",
    "    test_batch_sizes=[16, 16, 16, 16],\n",
    "    train_resolution=128,  # change resolution to download different data\n",
    "    test_resolutions=[16, 32, 64, 128],\n",
    ")\n",
    "\n",
    "# load darcy flow dataset\n",
    "\n",
    "\n",
    "def load_darcy_flow_small(\n",
    "    n_train,\n",
    "    n_tests,\n",
    "    data_root=root_dir,\n",
    "    test_resolutions=(16, 32),\n",
    "    train_resolution=16,\n",
    "):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    batch_size = 16\n",
    "    test_batch_sizes = [batch_size] * len(test_resolutions)\n",
    "\n",
    "    dataset = DarcyDataset(\n",
    "        root_dir=data_root,\n",
    "        n_train=n_train,\n",
    "        n_tests=n_tests,\n",
    "        batch_size=batch_size,\n",
    "        test_batch_sizes=test_batch_sizes,\n",
    "        train_resolution=train_resolution,\n",
    "        test_resolutions=test_resolutions,\n",
    "        encode_input=False,\n",
    "        encode_output=True,\n",
    "        channel_dim=1,\n",
    "        encoding='channel-wise',\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    # return dataloaders for backwards compat\n",
    "    train_loader = DataLoader(\n",
    "        dataset.train_db,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    test_loaders = {}\n",
    "    for res, test_bsize in zip(test_resolutions, test_batch_sizes):\n",
    "        test_loaders[res] = DataLoader(\n",
    "            dataset.test_dbs[res],\n",
    "            batch_size=test_bsize,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "\n",
    "    return train_loader, test_loaders, dataset.data_processor\n",
    "\n",
    "\n",
    "train_resolution = 16\n",
    "train_loader, test_loaders, output_encoder = load_darcy_flow_small(\n",
    "    n_train=1000,\n",
    "    # batch_size=16,\n",
    "    train_resolution=train_resolution,\n",
    "    test_resolutions=[16, 32, 64, 128],\n",
    "    n_tests=[100, 100, 100, 100],\n",
    "    # test_batch_sizes=[32, 32, 32, 32],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ecd992f-21b8-4e28-92fb-3b107127e52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[[[1., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 0., 0., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 0.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 1.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]]), 'y': tensor([[[[1.1645e-05, 3.8866e-04, 6.2636e-04,  ..., 2.0518e-03,\n",
      "           1.4713e-03, 3.0848e-03],\n",
      "          [3.8803e-04, 2.0817e-02, 3.5712e-02,  ..., 1.3034e-01,\n",
      "           1.2955e-01, 1.3645e-01],\n",
      "          [6.2186e-04, 3.5467e-02, 6.2444e-02,  ..., 2.4814e-01,\n",
      "           2.5193e-01, 2.0839e-01],\n",
      "          ...,\n",
      "          [8.5640e-03, 4.6371e-01, 7.5949e-01,  ..., 8.6734e-01,\n",
      "           7.1141e-01, 4.3092e-01],\n",
      "          [7.2435e-03, 3.8212e-01, 6.1842e-01,  ..., 7.1836e-01,\n",
      "           5.8678e-01, 3.5949e-01],\n",
      "          [4.9212e-03, 2.4092e-01, 3.7641e-01,  ..., 4.3617e-01,\n",
      "           3.6081e-01, 2.2904e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.2979e-05, 4.7072e-04, 7.5895e-04,  ..., 8.5980e-04,\n",
      "           5.9369e-04, 3.6482e-04],\n",
      "          [4.7817e-04, 2.6503e-02, 4.4924e-02,  ..., 4.5125e-02,\n",
      "           3.3255e-02, 1.9291e-02],\n",
      "          [1.3278e-03, 4.8944e-02, 8.5003e-02,  ..., 7.4926e-02,\n",
      "           5.7590e-02, 3.2611e-02],\n",
      "          ...,\n",
      "          [1.2148e-03, 7.4416e-02, 1.4172e-01,  ..., 2.4620e-01,\n",
      "           1.0034e-01, 3.2151e-02],\n",
      "          [1.0146e-03, 6.2510e-02, 1.2260e-01,  ..., 8.2960e-02,\n",
      "           5.5662e-02, 2.9225e-02],\n",
      "          [6.3594e-04, 3.8338e-02, 1.2140e-01,  ..., 4.6406e-02,\n",
      "           3.2745e-02, 1.8216e-02]]],\n",
      "\n",
      "\n",
      "        [[[1.2470e-05, 4.4230e-04, 7.3237e-04,  ..., 8.6310e-03,\n",
      "           7.3520e-03, 4.9889e-03],\n",
      "          [4.4154e-04, 2.4307e-02, 4.2620e-02,  ..., 4.7020e-01,\n",
      "           3.9000e-01, 2.4560e-01],\n",
      "          [7.2642e-04, 4.2332e-02, 7.6475e-02,  ..., 7.8269e-01,\n",
      "           6.3802e-01, 3.8705e-01],\n",
      "          ...,\n",
      "          [6.7551e-04, 3.7756e-02, 6.2201e-02,  ..., 6.4526e-01,\n",
      "           6.0919e-01, 3.9134e-01],\n",
      "          [5.0953e-04, 2.7877e-02, 4.6050e-02,  ..., 4.8164e-01,\n",
      "           4.7408e-01, 3.1485e-01],\n",
      "          [3.2133e-04, 1.6397e-02, 2.6395e-02,  ..., 2.9073e-01,\n",
      "           2.8980e-01, 2.0055e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.7279e-04, 4.6231e-03, 6.6563e-03,  ..., 7.5615e-04,\n",
      "           5.9272e-04, 3.6854e-04],\n",
      "          [4.5678e-03, 2.1930e-01, 3.4150e-01,  ..., 4.4156e-02,\n",
      "           3.3644e-02, 1.9583e-02],\n",
      "          [6.2267e-03, 3.2044e-01, 5.2113e-01,  ..., 7.9178e-02,\n",
      "           5.9224e-02, 3.3277e-02],\n",
      "          ...,\n",
      "          [4.0621e-03, 1.7606e-01, 2.1526e-01,  ..., 9.9147e-02,\n",
      "           6.9269e-02, 3.8263e-02],\n",
      "          [3.8130e-03, 1.5710e-01, 1.7087e-01,  ..., 7.6575e-02,\n",
      "           5.4739e-02, 3.0370e-02],\n",
      "          [2.7924e-03, 9.0278e-02, 3.1613e-02,  ..., 4.2959e-02,\n",
      "           3.1604e-02, 1.8171e-02]]],\n",
      "\n",
      "\n",
      "        [[[1.1587e-05, 3.8508e-04, 6.2000e-04,  ..., 6.1743e-03,\n",
      "           5.7548e-03, 4.2174e-03],\n",
      "          [3.8390e-04, 2.0574e-02, 3.5291e-02,  ..., 3.0951e-01,\n",
      "           2.8678e-01, 1.9599e-01],\n",
      "          [6.1079e-04, 3.4844e-02, 6.1779e-02,  ..., 4.6241e-01,\n",
      "           4.3845e-01, 2.9212e-01],\n",
      "          ...,\n",
      "          [9.8549e-04, 6.0486e-02, 9.3378e-02,  ..., 8.5413e-01,\n",
      "           6.0547e-01, 2.9767e-01],\n",
      "          [2.3157e-03, 6.4872e-02, 7.5279e-02,  ..., 6.5515e-01,\n",
      "           3.7778e-01, 4.2565e-02],\n",
      "          [1.9030e-03, 4.0029e-02, 4.3853e-02,  ..., 3.7539e-01,\n",
      "           1.7091e-01, 2.4123e-02]]],\n",
      "\n",
      "\n",
      "        [[[1.8058e-04, 5.0944e-03, 7.4054e-03,  ..., 9.5542e-04,\n",
      "           7.3427e-04, 5.4097e-04],\n",
      "          [5.1104e-03, 2.5214e-01, 3.9359e-01,  ..., 5.6950e-02,\n",
      "           4.2503e-02, 2.5335e-02],\n",
      "          [7.5297e-03, 3.9970e-01, 6.4640e-01,  ..., 1.0425e-01,\n",
      "           7.6072e-02, 4.1959e-02],\n",
      "          ...,\n",
      "          [6.6863e-04, 3.8281e-02, 6.6944e-02,  ..., 4.1603e-01,\n",
      "           2.9206e-01, 1.5627e-01],\n",
      "          [5.2706e-04, 2.9281e-02, 4.9970e-02,  ..., 1.1472e-01,\n",
      "           4.9753e-02, 3.2777e-02],\n",
      "          [3.3374e-04, 1.7270e-02, 2.8381e-02,  ..., 3.4088e-02,\n",
      "           2.7879e-02, 1.7669e-02]]]])}\n"
     ]
    }
   ],
   "source": [
    "for _idx, batch in enumerate(train_loader):\n",
    "    print(batch)\n",
    "    sample = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d9aad0-215b-4aff-8891-6630da2896fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lowpass_filter_2d(object):\n",
    "    \"\"\"Lowpass filter the image.\n",
    "\n",
    "    Args:\n",
    "        filter_limit: frequencies > filter_lim excluded\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filter_limit: int, img_size: int) -> None:\n",
    "        assert isinstance(filter_limit, int)\n",
    "        assert isinstance(img_size, int)\n",
    "        self.filter_limit = filter_limit\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # assert that filter limit is less than half img_size\n",
    "        assert self.filter_limit <= self.img_size // 2\n",
    "\n",
    "        # get filter\n",
    "        self.filter = get_2d_low_pass_filter(self.filter_limit, self.img_size)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        model_input, label = sample['x'], sample['y']\n",
    "\n",
    "        \n",
    "        filter_input = filter_batch(self.filter, model_input)\n",
    "        filter_label = filter_batch(self.filter, label)\n",
    "        \n",
    "        return {'x': filter_input, 'y': filter_label}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6c2d91-bc18-4703-9deb-0c5dbd6d5a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4510,  0.4030,  0.3159,  ...,  0.5556,  0.5167,  0.4821],\n",
       "          [ 0.6499,  0.5980,  0.4804,  ...,  0.7668,  0.7113,  0.6748],\n",
       "          [ 0.8045,  0.7405,  0.5866,  ...,  0.9156,  0.8543,  0.8243],\n",
       "          ...,\n",
       "          [-0.0433, -0.0363, -0.0135,  ...,  0.1041,  0.0215, -0.0270],\n",
       "          [ 0.0720,  0.0469,  0.0193,  ...,  0.1634,  0.1246,  0.0958],\n",
       "          [ 0.2459,  0.2034,  0.1425,  ...,  0.3321,  0.3050,  0.2768]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9663,  0.8855,  0.7902,  ...,  0.6679,  0.8757,  0.9765],\n",
       "          [ 0.8671,  0.7973,  0.7561,  ...,  0.6518,  0.8368,  0.9017],\n",
       "          [ 0.7499,  0.7113,  0.7316,  ...,  0.6029,  0.7544,  0.7896],\n",
       "          ...,\n",
       "          [ 0.8597,  0.9363,  0.9275,  ...,  0.3574,  0.5388,  0.7170],\n",
       "          [ 0.9471,  0.9489,  0.8710,  ...,  0.5045,  0.7013,  0.8597],\n",
       "          [ 0.9956,  0.9390,  0.8275,  ...,  0.6194,  0.8298,  0.9629]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7097,  0.9081,  0.9828,  ...,  0.0211,  0.1766,  0.4359],\n",
       "          [ 0.6775,  0.9089,  1.0466,  ..., -0.0262,  0.1420,  0.3987],\n",
       "          [ 0.6173,  0.8746,  1.0688,  ..., -0.0276,  0.1191,  0.3476],\n",
       "          ...,\n",
       "          [ 0.4285,  0.5380,  0.5320,  ...,  0.1510,  0.1509,  0.2674],\n",
       "          [ 0.5704,  0.7042,  0.7000,  ...,  0.1420,  0.1888,  0.3624],\n",
       "          [ 0.6757,  0.8401,  0.8634,  ...,  0.0894,  0.1979,  0.4264]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.3244,  0.1483,  0.0875,  ...,  1.1111,  0.8842,  0.5927],\n",
       "          [ 0.3636,  0.1399,  0.0149,  ...,  1.0972,  0.9170,  0.6467],\n",
       "          [ 0.4288,  0.1639, -0.0182,  ...,  1.0527,  0.9443,  0.7152],\n",
       "          ...,\n",
       "          [ 0.4482,  0.4043,  0.4327,  ...,  0.6918,  0.6205,  0.5323],\n",
       "          [ 0.3702,  0.2817,  0.3049,  ...,  0.9231,  0.7514,  0.5459],\n",
       "          [ 0.3255,  0.1945,  0.1872,  ...,  1.0628,  0.8357,  0.5614]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5766,  0.8372,  1.0172,  ..., -0.0084,  0.0865,  0.3006],\n",
       "          [ 0.5430,  0.8020,  0.9953,  ...,  0.0771,  0.1141,  0.2872],\n",
       "          [ 0.5188,  0.7671,  0.9676,  ...,  0.1835,  0.1670,  0.2937],\n",
       "          ...,\n",
       "          [ 0.6686,  0.8821,  1.0212,  ...,  0.0021,  0.1911,  0.4240],\n",
       "          [ 0.6332,  0.8712,  1.0254,  ..., -0.0562,  0.1182,  0.3630],\n",
       "          [ 0.6064,  0.8601,  1.0267,  ..., -0.0575,  0.0857,  0.3250]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5925,  0.4855,  0.4059,  ...,  0.9074,  0.8314,  0.7160],\n",
       "          [ 0.4505,  0.2885,  0.1884,  ...,  1.0501,  0.8779,  0.6600],\n",
       "          [ 0.3646,  0.1636,  0.0366,  ...,  1.1075,  0.8883,  0.6208],\n",
       "          ...,\n",
       "          [ 0.8238,  0.9190,  0.8819,  ...,  0.2519,  0.4149,  0.6310],\n",
       "          [ 0.8223,  0.8582,  0.8085,  ...,  0.4427,  0.5720,  0.7139],\n",
       "          [ 0.7354,  0.6979,  0.6341,  ...,  0.6874,  0.7260,  0.7438]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowpass_transform = lowpass_filter_2d(3, 16)\n",
    "\n",
    "lowpass_transform(sample)['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c4d5ac4-12b8-4387-b426-955366a8a45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2e0c9d6d70>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_batch = lowpass_transform(sample)['x']\n",
    "\n",
    "samples = 16\n",
    "s = 16\n",
    "filtered_sp = get_energy_curve(\n",
    "    filtered_batch[:samples].reshape(samples * 1, s, s)\n",
    ")\n",
    "\n",
    "unfiltered_sp = get_energy_curve(\n",
    "    sample['x'][:samples].reshape(samples * 1, s, s)\n",
    ")\n",
    "\n",
    "plt.plot(filtered_sp, color=\"blue\", label=\"filtered\")\n",
    "plt.plot(unfiltered_sp, color=\"red\", label=\"unfiltered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b287cd5-db23-4c70-a09f-aa667f8a8e6c",
   "metadata": {},
   "source": [
    "# PDE BENCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd180d-dbfb-4016-a041-fdd5dc412b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pdebench/PDEBench/blob/main/pdebench/models/fno/utils.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import math as mt\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class FNODatasetSingle(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename,\n",
    "        initial_step=10,\n",
    "        saved_folder=\"../data/\",\n",
    "        reduced_resolution=1,\n",
    "        reduced_resolution_t=1,\n",
    "        reduced_batch=1,\n",
    "        if_test=False,\n",
    "        test_ratio=0.1,\n",
    "        num_samples_max=-1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param filename: filename that contains the dataset\n",
    "        :type filename: STR\n",
    "        :param filenum: array containing indices of filename included in the dataset\n",
    "        :type filenum: ARRAY\n",
    "        :param initial_step: time steps taken as initial condition, defaults to 10\n",
    "        :type initial_step: INT, optional\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Define path to files\n",
    "        root_path = Path(Path(saved_folder).resolve()) / filename\n",
    "        if filename[-2:] != \"h5\":\n",
    "            # print(\".HDF5 file extension is assumed hereafter\")\n",
    "\n",
    "            with h5py.File(root_path, \"r\") as f:\n",
    "                keys = list(f.keys())\n",
    "                keys.sort()\n",
    "                if \"tensor\" not in keys:\n",
    "                    _data = np.array(\n",
    "                        f[\"density\"], dtype=np.float32\n",
    "                    )  # batch, time, x,...\n",
    "                    idx_cfd = _data.shape\n",
    "                    if len(idx_cfd) == 3:  # 1D\n",
    "                        self.data = np.zeros(\n",
    "                            [\n",
    "                                idx_cfd[0] // reduced_batch,\n",
    "                                idx_cfd[2] // reduced_resolution,\n",
    "                                mt.ceil(idx_cfd[1] / reduced_resolution_t),\n",
    "                                3,\n",
    "                            ],\n",
    "                            dtype=np.float32,\n",
    "                        )\n",
    "                        # density\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data[:, :, :], (0, 2, 1))\n",
    "                        self.data[..., 0] = _data  # batch, x, t, ch\n",
    "                        # pressure\n",
    "                        _data = np.array(\n",
    "                            f[\"pressure\"], dtype=np.float32\n",
    "                        )  # batch, time, x,...\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data[:, :, :], (0, 2, 1))\n",
    "                        self.data[..., 1] = _data  # batch, x, t, ch\n",
    "                        # Vx\n",
    "                        _data = np.array(\n",
    "                            f[\"Vx\"], dtype=np.float32\n",
    "                        )  # batch, time, x,...\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data[:, :, :], (0, 2, 1))\n",
    "                        self.data[..., 2] = _data  # batch, x, t, ch\n",
    "\n",
    "                        self.grid = np.array(f[\"x-coordinate\"], dtype=np.float32)\n",
    "                        self.grid = torch.tensor(\n",
    "                            self.grid[::reduced_resolution], dtype=torch.float\n",
    "                        ).unsqueeze(-1)\n",
    "                        # print(self.data.shape)\n",
    "                    if len(idx_cfd) == 4:  # 2D\n",
    "                        self.data = np.zeros(\n",
    "                            [\n",
    "                                idx_cfd[0] // reduced_batch,\n",
    "                                idx_cfd[2] // reduced_resolution,\n",
    "                                idx_cfd[3] // reduced_resolution,\n",
    "                                mt.ceil(idx_cfd[1] / reduced_resolution_t),\n",
    "                                4,\n",
    "                            ],\n",
    "                            dtype=np.float32,\n",
    "                        )\n",
    "                        # density\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data, (0, 2, 3, 1))\n",
    "                        self.data[..., 0] = _data  # batch, x, t, ch\n",
    "                        # pressure\n",
    "                        _data = np.array(\n",
    "                            f[\"pressure\"], dtype=np.float32\n",
    "                        )  # batch, time, x,...\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data, (0, 2, 3, 1))\n",
    "                        self.data[..., 1] = _data  # batch, x, t, ch\n",
    "                        # Vx\n",
    "                        _data = np.array(\n",
    "                            f[\"Vx\"], dtype=np.float32\n",
    "                        )  # batch, time, x,...\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data, (0, 2, 3, 1))\n",
    "                        self.data[..., 2] = _data  # batch, x, t, ch\n",
    "                        # Vy\n",
    "                        _data = np.array(\n",
    "                            f[\"Vy\"], dtype=np.float32\n",
    "                        )  # batch, time, x,...\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data, (0, 2, 3, 1))\n",
    "                        self.data[..., 3] = _data  # batch, x, t, ch\n",
    "\n",
    "                        x = np.array(f[\"x-coordinate\"], dtype=np.float32)\n",
    "                        y = np.array(f[\"y-coordinate\"], dtype=np.float32)\n",
    "                        x = torch.tensor(x, dtype=torch.float)\n",
    "                        y = torch.tensor(y, dtype=torch.float)\n",
    "                        X, Y = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "                        self.grid = torch.stack((X, Y), axis=-1)[\n",
    "                            ::reduced_resolution, ::reduced_resolution\n",
    "                        ]\n",
    "\n",
    "                    if len(idx_cfd) == 5:  # 3D\n",
    "                        self.data = np.zeros(\n",
    "                            [\n",
    "                                idx_cfd[0] // reduced_batch,\n",
    "                                idx_cfd[2] // reduced_resolution,\n",
    "                                idx_cfd[3] // reduced_resolution,\n",
    "                                idx_cfd[4] // reduced_resolution,\n",
    "                                mt.ceil(idx_cfd[1] / reduced_resolution_t),\n",
    "                                5,\n",
    "                            ],\n",
    "                            dtype=np.float32,\n",
    "                        )\n",
    "                        # density\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data, (0, 2, 3, 4, 1))\n",
    "                        self.data[..., 0] = _data  # batch, x, t, ch\n",
    "                        # pressure\n",
    "                        _data = np.array(\n",
    "                            f[\"pressure\"], dtype=np.float32\n",
    "                        )  # batch, time, x,...\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data, (0, 2, 3, 4, 1))\n",
    "                        self.data[..., 1] = _data  # batch, x, t, ch\n",
    "                        # Vx\n",
    "                        _data = np.array(\n",
    "                            f[\"Vx\"], dtype=np.float32\n",
    "                        )  # batch, time, x,...\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data, (0, 2, 3, 4, 1))\n",
    "                        self.data[..., 2] = _data  # batch, x, t, ch\n",
    "                        # Vy\n",
    "                        _data = np.array(\n",
    "                            f[\"Vy\"], dtype=np.float32\n",
    "                        )  # batch, time, x,...\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data, (0, 2, 3, 4, 1))\n",
    "                        self.data[..., 3] = _data  # batch, x, t, ch\n",
    "                        # Vz\n",
    "                        _data = np.array(\n",
    "                            f[\"Vz\"], dtype=np.float32\n",
    "                        )  # batch, time, x,...\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data, (0, 2, 3, 4, 1))\n",
    "                        self.data[..., 4] = _data  # batch, x, t, ch\n",
    "\n",
    "                        x = np.array(f[\"x-coordinate\"], dtype=np.float32)\n",
    "                        y = np.array(f[\"y-coordinate\"], dtype=np.float32)\n",
    "                        z = np.array(f[\"z-coordinate\"], dtype=np.float32)\n",
    "                        x = torch.tensor(x, dtype=torch.float)\n",
    "                        y = torch.tensor(y, dtype=torch.float)\n",
    "                        z = torch.tensor(z, dtype=torch.float)\n",
    "                        X, Y, Z = torch.meshgrid(x, y, z, indexing=\"ij\")\n",
    "                        self.grid = torch.stack((X, Y, Z), axis=-1)[\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "\n",
    "                else:  # scalar equations\n",
    "                    ## data dim = [t, x1, ..., xd, v]\n",
    "                    _data = np.array(\n",
    "                        f[\"tensor\"], dtype=np.float32\n",
    "                    )  # batch, time, x,...\n",
    "                    if len(_data.shape) == 3:  # 1D\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            ::reduced_resolution_t,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data[:, :, :], (0, 2, 1))\n",
    "                        self.data = _data[:, :, :, None]  # batch, x, t, ch\n",
    "\n",
    "                        self.grid = np.array(f[\"x-coordinate\"], dtype=np.float32)\n",
    "                        self.grid = torch.tensor(\n",
    "                            self.grid[::reduced_resolution], dtype=torch.float\n",
    "                        ).unsqueeze(-1)\n",
    "                    if len(_data.shape) == 4:  # 2D Darcy flow\n",
    "                        # u: label\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            :,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data[:, :, :, :], (0, 2, 3, 1))\n",
    "                        # if _data.shape[-1]==1:  # if nt==1\n",
    "                        #    _data = np.tile(_data, (1, 1, 1, 2))\n",
    "                        self.data = _data\n",
    "                        # nu: input\n",
    "                        _data = np.array(\n",
    "                            f[\"nu\"], dtype=np.float32\n",
    "                        )  # batch, time, x,...\n",
    "                        _data = _data[\n",
    "                            ::reduced_batch,\n",
    "                            None,\n",
    "                            ::reduced_resolution,\n",
    "                            ::reduced_resolution,\n",
    "                        ]\n",
    "                        ## convert to [x1, ..., xd, t, v]\n",
    "                        _data = np.transpose(_data[:, :, :, :], (0, 2, 3, 1))\n",
    "                        self.data = np.concatenate([_data, self.data], axis=-1)\n",
    "                        self.data = self.data[:, :, :, :, None]  # batch, x, y, t, ch\n",
    "\n",
    "                        x = np.array(f[\"x-coordinate\"], dtype=np.float32)\n",
    "                        y = np.array(f[\"y-coordinate\"], dtype=np.float32)\n",
    "                        x = torch.tensor(x, dtype=torch.float)\n",
    "                        y = torch.tensor(y, dtype=torch.float)\n",
    "                        X, Y = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "                        self.grid = torch.stack((X, Y), axis=-1)[\n",
    "                            ::reduced_resolution, ::reduced_resolution\n",
    "                        ]\n",
    "\n",
    "        elif filename[-2:] == \"h5\":  # SWE-2D (RDB)\n",
    "            # print(\".H5 file extension is assumed hereafter\")\n",
    "\n",
    "            with h5py.File(root_path, \"r\") as f:\n",
    "                keys = list(f.keys())\n",
    "                keys.sort()\n",
    "\n",
    "                data_arrays = [\n",
    "                    np.array(f[key][\"data\"], dtype=np.float32) for key in keys\n",
    "                ]\n",
    "                _data = torch.from_numpy(\n",
    "                    np.stack(data_arrays, axis=0)\n",
    "                )  # [batch, nt, nx, ny, nc]\n",
    "                _data = _data[\n",
    "                    ::reduced_batch,\n",
    "                    ::reduced_resolution_t,\n",
    "                    ::reduced_resolution,\n",
    "                    ::reduced_resolution,\n",
    "                    ...,\n",
    "                ]\n",
    "                _data = torch.permute(_data, (0, 2, 3, 1, 4))  # [batch, nx, ny, nt, nc]\n",
    "                gridx, gridy = (\n",
    "                    np.array(f[\"0023\"][\"grid\"][\"x\"], dtype=np.float32),\n",
    "                    np.array(f[\"0023\"][\"grid\"][\"y\"], dtype=np.float32),\n",
    "                )\n",
    "                mgridX, mgridY = np.meshgrid(gridx, gridy, indexing=\"ij\")\n",
    "                _grid = torch.stack(\n",
    "                    (torch.from_numpy(mgridX), torch.from_numpy(mgridY)), axis=-1\n",
    "                )\n",
    "                _grid = _grid[::reduced_resolution, ::reduced_resolution, ...]\n",
    "                _tsteps_t = torch.from_numpy(\n",
    "                    np.array(f[\"0023\"][\"grid\"][\"t\"], dtype=np.float32)\n",
    "                )\n",
    "\n",
    "                tsteps_t = _tsteps_t[::reduced_resolution_t]\n",
    "                self.data = _data\n",
    "                self.grid = _grid\n",
    "                self.tsteps_t = tsteps_t\n",
    "\n",
    "        if num_samples_max > 0:\n",
    "            num_samples_max = min(num_samples_max, self.data.shape[0])\n",
    "        else:\n",
    "            num_samples_max = self.data.shape[0]\n",
    "\n",
    "        test_idx = int(num_samples_max * test_ratio)\n",
    "        if if_test:\n",
    "            self.data = self.data[:test_idx]\n",
    "        else:\n",
    "            self.data = self.data[test_idx:num_samples_max]\n",
    "\n",
    "        # Time steps used as initial conditions\n",
    "        self.initial_step = initial_step\n",
    "\n",
    "        self.data = self.data if torch.is_tensor(self.data) else torch.tensor(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, ..., : self.initial_step, :], self.data[idx], self.grid\n",
    "\n",
    "\n",
    "class FNODatasetMult(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename,\n",
    "        initial_step=10,\n",
    "        saved_folder=\"../data/\",\n",
    "        if_test=False,\n",
    "        test_ratio=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param filename: filename that contains the dataset\n",
    "        :type filename: STR\n",
    "        :param filenum: array containing indices of filename included in the dataset\n",
    "        :type filenum: ARRAY\n",
    "        :param initial_step: time steps taken as initial condition, defaults to 10\n",
    "        :type initial_step: INT, optional\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Define path to files\n",
    "        self.file_path = Path(saved_folder + filename + \".h5\").resolve()\n",
    "\n",
    "        # Extract list of seeds\n",
    "        with h5py.File(self.file_path, \"r\") as h5_file:\n",
    "            data_list = sorted(h5_file.keys())\n",
    "\n",
    "        test_idx = int(len(data_list) * (1 - test_ratio))\n",
    "        if if_test:\n",
    "            self.data_list = np.array(data_list[test_idx:])\n",
    "        else:\n",
    "            self.data_list = np.array(data_list[:test_idx])\n",
    "\n",
    "        # Time steps used as initial conditions\n",
    "        self.initial_step = initial_step\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Open file and read data\n",
    "        with h5py.File(self.file_path, \"r\") as h5_file:\n",
    "            seed_group = h5_file[self.data_list[idx]]\n",
    "\n",
    "            # data dim = [t, x1, ..., xd, v]\n",
    "            data = np.array(seed_group[\"data\"], dtype=\"f\")\n",
    "            data = torch.tensor(data, dtype=torch.float)\n",
    "\n",
    "            # convert to [x1, ..., xd, t, v]\n",
    "            permute_idx = list(range(1, len(data.shape) - 1))\n",
    "            permute_idx.extend([0, -1])\n",
    "            data = data.permute(permute_idx)\n",
    "\n",
    "            # Extract spatial dimension of data\n",
    "            dim = len(data.shape) - 2\n",
    "\n",
    "            # x, y and z are 1-D arrays\n",
    "            # Convert the spatial coordinates to meshgrid\n",
    "            if dim == 1:\n",
    "                grid = np.array(seed_group[\"grid\"][\"x\"], dtype=\"f\")\n",
    "                grid = torch.tensor(grid, dtype=torch.float).unsqueeze(-1)\n",
    "            elif dim == 2:\n",
    "                x = np.array(seed_group[\"grid\"][\"x\"], dtype=\"f\")\n",
    "                y = np.array(seed_group[\"grid\"][\"y\"], dtype=\"f\")\n",
    "                x = torch.tensor(x, dtype=torch.float)\n",
    "                y = torch.tensor(y, dtype=torch.float)\n",
    "                X, Y = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "                grid = torch.stack((X, Y), axis=-1)\n",
    "            elif dim == 3:\n",
    "                x = np.array(seed_group[\"grid\"][\"x\"], dtype=\"f\")\n",
    "                y = np.array(seed_group[\"grid\"][\"y\"], dtype=\"f\")\n",
    "                z = np.array(seed_group[\"grid\"][\"z\"], dtype=\"f\")\n",
    "                x = torch.tensor(x, dtype=torch.float)\n",
    "                y = torch.tensor(y, dtype=torch.float)\n",
    "                z = torch.tensor(z, dtype=torch.float)\n",
    "                X, Y, Z = torch.meshgrid(x, y, z, indexing=\"ij\")\n",
    "                grid = torch.stack((X, Y, Z), axis=-1)\n",
    "\n",
    "        return data[..., : self.initial_step, :], data, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22666f4-8ca5-46af-a5de-3917bd9bd2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FNODatasetSingle(\n",
    "        filename=\"2D_DarcyFlow_beta0.01_Train.hdf5\",\n",
    "        initial_step=10,\n",
    "        saved_folder=\"/pscratch/sd/m/mansisak/PDEBench/pdebench_data/2D/DarcyFlow/\",\n",
    "        reduced_resolution=1,\n",
    "        reduced_resolution_t=1,\n",
    "        reduced_batch=7,\n",
    "        if_test=False,\n",
    "        test_ratio=0.1,\n",
    "        num_samples_max=-1,\n",
    "    )\n",
    "\n",
    "print(data[0][1].shape)\n",
    "initial = data[0][0]\n",
    "label = data[0][1]\n",
    "grid = data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdab373-967a-46e6-b775-4ce120c4c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(initial, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8a55b-1ae6-4380-85c8-3171df3cd429",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(initial[:,:, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bc056-3384-4f23-b6c1-d405db2ea1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label[:,:, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835650a-30d6-4c0d-8cca-b9e7e3ce520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(grid[:,:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0281f-910f-491e-a82e-0cb48440fdfc",
   "metadata": {},
   "source": [
    "# TODO: produce data loader which can apply a specific filter and use it to train the model!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "operator_alias",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
